4622 en Lecture 1: The Origins of the Space Shuttle 
4623 en Lecture 2: Space Shuttle History 
4624 en Lecture 3: Orbiter Sub-System Design 
4625 en Lecture 4: The Decision to Build the Shuttle 
4626 en Lecture 5: Orbiter Structure + Thermal Protection System 
4627 en Lecture 6: Propulsion - Space Shuttle Main Engines 
4628 en Lecture 7: Aerodynamics - (From Sub - to Hypersonic and Back) 
4629 en Lecture 8: Landing and Mechanical Systems 
4630 en Lecture 9: OMS, RCS, Fuel Cells, Auxiliary Power Unit and Hydraulic Systems 
4631 en Lecture 10: The DoD and the Space Shuttle 
4632 en Lecture 11: Use of Subsystems as a Function of Flight Phase 
4634 en Lecture 13: Environmental Control Systems 
4635 en Lecture 14: Ground Operations - Launching the Shuttle 
4636 en Lecture 15: Space Shuttle Accidents 
4637 en Lecture 16: Guidance, Navigation and Control 
4638 en Lecture 17: Mission Control 1 
4639 en Lecture 18: Mission Control 2 
4640 en Lecture 19: Design Process as it Relates to the Shuttle 
4641 en Lecture 20: EVA and Robotics on the Shuttle 
4642 en Lecture 21: Systems Engineering for Space Shuttle Payloads 
4643 en Lecture 22: Test Flying the Space Shuttle 
5022 en Welcome and introduction to WS DEBATE 
5023 en Welcome and introduction to WS DEBATE 
5027 en Research on position of women in science 
5034 en Experience of women in politics in socialism and transition period 
7160 en Lecture 1: Introduction - Sampling Theorem and Orthonormal PAM/QAM - Capacity of AWGN Channels 
7161 en Lecture 2: Performance of Small Signal Constellations 
7162 en Lecture 3 and 4: Hard-decision and Soft-decision Decoding 
7163 en Lecture 5 and 6: Introduction to Binary Block Codes 
7164 en Lecture 7, 8 and 9: Introduction to Finite Fields 
7165 en Lecture 10, 11 and 12: Reed-Solomon Codes 
7166 en Lecture 13 and 14: Introduction to Convolutional Codes 
7167 en Lecture 15 and 16: Trellis Representations of Binary Linear Block Codes 
7168 en Lecture 17 and 18: Codes on Graphs 
7169 en Lecture 19: The Sum-Product Algorithm 
7170 en Lecture 20 and 21: Turbo, LDPC, and RA Codes 
7171 en Lecture 22 and 23: Lattice and Trellis Codes 
7172 en Lecture 1: Introduction; Basics of Legal Research; Legal Citations 
7173 en Lecture 2: LexisNexis®; 1976 Copyright Act Here's a LexisNexis® handout you may find helpful. (PDF)nnPlease read these cases to discuss in class:nn- [[http://caselaw.lp.findlaw.com/scripts/getcase.pl?court=us&vol=499&invol=340|Feist Publications, Inc. v. Rural Telephone Service Co.]], 499 U.S. 340 (1991). ([[http://ocw.mit.edu/NR/rdonlyres/Electrical-Engineering-and-Computer-Science/6-912January--IAP--2006/A7D69482-B8C0-4A1D-952E-A58ADAC98216/0/feist.pdf|PDF]])nn- [[http://caselaw.lp.findlaw.com/scripts/getcase.pl?court=us&vol=000&invol=U10426|Campbell v. Acuff-Rose Music, Inc.]], 510 U.S. 569 (1994). ([[http://ocw.mit.edu/NR/rdonlyres/Electrical-Engineering-and-Computer-Science/6-912January--IAP--2006/1B8A67E3-1DC4-4E7C-8B56-E8C48E4056B3/0/campbell.pdf|PDF]])nn- On Command Video Corp. v. Columbia Pictures Industries, 777 F. Supp. 787 (N.D. Cal. 1991). (PDF)nn- [[http://caselaw.lp.findlaw.com/scripts/getcase.pl?court=us&vol=464&invol=417|ony Corp. of America v. Universal City Studios, Inc.]], 464 U.S. 417 (1984). ([[http://ocw.mit.edu/NR/rdonlyres/Electrical-Engineering-and-Computer-Science/6-912January--IAP--2006/E631BDFB-DF7D-4DA7-B6A7-1861AEF62A60/0/sony.pdf|PDF]])nn(Remember, just type the middle part of the citation (e.g., "499 U.S. 340") into the "Get a Case" citation field in Lexis-Nexis to call up the opinion.)
7174 en Lecture 3: Copyright applied to Music, Computers; Napster®; Peer-to-Peer File Sharing Go to LexisNexis®, "Legal Research," then "Federal Code," and finally "Guided Search." (That will take you to this form.)nnSearch for "17 uscs" in "Cite." This will pull up the entire [[http://www.copyright.gov/title17/|Copyright Act]]. Alternately, you can use the [[http://www.copyright.gov/|U. S. Copyright Office]]'s file: [[http://ocw.mit.edu/NR/rdonlyres/Electrical-Engineering-and-Computer-Science/6-912January--IAP--2006/AEADEBD6-84C3-4568-8131-467C351748A1/0/copyright_law.pdf|(PDF - 3 MB]]).nnRead the following sections of the statute. (If the section seems to go on and on, just read the first few subsections. You may find it helpful to look back at the definitions in section 101.)nn* 106 (Core Rights)n* 106A (Limited Moral Rights)n* 107 (Fair Use)n* 109 (First Sale)n* 110 (Exempt Performances)n* 115 Only Read Subsections (a) and (b) (Musical "Covers")n* 117 (Computer Programs)n* 302 (Duration of Copyright)n* 401 (Copyright Notice)n* 411 (Registration)n* 504 (Damages for Infringement)n* 506 (Criminal Violations, including LaMacchia Law)
7175 en Lecture 4: Software Licensing; DVDs and Encryption Read [[http://www4.law.cornell.edu/uscode/html/uscode17/usc_sec_17_00000512----000-.html|17 U.S.C. § 512.]] (Remember: Use "17 uscs sec 512".)nnRead [[http://www.law.cornell.edu/copyright/cases/180_F3d_1072.htm|Recording Industry Ass'n of America v. Diamond Multimedia Systems, Inc.]], 180 F.3d 1072 (9th Cir. 1999).nnRead [[http://caselaw.lp.findlaw.com/scripts/getcase.pl?navby=case&court=2nd&no=00-9185|Universal City Studios, Inc. v. Corley]], 273 F.3d 429 (2d Cir. 2001). [[http://ocw.mit.edu/NR/rdonlyres/Electrical-Engineering-and-Computer-Science/6-912January--IAP--2006/10BD93D5-3B98-47E4-B1CB-20658542E0DC/0/corley.pdf|(PDF)]]nnRead [[http://caselaw.lp.findlaw.com/scripts/getcase.pl?navby=case&court=fed&no=04-1118|Chamberlain Group, Inc. v. Skylink Technologies, Inc.]], 381 F.3d 1178 (Fed. Cir. 2004). [[http://ocw.mit.edu/NR/rdonlyres/Electrical-Engineering-and-Computer-Science/6-912January--IAP--2006/8732161B-AF8C-44F2-B86D-8D32087A244C/0/chamberlain.pdf|(PDF)]]nnRead Jonathan Zittrain's essay, "The Copyright Cage," in [[http://www.legalaffairs.org/issues/July-August-2003/feature_zittrain_julaug03.msp|legalaffairs magazine]].
7304 en Lecture 1: Derivatives, slope, velocity, rate of change 
7305 en Lecture 2: Limits, continuity - Trigonometric limits 
7306 en Lecture 3: Derivatives of products, quotients, sine, cosine 
7307 en Lecture 4: Chain rule - Higher derivatives 
7308 en Lecture 5: Implicit differentiation, inverses 
7309 en Lecture 6: Exponential and log - Logarithmic differentiation; hyperbolic functions 
7310 en Lecture 7: Hyperbolic functions (cont.) and exam 1 review 
7327 en Lecture 2: Determinants; cross product 
7328 en Lecture 3: Matrices; inverse matrices 
7329 en Lecture 4: Square systems; equations of planes 
7330 en Lecture 5: Parametric equations for lines and curves 
7331 en Lecture 6: Velocity, acceleration - Kepler's second law 
7500 en Lecture 8: Level curves; partial derivatives; tangent plane approximation 
7501 en Lecture 9: Max-min problems; least squares 
7503 en Lecture 10: Second derivative test; boundaries and infinity 
7504 en Lecture 11: Differentials; chain rule 
7505 en Lecture 12: Gradient; directional derivative; tangent plane 
7507 en Lecture 14: Non-independent variables 
7508 en Lecture 15: Partial differential equations; review 
7510 en Lecture 17: Double integrals in polar coordinates; applications 
7511 en Lecture 18: Change of variables 
7512 en Lecture 19: Vector fields and line integrals in the plane 
7513 en Lecture 20: Path independence and conservative fields 
7514 en Lecture 21: Gradient fields and potential functions 
7515 en Lecture 22: Green's theorem 
7516 en Lecture 23: Flux; normal form of Green's theorem 
7517 en Lecture 24: Simply connected regions; review 
7518 en Lecture 25: Triple integrals in rectangular and cylindrical coordinates 
7519 en Lecture 26: Spherical coordinates; surface area 
7520 en Lecture 27: Vector fields in 3D; surface integrals and flux 
7522 en Lecture 29: Divergence theorem (cont.): applications and proof 
7523 en Lecture 30: Line integrals in space, curl, exactness and potentials 
7524 en Lecture 31: Stokes' theorem 
7525 en Lecture 32: Stokes' theorem (cont.); review 
7526 en Lecture 33: Topological considerations - Maxwell's equations 
7528 en Lecture 35: Final review (cont.) 
7722 en Photography of African American cultural landscapes 
7723 en Secret games - collaborations with children 
7724 en Photography and its role in rebuilding New Orleans 
7728 en To see is the root of idea 
7746 en Session 1 Heros, Tolkien, Lewis, Beowolf.
7747 en Session 2 Analyzing historical data, making sense of evidence, Why do history at all?
7762 en Session 1 * Introduction to the class and to the aesthetics of film; explanation of syllabus.nn* Discussion of "The Lady Eve" (1941), a film by Sturges, a contemporary of Orson Welles. Film will be shown in this week's class; scholar Marian Keane's commentary, an important example of a woman's perspective on the film, will be shown next week.nn* Looking at movies is an art form and a skill that requires training. Prof. Singer brings to this course the perspective of a philosopher, which is not the case in all film courses.nn* Singer's philosophy of teaching: he is willing to make himself available and put himself forward as an artist does; teaching is a form of self-expression, like art.nn* On philosophy in film and other media (versus the philosophy of film). The idea that film is a respectable art with philosophical content, not just entertainment, is fairly new within the academy (within the last 20 years).nn* How can films be philosophical? Singer's book Reality Transformed addresses this by looking at the concepts of formalism and realism together. Formalists can be characterized by their use of the camera, techniques of cinematography, structural issues (i.e., Hitchcock, who was a master of technique). Realists, as described by French theorist Andre Bazin, see the importance of film as part of the human desire to capture reality.nn* Singer argues that films can't be appreciated unless you savor what they mean; without technique, there is no meaning, and without meaning, technique has no human importance. Only when you see formalism and realism in constant dialectic and interaction can you appreciate what a film is capable of.nn* On the role of myths and mythmaking in film. How do stories reach people? Discussion of various myths of love and how they have been dealt with in film.nn* Continuing overview of syllabus and films to be screened throughout semester.nn* Review of course expectations and requirements.
7763 en Session 2 * Review of previous session.n* How does one apply philosophical analysis of any sort to a work of art such as a movie?n* What is the meaning of the tree of knowledge in the garden of Eden? How is this philosophically important?n* There is an overlap between science, technology, and art. Mathematics can be thought of as a theoretical art.n* Fundamental question of Reality Transformed: Formalists vs. Realistsn* Three Philosophical Filmmakers - all 3 filmmakers are both formalists and realists in their own ways.n* Are films the modern medium of choice for myth-making and disseminating philosophy?n* Hitchcock emphasizes "reality effect" - emphasis on formalism, but striving to bring out realism. Similar to realism portrayed in "The Green Mile."n* Assigning biblical characters to "The Lady Eve." Which characters are Adam and Eve? Who is the snake?n* The myth of the whore/virgin - can the two be harmonized in "The Lady Eve"? Jean journeys from one to the other throughout the movie.
7764 en Session 3 * Edward Song's comments on the week's reading and movie.n* David Levinson's comments on the week's reading and movie.n* Film can focus attention on different aspects of the performance with more specificity than a live performance. This is borne out in the commentary for "The Lady Eve."n* Discussion of camera work as described in Three Philosophical Filmmakers, as compared to the camera work in "The Lady Eve."n* Where / how is the character Charles in "The Lady Eve" likely to find love? What is the nature of love between human beings? Between humans and pets? How is this reflected in the cinematic voice of the filmmaker?n* Discussion of the quality of the cinematography in "Amélie," and the ideals of romantic love (Chekhov short story).n* Did the character Eve in "The Lady Eve" sell out in the end of the movie?
7765 en Session 4 * Discussion of Miguel's paper #2 topics: "Rebel Without a Cause" and how teenagers deal with death.n* Discussion of Edward's paper #2 topics: the ideological contrast presented by the knight and his squire in "The Seventh Seal."n* Discussion of David's paper #2 topics: Responding to Heidegger's position on death and religion, using "The Seventh Seal."n* Discussion of Cathy's paper #2 topics: Three main myths depicted in "8 1/2."n* Discussion of Terry's paper #2 topics: Theological representations in "The Seventh Seal."n* Discussion of Lauren's paper #2 topics: How does the meaning of life affect the outlook of each character in "The Seventh Seal"?n* Discussion of Phillip's paper #2 topics: How imagination enhances meaning in life, as seen in "Life Is Beautiful" and "Amélie."n* Discussion of Peter's paper #2 topics: Contrast between self love and selfishness in relation to "Pride and Prejudice."
8321 en Les super états: la superfluidité 
8329 en The unexpected side of the LHC 
8386 en Class 1: U.S. Planning and Realities of Post-war Iraq 
8387 en Class 2: Politics and Society in Iraq in the 20th Century 
8388 en Class 3: Comparative Insights: Marshall Plan, Japan, and Iraq 
8389 en Class 4: Reconstructing "a New Liberal Iraq" 
8390 en Class 5: Consolidating Iraqi Democracy: the Institutional Context 
8391 en Class 6: The Arab Discourse on Iraq and the International Role 
8392 en Class 11: The Discourse of Iraqis and Arabs on the Reconstruction of a New Iraq Political Cartoons and Reflection by Noam Chomsky - Final presentation by Professor Yosef Jabareen of Arab political cartoons followed by reflections by Professor Noam Chomsky on the Iraqi occupation and the hegemonizing agenda of the U.S. in the Middle East.
8393 en Lecture 3: Newton's Method 
8394 en Lecture 18: Duality Theory I 
8395 en Lecture 23: Semidefinite Optimization I 
8420 en Lecture 1: Who Develops Breakthrough New Products and Services - Users or Manufacturers? Before thinking about how to do concept development, we will explore who does this activity. Specifically, is the concept developer really a manufacturer - or is it a product or service user?
8421 en Lecture 2: Systematic Generation of Ideas for "Breakthrough" New Products and Services - the "Lead User Method" Users innovate when it is in their interest to do so. But not all user innovations will make a good product from a product manufacturer's standpoint. Therefore, manufacturers must identify and learn from "lead" users. 3M and other firms have learned to network their way to lead users and then combine lead user ideas with their own to create "breakthrough" new products and services. The reading for this lecture describes the 3M experience.
8422 en Lecture 4: Systematic Generation of Incremental Improvements to Existing Products and Services, Traditional Marketing Research Concept Generation Techniques Traditional market research techniques are most advanced in consumer products fields. Here, user needs are analyzed via multiattribute techniques, marketing and R&D personnel then use this data to develop new product concepts. Finally, the market potential of these ideas is explored via "focus groups" of representative consumers, questionnaires, etc.
8775 en Brains not Bullets? From Terrorism, Insurgencies and Drug Wars, to Street Gangs and World of Warcraft In previous work, we suggested that common dynamical patterns underlie the evolution of irregular warfare and global terrorism. We offered a simple model to explain all these findings, based on a common ’soup’ of continually evolving attack units. This talk updates this line of research, in light of new results. In addition to confirming the robustness of our model, these results offer a quantitative explanation of why the insurgent war in Iraq, and the drug war in Colombia, have evolved in the way that they have – and how the emerging wars in Afghanistan and Mexico might evolve in the future. These findings strengthen our earlier hypothesis that the commonality of observed dynamics are a consequence of how humans naturally ’do’ conflict, irrespective of the individual conflict’s specific origin, geographic location, ideology, and religious issues. Having established the quantitative power of our model, we use it to predict the duration of wars, and test out the consequences of different intervention strategies. We then turn to look at the connection with transnational ’maras’, street gangs, and online gangs which form around Internet role-playing games such as World of Warcraft.
8776 en Growing Sovereignty: Modeling the Shift from Indirect to Direct Rule Drawing on theories of historical sociology, we model the emergence of the territorial state in early modern Europe. Our modeling effort focuses on systems change with respect to the shift from indirect to direct rule. We first introduce a one-dimensional model that captures the tradeoff between organizational and geographic distances. In a second step, we present an agent-based model that features states with a varying number of organizational levels. This model explicitly represents causal mechanisms of conquest and internal state-building through organizational bypass processes. The computational findings confirm our hypothesis that technological change is sufficient to trigger the emergence of modern, direct state hierarchies. Our theoretical findings indicate that the historical transformation from indirect to direct rule presupposes a logistical, rather than the commonly assumed exponential, form of the loss-of-strength gradient.
8777 en Dynamics of Terrorist Groups The behavior of terrorist groups may seem highly strategic and thus largely contingent (unpredictable), however, by taking a comparative approach in considering data on terrorist attacks, we find that surprising patterns emerge. In this talk, Ill describe recent work on discovering and understanding the structure of terrorist attacks over the past 40 years, and in particular the regular behavior of terrorist organizations. These results shed new light on the origin of severe terrorist attacks and point to fundamental constraints on the dynamics of terrorism.
8778 en Policy Informatics for Complex Systems Mental models are inadequate for coping with crises in complex socioeconomic systems. Modern information technology can support evidencebased policies using simulations to synthesize data. Synthetic data provide a natural representation of situations and hypothetical outcomes, suitable for use by policy-makers. This talk will explore issues arising in the emerging science of policy informatics: distinctions between support for planning or response efforts; determining requirements for resolution, fidelity, precision, and accuracy in synthetic data; communicating between model developers and stakeholders which includes designing informative experiments and interpreting outcomes; and assessing adequacy of models. Examples will be drawn from practical experiencesnwith the novel H1N1 influenza.
8779 en Cooperation and Conflict in the Prisoner’s Dilemma and the Emergence of Norms According to Thomas Hobbes’ Leviathan, ”the life of man [is] solitary, poor, nasty, brutish, and short”, and it would need powerful social institutions to establish social order. In reality, however, social cooperation can also arise spontaneously, based on local interactions rather than centralized control. The self-organization of cooperative behavior is particularly puzzling for social dilemmas related to sharing natural resources or creating common goods. Such situations are often described by the prisoner’s dilemma. Here, we report the sudden outbreak of predominant cooperation in a noisy world dominated by selfishness and defection, when individuals imitate superior strategies and show success driven migration. In our model, individuals are unrelated, and do not inherit behavioral traits. They defect or cooperate selfishly when the opportunity arises, and they do not know how often they will interact or have interacted with someone else. Moreover, our individuals have no reputation mechanism to form friendship networks, nor do they have the option of voluntary interaction or costly punishment. Therefore, the outbreak of prevailing cooperation, when directed motion is integrated in a game-theoretical model, is remarkable, particularly when random strategy mutations and random relocations challenge the formation and survival of cooperation clusters. Finally, new results will be presented on the issue of conflict in the prisoner’s dilemma and on the emergence of norms, when group dynamical effects are taken into account.
8780 en Explaining and Forecasting the Psychological Component of Economic Activity We develop a methodology for estimating the parameters of dynamic opinion or expectation formation processes with social interactions. We study a simple stochastic framework of a collective process of opinion formation by a group of agents who face a binary decision problem. The aggregate dynamics of the individuals’ decisions can be analyzed via the stochastic process governing the ensemble average of choices. Numerical approximations to the transient density for this ensemble average allow the evaluation of the likelihood function on the base of discrete observations of the social dynamics. This generic approach can be used to estimate the parameters of various opinion formation processes from a variety of available aggregate data. Our applications include: (i) identification of interaction effects in a well-known business climate index as well as (ii) analysis of sentiment data from the German stock market. In both cases we find strong evidence of strong social interactions with the potential of generating abrupt swings in the average mood of respondents. In this way, the psychological component or the imprints of animal spirits in economic data can be identified.
8781 en Financially Constrained Business Fluctuations in an Evolving Network Economy We explore the properties of a credit network characterized by inside credit, i.e. credit relationships connecting downstream (DS) and upstream (US) firms, and outside credit, i.e. credit relationships connecting firms and banks. The structure of the network changes over time due to the preferred-partner choice rule. The net worth of DS firms turns out to be the driver of growth and fluctuations. US production, in fact, is determined by demand of intermediate inputs on the part of DS firms. The output of simulations shows that a business cycle at the macroeconomic level can develop as a consequence of the complex interaction of the heterogeneous financial conditions of the agents involved. In this context we can study the emergence of bankruptcy chains. We can alsonreproduce the main facts of firms’ demography: power law distribution of firms’ size and Laplace ditribution of growth rates.
8782 en Financial Regulation: An Attempt to Regulate Complexity The talk reviews the main components of the financial crisis of 2007-09 from a complexity perspective and argues that two decades of ideology driven deregulation, the surge of securitization, the spreading of OTCnderivatives and off balance sheet items, excessive leverage, collaterized debt obligations and structured investment vehicles led to the creation of a huge shadow banking system that became opaque and complex tonsuch a degree as to be totally unknowable. Such a system is impossible to regulate. As a result of massive government interventions, partial or complete nationalization, and a series of collapses, the system is in thenprocess of deleveraging, and a large body of new regulation is in the making. The talk will give a sketchy oversight of what is perhaps the most coherent set of proposals for the new regulation, due to the de Larosiere Committee. Finally, the idea of an adaptive regulatory regime will be put forward.
8783 en Self-Organization and Finite Size Effects in Agent Models for Financial Markets The deviation from a Random Walk behavior in financial time series have been identified as Stylized Facts (SF) and are common to all markets. The main ones are that fluctuations are much lager than those predicted from the standard economic theory (gaussian fluctuations), the clustering of volatility and a substantial non stationarity of all properties. Many Agent Based Models have been proposed to explain these phenomena and several are indeed able to reproduce some of them. However, the situation is still problematic becaus these models are typically rather complicated with various ad hoc assumptions. This has prevented a systhematic study of these effects. We have tried therefore to define a workable Agent based Model [1], which contains the essential elements, but in a mathematically simple and well defined framework. In addition we have considered some new important elements like the nonstationarity of the process with respect to the number of agents and the question of the self-organization. Namely why all markets evolve spontaneously towards the situation corresponding to the SF, considering that in all models this is restricted to a very narrow range of parameters. The SF are shown to correspond to finite size effects (with respect to time and to the number of agents N) which, however, can be active at different time scales. This implies that strict universality cannot be expected inndescribing these properties in terms of effecive critical exponents. The introduction of a threshold in the agents action (small price movements lead to no action) triggers the self-organization towards the intermittentnstate corresponding to the SF. From these studies the herding phenomenon seems to be a crucial one beyond the standard theory as a triggering element of bubbles and crashs which develop spontaneously without a cause-effect relation. The model can also be used backwards to derive the strategies of the agents from the price time series. Other applications are under consideration like the problem of finite liquidity and the possibility that the reference fundamental price is subject to large fluctuations if one cnsiders that all markets are linked into a large network [2].
8785 en Financial Bubbles, Real Estate Bubbles, Derivative Bubbles, and the Financial and Economic Crisis The financial crisis of 2008, which started with an initially well-defined epicenter focused on mortgage backed securities (MBS), has been cascading into a global economic recession, whose increasing severity and uncertain duration had led and is continuing to lead to massive losses and damage for billions of people. Heavy central bank interventions and government spending programs have been launched worldwide and especially in the USA and Europe, in the hope to unfreeze credit and boltster consumption. Here, I present evidence and articulate a general framework that allows one to diagnose the fundamental cause of the unfolding financial and economic crisis: the accumulation of several bubbles and their interplay and mutual reinforcement has led to an illusion of a perpetual money machine allowing financial institutions to extract wealth from an unsustainable artificial process. Taking stock of this diagnostic, I conclude that many of the intervention to address the so-called liquidity crisis and to encouragemore consumption are ill-advised and even dangerous, given the lack of precautionary reserves that have been unaccumulated in the good times and the huge liabilities. The most interesting presents times constitute unique opportunities but also great challenges, for which I offer a few recommendations.
8786 en Efficient Immunization Approaches to Avoid Epidemic Spreading We will show how methods based on statistical physics and complex networks approaches may help to predict the appearing of crises such as epidemics. These methods also suggest efficient immunization strategies to coop with such crises. The epidemics could occur in social systems as well as in communication networks such computers or cellphones. The methods are based on the percolation theory approach which is extended to complex networks to include more realistic scenarios, such as the limited time of epidemics or the dynamical nature of links. Questions such as how to identify the most crucial spreaders and giving a limited amount of immunization doses, how to prioritize the recipients? will be also discussed.
8787 en Planning for Pandemic Outbreaks with Large Scale Computational Models We present a class of computational epidemic models that integrate transportation and census data on the worldwide scale. The defined models allow the analysis of the impact of complex mobility networks on the behavior of emergent disease spreading and the general issue of the predictive power offered by computational approaches. In this framework it is possible to tackle foundational issues by using the particle network approach and provide new mathematical and computational tools for the study of large scale epidemics. We will focus the discussion on the analysis of invasion thresholds and the definition of epidemic pathways. Based on these results we present the Global Epidemic Modeler (GEM) computational platform that can be used in the analysis of intervention and mitigation policies for emerging disease outbreak.
8788 en Economic Fluctuations and Statistical Physics: Quantifying Extremely Rare and Much Less Rare Events Recent analysis of truly huge quantities of empirical data suggests that classic economic theories not only fail for a few outliers, but that there occur similar outliers of every possible size. In fact, if one analyzes only a small data set (say 104 data points), then outliers appear to occur as “rare events.” However, when we analyze orders of magnitude more data (108 data points!), we find orders of magnitude more outliers - so ignoring them is not a responsible option, and studying their properties becomes a realistic goal. We find that the statistical properties of these “outliers” are identical to the statistical properties of everyday fluctuations. For example, a histogram giving the number of fluctuations of a given magnitude x for fluctuations ranging in magnitude from everyday fluctuations to extremely rare fluctuations that occur with a probability of only 10?8 is a perfect straight line in a double-log plot. n  Two unifying principles that underlie much of the finance analysis we will present are scale invariance and universality [ R. N. Mantegna/HES, Introduction to Econophysics: Correlations & Complexity in Finance (Cambridge U. Press, 2000)]. Scale invariance is a property not about algebraic equations but rather about functional equations, which have as their solutions not numbers but rather functional forms - power laws.nThe key idea of universality is that the identical set of “scaling laws” hold across diverse markets, and over diverse time periods. n  We demonstrate the principles of scaling and universality by describing very recent unpublished work [HES/T. Preis/J. J. Schneider “New Laws Describing Trend Switching Processes in Financial Markets”, submitted]. For an intriguing variety of switching processes in nature, the underlying complex system abruptly changes at a specific “phase transition” point from one state to another in a highly discontinuous fashion. Examples of phase transitions range from magnetism in statistical physics to physiology and macroscopic social phenomena. Financial market fluctuations are characterized by many abrupt switchings on very short time scales from increasing “microtrends” to decreasing “microtrends”—and vice versa. We ask whether these ubiquitous switching processes have quantifiable features analogous to those present in phase transitions, and find striking scale-free behavior of the time intervals between transactions both before and after the switching occurs. We interpret our findings as being consistent with time-dependent collective behavior of financial market participants. We test the possible universality of our result by performing a parallel analysis of transaction volume fluctuations.
8790 en Mechanisms of Systemic Risk: Contagion, Reinforcement, Redistribution The term ’systemic risk’ commonly denotes the risk that a whole system consisting of many interacting agents fails. It is a macroscopic property which emerges from the nonlinear interactions of agents. In fact, ’systemicnrisk’ already implies that the failure of the system cannot be fully explained by the failure of a single agent. Instead, one has to understand how such singular failures are able to spread through the whole system, affecting other agents. Here, in addition to network topology, dynamic mechanisms such as contagion (similar to epidemic processes or herding behavior), reinforcement (of prevailing trends), and redistribution (e.g. of load, stress, or debt) play a considerable role. The talk aims at categorizing some of the existing models in a common framework, first, and discussing a specific model of financial networks, afterwards, to elucidate the critical conditions for the breakdown of a system.
8791 en Two Models of Collective Firm Bankruptcies Two models of collective firm bankruptcies will be presented. The first one uses the Potts spin glass approach for firms rating evolution where two sources of defaults are taken into account: individual dynamics of economic development and ordering interactions between firms. We show that such a defined model leads to a phase transition, which results in collective defaults. In the case when the individual firm dynamics favors dumping of rating changes, there is an optimal strength of firms interactions from the risk point of view. For small interaction strength parameters there are many independent bankruptcies of individual companies. For large parameters there are giant collective defaults of firm clusters.n  The second model represents defaults of companies in multi-stage supply chain networks. We introduced a modified approach that represents in more details the real economic environment in which firms are operating.nWe focused on the influence of local processes on the global economic behaviour of the system and studied how the proposed modifications change the general properties of the model. To realistically simulate the economic environment of companies, we introduced the following features to the model: evolution of a supply chain network with the reconfiguration of links, price dispersion and the dynamics of prices and costs of production. At the certain point of the system’s evolution, the meta-stable structures of the network occur. As a result of the dynamics of prices and costs of production, we observed both the emergence of highly profitable supply chains with the high market share and the avalanches of bankruptcies.
8792 en Prediction and its Limits in Socio-Economic Systems Science recognizes a number of limitations to its power to predict the future of physical systems. Such limits typically stem from dynamical chaos or the impracticability of dealing with large numbers of variables. In this talk, I will review recent work pertaining to the limits of predictability in complex systems, and to novel features arising in systems involving adaptive agents, such as humans. The ”socio-physics” approach often elicits the criticism (from social scientists) that it ignores the ”reflexive” character of social reality – that valid insights into individual human behaviour, or social patterns, may quickly cause people to alter their behaviour, destroying the validity of those insights. I will argue that reflexitivity is a real phenomenon, but that it’s not a fundamentalnbarrier to socio-economic prediction, for two reasons. First, people under observation often become habituated to their environment and exhibit stable behaviour. Second, science is gaining an ability to model the reflexive process itself, by learning to model the process of human thinking. I’ll conclude by reviewing studies in political science which suggest that a key reflexive feedback driving many of the most recent financial crises has been the evolution of ”triangles” of power among government agencies, regulators and special interests, such as the financial industry. Any successful effort to prevent such crises may need to exert control over such feedbacks, and produce a regulatory framework that is in some sense ”lobby proof.”
8793 en Travel and Social Capital: Some Empirical Evidence The presentation will present a theoretical and empirical discussion of the links between the generalized costs of travel, the use of space and the (spatial) structure of social networks of the population.n  After a brief historical overview of the changes in the generalized costs of travel and associated time-space compression of the industrialized countries, the question will be discussed what impact these changes should have on the structure of the social networks with respect to the size, overlap, spatial location.n  The second part of the talk focuses on the empirical work done so far to provide empirical evidence for the theoretical expectations formulated above. The challenges inherent in the survey work will presented, as well as the key empirical results obtained so far.
8794 en Microscopic Simulation of Tsunami-Related Evacuation of the City of Padang The evacuation of whole cities or even regions is an important problem, as demonstrated by recent events such as the evacuation of Houston in the case of Hurricane Rita, or the evacuation of coastal cities in the case of tsunamis. A robust and flexible simulation framework for such large scale disasters helps to predict the evacuation process. Furthermore, it is possible to recognize bottlenecks in advance, so that an elimination ofnthose bottlenecks is possible. This should lead to a better preparedness for cities or regions that face a high risk of natural disasters. Existing methods are either geared towards smaller problems (e.g. Cellular Automatantechniques or methods based on differential equations) or are not microscopic (e.g. methods based on dynamic traffic assignment). Our work uses a technique that is both microscopic and capable to process large problems. The simulation is applied to the Indonesian city of Padang. The city faces a high risk of being inundated by an earth quake triggered tsunami.
8795 en Macroscopic Modeling of Traffic in Congested Cities: Empirical Evidence, Analytical Derivations and Control Applications Various theories have been proposed to describe vehicular traffic movement in cities on an aggregate level. They fall short to create a macroscopic model with variable inputs and outputs that could describe a rush hour dynamically. This work shows that a Macroscopic Fundamental Diagram (MFD) relating production (the product of average flow and network length) and accumulation (the product of average density and network length) exists for neighborhoods of cities in the order of 5 ? 10 km2. It also demonstrates that conditional on accumulation large networks behave predictably and independently of their Origin-Destination tables. These results are based on analysis using simulation of large scale city networks and real data from urban metropolitan areas. Regularity conditions under which an MFD exists for different types of networks are proposed and tested. Further analysis of real data shows that an MFD is not a universal recipe that can describe any type of large network. For example, MFDs for spatially inhomogeneous networks or non-redundant networks, like freeway traffic systems, are highly scattered. An analytical model based on Variational Theory describes the connection between network structure and a network’s MFD for urban neighborhoods controllednat least in part by traffic signals. The MFD is applied to develop control strategies based on neighborhood accumulation and speeds and improve accessibility without the uncertainty inherent in forecast-based approaches.
8796 en Statistical Physics and Social Systems: A Critical Perspective (The Case of Urban Mobility) The application of Statistical Physics to social systems is mainly related to looking for macroscopic laws that are derived from experimental data average in time or space under the assumption that the averaged systemnis in a stationary state. The final goal is to correlate the statistical laws to the microscopic properties of the system: for example to understand the nature of the microscopic interactions or to point out the existence of interaction networks. However the probability theory suggests the existence of few classes of stationary distributions in the thermodynamics limit, so that the question is if a statistical physics approach could be ablento point out the complex nature of the social systems. We have analyzed a GPS data base for individual mobility (2% of individual vehicles are monitored in Italy for insurance reasons) to look for statistical laws on path length distributions, elapsed time in the different activities related to mobility, flux distribution in the road network and frequency rank distribution for the individual destinations. We show as simple generic assumptions on the microscopic behavior could explain the existence of stationary macroscopic laws. Our conclusion is that the understanding of the system complexity requires dynamical data base for the microscopic behavior on a large scale time-dependent environment that allows to study the evolution of the transient states. Theoretical results on long range interacting systems suggest that the transient states may provide much more information of the microscopic interaction nature. Concerning human mobility the GPS data base will be improved in the next future by enhancing the recording time sampling and by increasing the sample size.
8797 en Spiraling Toward Complete Markets and Financial Instability The proliferation of financial instruments provides more opportunities to hedge risks, reducing transaction costs and making markets more complete. These predictions sharply contrasts with recent experience, where asymmetric information and imperfect competition have played a major role in turning expanding credit derivative markets into ”financial weapons of mass destruction”.n  Here I argue that the escalation of market imperfections originates from the changes which take place in the nature of the market equilibria, when the repertoire of financial instruments expands. This is done in the limit of a large random economy, where a set of consumers invests in financial instruments engineered by banks, in order to optimize their future consumption.n  I show that, even in the ideal case of perfect competition, where full information is available to all market participants, as markets approach completeness and transaction costs vanish, the equilibrium develops a marked vulnerability (or susceptibility) to market imperfections. Therefore, the onset of instability does not require large shocks, but it rather arises from the intrinsic nature of the equilibrium.n  One particularly devastating effect is that, replicating portfolios used by banks to hedge new instruments, require trading volumes, within the financial sector, which diverge as the market approaches completeness. Such interbank market itself develops a divergent susceptibility, as the theoretical limit of complete markets is approached.n  A similar approach shows that the expansion of derivative markets generates instability and large movements in underlying markets. These results suggest that the proliferation of financial instrumentsnexacerbates the effects of market imperfections. In order to prevent an escalation of perverse effects, markets may necessitate institutional structures which are more and more conspicuous as they expand.
8798 en The Role of Tie Strength in the Cohesion of the Society: A Tribute to Mark Granovetter Electronic databases, from phone to emails logs, currently provide detailed records of human communication patterns, offering novel avenues to map and explore the structure of social and communication networks. We examine the communication patterns of millions of mobile phone users, allowing us to simultaneously study the local and the global structure of a society-wide communication network. We observe a coupling between interaction strengths and the networks local structure, and conclude that social networks are robust to the removal of the strong ties, but fall apart following a phase transition if the weak ties are removed. We show that this coupling significantly slows the diffusion process, resulting in dynamic trapping of information in communities, and find that when it comes to information diffusion, weak and strong ties are for different reasons both simultaneously ineffective.n  Using the aggregate records of a mobile phone service provider about private voice calls of more than 4 million users we construct over 18 weeks a weighted network of interactions where the tie strength is taken proportional to the total duration of the calls. We introduce a measure of the link overlap and show that nodes (i.e., people) with strong links have a large friendship overlap. This way we prove for the first time the Granovetter hypothesis about the strength of weak ties at a societal scale. The network has a strongly modular structure with highly wired communities with strong ties, which are connected by weak links. n  A global consequence of this structure is that the network connectedness is resilient against removal of strong links while it falls apart whenthe weak links are cancelled. n  The intimate relationship between link weights and topology has strong influence on the dynamic properties of the network. Using the simplest diffusive spreading dynamics we demonstrated that the probability of getting new information (or, alternatively, getting infected) via a strong or weak link is low, in most cases links with an intermediate strength play the role of the transmitter. n  In order to understand the peculiar interplay between topology and link weights we constructed a model of the social network. The model has strong simplifications and is based on elementary steps of link formation and tie strengthening. We deal with a constant number of nodes. In order to reach stationarity time to time a node is eliminated and, at the same time, a new one without any connections is born. Links are created either at random (with very low probability), or using already existing links (friends of friends get friends). An important element of the model is that whenever a link is used, there is a strengthening effect, described by a parameter ?. The resulting network describes well the qualitative features of the call network, including the strength of the weak ties and the trapping effect.
8799 en The Weave of Social Life: How Social Interactions Shape the Individual One of the deepest problems in the social sciences concerns the causal impact of society, that is, of properties of the group, on the properties of individuals. This problem arises because individuals affect the properties of groups and vice versa such that it is very difficult to get at causality. Here we take advantage of the possibility to affect the properties of internet communities to show that groups with a higher density of social interactions render their members generally more altruistic and trusting towards anonymous strangers. Moreover, a higher density of social interactions also causes a boost in trust towards those who reciprocate favours while it diminishes trust towards those who fail to reciprocate, thus generating a much stronger implicit punishment for untrustworthy individuals. Finally, increased social contact also enhances the strategic sophistication of individuals and raises the prevalence of Machiavellian strategies. These results indicate that the density of social interactions has a deep impact on individuals’ preferences, beliefs, and behaviors, lending support to sociological views of society.
8800 en How Do Economies Grow, How Do They Interract, How Do They Fall and How Do They Recover? The stochastic spatially extended generalized Lokta-Volterra approach was introduced a few years ago and was applied using analytical (field theory and statistical mechanics methods), numerically (computer experiments) and empirical (gathering and processing real data) techniques to a wide range of natural, biological, economic and social systems.n  In this talk I will describe its recent application to the study of interactions between economic sectors, countries and blocks. The theory predicts robustly in a very wide range of conditions systematic regularities in the growth rates evolution of various subsystems.n  The after-shocks J-curve phenomenon (economic decay and rebound induced by the emergence of singular growth centers) is revisited and more empirical support is given to the theory. In particular we show that the data support to the connection between the economic minimum and the crossover of the new emergent leading sector with the old decaying one. We describe the Growth Alignment Effect (GAE), its theoretical basis and demonstrate it empirically for numerous cases in the international and intranational economies. The GAE is the concept that in steady state the growth rates of the GDP per capita of the various system components align. We differentiate the GAE predictions from the usual convergence or divergence conceptual framework that dominated in economic studies until now.
8801 en Robustness of Social Networks Networks typically cease to be operational when they fall apart in disconnected pieces. This can be desired as in the case of criminal networks or should be avoided for instance in the case of communication systems.nDestruction can happen randomly of due to a malicious attack. I will present various strategies of optimizing the robustness of networks preserving their degree distribution. A novel topology emerges. Applications to power networks, botnets, road systems and brain models will be discussed.
8804 en Tutorial about Sociodynamics Sociodynamics intends to provide an integrated strategy for mathematical modelling of collective dynamic processes in the human society. The approach is far more general than catastrophe theory: It comprises the full dynamics of key-variables, namely their chance behaviour as well as their quasi-deterministic evolution.It connects the bottom-up and top-down interaction between microlevel and macrolevel of the social system. The design principles start from the elementary dynamics of the key-variables in terms of socially interpretable probabilistic transition rates and end in evolution equations for them: The master equation for the probability distribution over key-variables comprises mean behaviour and fluctuations as well. The quasi-meanvalue-equations derivable from the master equation describe the mean evolution only.
8805 en How Crime Bursts Can Occur with Minor Changes in Retribution Policy We model a system of interacting agents characterized by a given wealth and a certain criminal propensity, measured by an honesty coefficient. This honesty is related to intrinsic factors, like moral barriers, and extrinsic ones, as the risk of being imprisoned if committing an offense. In the simulation the honesty level of the agents is variable, and a function of the level of punition, on one hand, and on the contact with other agents (learning or contagion effect) on the other hand. The number of crimes per habitant is measured as a function of the probability of being caught. A sharp phase transition is observed as a function of the probability of punishment. That means that once criminality has attained a high level, the probability of retribution must considerably increase in order to come back to a state of low criminality. Also, some precursor signals are observed that indicate possible bursts of crime activity. We also analyze other consequences of criminality as the growth of the economy, the inequality in the wealth distribution (the Gini coefficient) and other relevant quantities under different scenarios of criminal activity and probabilities of apprehension.
8806 en Commitment as Unrewarded Behaviour The purpose of my presentation on commitment is to show that in its core resides unrewarded behaviour, which is rare but essential to the conduct of social communities. Commitment is defined as an unequivocal behaviour of delivery, carried out under the worst conditions, when communities are unable to reward it. Unrewarded commitment is explored among 316 respondents. The newly defined commitment is mapped against conventional scales of commitment, and measures of perceived organizational power, perceived employment alternatives, personal values. Two types of unrewarded behaviour are identified: unrewarded commitment, which is not unrelated to traditional measures of commitment, such as affective and normative commitment, and extreme or Sisyphean unrewarded commitment, which displays organizational behaviour even with no affective, normative or instrumental attachment to the community.
8808 en Cooperation and Conflict in Wikipedia We present a model of the collaborative process of document authoring that takes place in the free online encyclopedia Wikipedia. We consider the process of editing of a Wikipedia page by a group of agents with different opinions (points of view on the topic of the page), which are continuous variables, coupled with a time-inhomogeneous process for the WWW browsing to the page, in which motivational factors for the participation to the collaborative process affect the rate of visits to the page. The model of editing takes inspiration from response models of biological neurons. Social interactions between agents are thus mediatednby the content of the page. In this original context of opinion dynamics, editors and regular users of Wikipedia are seen as actors having different objectives about what the point of view expressed by a Wikipedia page should be like, and modify it using a simple greedy heuristic. Interesting phenomena like the emergence of a shared, neutral point view may then be cast in the light of this dynamics of opinions. When the model of opinion dynamics is considered in isolation, simulations show that agents with opinions and rates of activity that are fixed in time converge on a pages opinion that reflect, on average, a shared point of view between.
8809 en Measuring the Response of a Social System We study the relaxation response of a social system after endogenous and exogenous bursts of activity using the time series of daily views for nearly 5 million videos on YouTube. We find that most activity can be described accurately as a Poisson process. However, we also find hundreds of thousands of examples in which a burst of activity is followed by an ubiquitous power-law relaxation governing the timing of views. We find that these relaxation exponents cluster into three distinct classes and allow for the classification of collective human dynamics. This is consistent with an epidemic model on a social network containing two ingredients: a power-law distribution of waiting times between cause and action and an epidemic cascade of actions becoming the cause ofnfuture actions. This work is a conceptual extension of the fluctuation dissipation theorem to social systems and provides a unique framework for the investigation of timing in complex systems.
8810 en Nature’s Solution to the Problem of Biological Logistics The ability of cells to survive and participate in a community, such as the human body, requires a logistical network that distributes nutrients, cellular contents, and information at biologically reasonable time-scales. How does a robust, adaptable system emerge from the sum of individual hard-wired molecular agents operating in a noisy environment? For example, motor proteins deliver cargo along intracellular filaments to appropriate sites in a network of molecular compartments whose connectivity is slowly becoming elucidated. However, surprisingly little is known about what these motor proteins do, specifically, (1) where they go in cells, (2) what they transport, and (3) how their activity is regulated. To address these basic questions, we modified motor proteins so they could be visualized in live cells and recovered with their physical binding partners. Individual motor proteins were also removed from cells to determine the overall effect on different cellular trafficking pathways. We found that different motor proteins are targeted to unique sub-cellular compartments and identified regulatory proteins resident in these compartments that could serve as molecular postal codes, or otherwise regulate the cycle of cargo binding and release. Overall, this provides an example of a biological solution to managing complex systems using a limited number of components.
8811 en Do some Value-at-Risk Models Provoke Financial Market Destabilization? - An Agent-Based Financial Market Perspective The aim of this paper is to explore the impact of different Value-at-Risk (VaR) models on the stability of financial markets. Based on a numerical analysis, we test how simple and more sophisticated Value-at-Risknmodels affect financial market stability. This is important to implement effective regulations to prevent financial market instability in advance. The Basel Committee of Banking and Supervision (BCBS) does not stipulate that banks use a special type of VaR model. Therefore, in practice, many banks use methods with quite simple assumptions. Testing the efficiency and reliability of different VaR models with different underlying assumptions was and still is an important strand of research. Here, mostly the accuracy of estimating a special quantile is exploited. In this study, we explore the adequacy of VaR models from another perspective. We adjust a heterogeneous agent model by integrating regulations of Basel II concerning market risk. For this purpose, we use the financial market model by Lux/Marchesi (1999/2000) that can replicate stylized facts of financial markets quite well. First results indicate that the formula for calculating the level of regulatory capital for market risk prescribed by the BCBS prohibits more market stability by the use of more sophisticated models.
8812 en Clustering Dynamics Through an Emerging Market Crash in the Global Crisis 2007-2009 We investigate the dynamics of stock clustering in the Johannesburg Stock Exchange (JSE), an emerging market, through the financial market crash of 2008. In particular we apply the fully unsupervised parameter free data clustering technique pioneered by Giada and Marsili (2002) to investigate the changing correlation structure of stocks, as well as clustering in daily market-wide activity, in a crisis. We compare our findings with an identical analysis of the London Stock Exchange through the same crisis period.
8813 en Early Signs of Financial Crises For most diseases, it is always better to medically intervene earlier compared to later. This is because treatment at an early stage is generally more effective, and less expensive. The same is probably true for economies and financial markets. In the current global financial crisis, we have seen billions of dollars sunk into relief and stimulus packages, with hardly any positive result to show for the effort. The reason is clear: these intervention measures are too late. To implement more effective, and less costly economic and fiscal policies, it is important to detect the onset of a financial crisis early. At the same time, we do not want excessive reactions, when the market has merely caught a ’cold’. In this talk, I will describe recent work, based on the statistical segmentation and clustering analysis of financial time series data, that points to characteristic early signs prior to financial crises, characteristic early signs prior to a true recovery, and the characteristic time scales involved for bothnprocesses. By looking into a period that covers both the current crisis, as well as the most recent past crisis, we also hope to learn lessons on which intervention measures are effective, and which intervention measuresnare not.
8817 en Contagion of Norm Breaking vs the Number of Righteous People The Norm Game introduced by Axelrod is investigated by computer simulations carried out for agents distributed in a random network. The agents are labeled as sinners or punishers after their first decision. The stationary state shows a bistable behaviour: all become sinners or all become punishers [1,2]. Here we show how this bistability changes when some amount of agents always break the norm or always punish.
8832 en Welcome Speech at the MLSS 2009 
8833 en Kernel Methods and Support Vector Machines Kernel methods have become a standard tool for pattern analysis during the last fifteen years since the introduction of support vector machines. We will introduce the key ideas and indicate how this approach to pattern analysis enables a relatively easy plug and play application of different tools. The problem of choosing and designing a kernel for specific types of data will also be considered and an overview of different kernels will be given.
8834 en Geometric Inference for Probability Distribution Data often comes in the form of a point cloud sampled from an unknown compact subset of Euclidean space. The general goal of geometric inference is then to recover geometric and topological features (Betti numbers, curvatures,...) of this subset from the approximating point cloud data. In recent years, it appeared that the study of distance functions allows to address many of these questions successfully. However, one of the main limitations of this framework is that it does not cope well with outliers nor with background noise. In this talk, we will show how to extend the framework of distance functions to overcome this problem. Replacing compact subsets by measures, we will introduce a notion of distance function to a probability distribution.nThese functions share many properties with classical distance functions, which makes them suitable for inference purposes. In particular, by considering appropriate level sets of these distance functions, it is possible to associate in a robust way topological and geometric features to a probability measure. If time permits, we will also mention a few other potential applications of this framework.
8835 en PAC-Bayes Analysis: Background and Applications 
8836 en Cut Locus and Topology from Point Data A cut locus of a point p in a compact Riemannian manifold M is defined as the set of points where minimizing geodesics issued from p stop being minimizing. It is known that a cut locus contains most of the topological information of M. Our goal is to utilize this property of cut loci to decipher the topology of M from a point sample. Recently it has been shown that Rips complexes can be built from a point sample P of M systematically to compute the Betti numbers, the rank of the homology groups of M. Rips complexes can be computed easily and therefore are favored over others such as restricted Delaunay, alpha, Cech, and witness complex. However, the sizes of the Rips complexes tend to be large. Since the dimension of a cut locus is lower than that of the manifold M, a subsample of P approximating the cut locus is usually much smaller in size and hence admits a relatively small Rips complex. nIn this talk we explore the above approach for point data sampled from surfaces embedded in any high dimensional Euclidean space. We present an algorithm that computes a subsample P' of a sample P of a 2-manifold where P' approximates a cut locus. Empirical results show that the first Betti number of M can be computed from the Rips complexes built on these subsamples. The sizes of these Rips complexes are much smaller than the one built on the original sample of M. nn
8837 en Graphical Models and Applications Compressed sensing is a recent set of mathematical results showing that sparse signals can be exactly reconstructed from a small number of linear measurements. Interestingly, for ideal sparse signals with no measurement noise, random measurements allow perfect reconstruction while measurements based on principal component analysis (PCA) or independent component analysis (ICA) do not. At the same time, for other signal and noise distributions, PCA and ICA can significantly outperform random projections in terms of enabling reconstruction from a small number of measurements. In this paper we ask: given a training set typical of the signals we wish to measure, what are the optimal set of linear projections for compressed sensing? We show that the optimal projections are in general not the principal components nor the independent components of the data, but rather a seemingly novel set of projections that capture what is still uncertain about the signal, given the training set. We also show that the projections onto the learned uncertain components may far outperform random projections. This is particularly true in the case of natural images, where random projections have vanishingly small signal to noise ratio as the number of pixels becomes large. Joint work with Hyun-Sung Chang and Bill Freeman. nI will give a brief introduction to questions of representation, learning and inference in probabilistic graphical models and illustrate these ideas in applications from our own work in computational biology and computer vision.
8838 en Euler Calculus and Topological Data Management This talk covers the basic of an integral calculus based on Euler characteristic, and its utility in data problems, particularly in aggregation of redundant data and inverse problems over networks. This calculus is a blend of integral-geometric and sheaf theoretic techniques, and leads to surprisingly practical algorithms and computations. Qualitative versions of integral transforms for signal processing will be stressed.
8839 en Seeking Interpretable Models for High Dimensional Data Extracting useful information from high-dimensional data is the focus of today's statistical research and practice. After broad success of statistical machine learning on prediction through regularization, interpretability is gaining attention and sparsity has been used as its proxy. With the virtues of both regularization and sparsity, Lasso (L1 penalized L2 minimization) has been very popular recently. In this talk, I would like to discuss the theory and pratcice of sparse modeling. First, I will give an overview of recent research on sparsity and explain what useful insights have been learned from theoretical analyses of Lasso. Second, I will present collaborative research with the Gallant Lab at Berkeley on building sparse models (linear, nonlinear, and graphical) that describe fMRI responses in primary visual cortex area V1 to natural images.
8840 en Learning Dictionaries for Image Analysis and Sensing Sparse representations have recently drawn much attention from the signal processing and learning communities. The basic underlying model consist of considering that natural images, or signals in general, admit a sparse decomposition in some redundant dictionary. This means that we can find a linear combination of a few atoms from the dictionary that lead to an efficient representation of the original signal. Recent results have shown that learning overcomplete non-parametric dictionaries for image representations, instead of using off-the-shelf ones, significantly improves numerous image and video processing tasks. nIn this talk, I will first present our results on learning multiscale overcomplete dictionaries for color image and video restoration. I will present the framework and provide numerous examples showing state-of-the-art results. I will then briefly show how to extend this to image classification, deriving energies and optimization procedures that lead to learning non-parametric dictionaries for sparse representations optimized for classification. I will conclude by showing results on the extension of this to sensing and the learning of incoherent dictionaries. The work I present in this talk is the result of great collaborations with J. Mairal (ENS, Paris), F. Rodriguez (UofM/Spain), J. Martin-Duarte (UofM/Kodak), I. Ramirez (UofM), F. Lecumberry (UofM), F. Bach (ENS, Paris), M. Elad (Technion, Israel), J. Ponce (ENS, Paris), and A. Zisserman (ENS/Oxford).
8842 en Examining the Relative Influence of Familial, Genetic, and Environmental Covariate Information in Flexible Risk Models We present a novel method for examining the relative influence of familial, genetic and environmental covariate information in flexible nonparametric risk models. Our goal is investigating the relative importance of these three sources of information as they are associated with a particular outcome. To that end, we developed a method for incorporating arbitrary pedigree information in a smoothing spline ANOVA (SS-ANOVA) model. By expressing pedigree data as a positive semidefinite kernel matrix, the SS-ANOVA model is able to estimate a log-odds ratio as a multicomponent function of several variables: one or more functional components representing information from environmental covariates and/or genetic marker data and another representing pedigree relationships. We report a case study on models for retinal pigmentary abnormalities in the Beaver Dam Eye Study (BDES). Our model verifies known facts about the epidemiology of this eye lesion - found in eyes with early age-related macular degeneration (AMD) - and shows significantly increased predictive ability in models that include all three of the genetic, environmental and familial data sources. The case study also shows that models that contain only two of these data sources, that is, pedigree-environmental covariates or pedigree-genetic markers, or environmental covariates-genetic markers, have comparable predictive ability, while less than the model with all three. This result is consistent with the notions that genetic marker data encodes - at least partly - pedigree data, and that familial correlations encode shared environment data as well.
8843 en Vision and Hodge Theory A general mathematical Hodge theory will be presented together with its relationship to spaces of images.
8844 en Learning Deformable Models It is widely recognized that the fundamental building block in high level computer vision is the deformable template, which represents realizations of an object class in the image as noisy geometric instantiations of an underlying model. The instantiations typically come from a subset of some group centered at the identity which act on the model or template. Thus in contrast to some machine learning applications where one tries to discover some unspecified manifold structure, here it is entirely determined by the group action and the model. Given a choice of group action and family of template models a major challenge is to use a sample of images of the object to estimate the model and the distribution on the group. The primary obstacle is that the instantiations or group elements that produced each image are unobserved. I will describe a general formulation of this problem and then show some practical applications to object detection and recognition.
8845 en Statistical Classification and Cluster Processes After an introduction to the notion of an exchangeable random partition, we continue with a more detailed discussion of the Ewens process and some of its antecedents. The concept of an exchangeable cluster process will be described, the main example being the Gauss-Ewens process. Some applications of cluster processes will be discussed, including problems of classification or supervised learning, and cluster analysis (unsupervised learning). A second type of probabilistic model based on point processes is also described. By contrast, which the Gauss-Ewes cluster process, the domain associated with each class is more diffuse and not localized in the feature space. For both models, the classification problem is interpreted as the problem of computing the predictive distribution for the class of a new object having a given feature vector. In one case, this is a conditional distribution given the observed features, in the other a Papangelou conditional intensity.
8846 en Theory, Methods and Applications of Active Learning Traditional approaches to machine learning and statistical inference are passive, in the sense that all data are collected prior to analysis in a non-adaptive fashion. One can envision, however more active strategies in which information gleaned from previously collected data is used to guide the selection of new data. This talk discusses the emerging theory of such "active learning" methods. I will show that feedback between data analysis and data collection can be crucial for effective learning and inference. The talk will describe two active learning problems. First, I will consider binary-valued prediction (classification) problems, for which the prediction errors of passive learning methods can be exponentially larger than those of active learning. Second, I will discuss the role of active learning in the recovery of sparse vectors in noise. I will show that certain weak, sparse patterns are imperceptible from passive measurements, but can be recovered perfectly using selective sensing.
8847 en Sparse Representations from Inverse Problems to Pattern Recognition Sparse representations are at the core of many low-level signal processing procedures and are used by most pattern recognition algorithms to reduce the dimension of the search space. Structuring sparse representations fro pattern recognition applications requires taking into account invariants relatively to physical deformations such as rotation scaling or illumination. Sparsity, invariants and stability are conflicting requirements which is a source of open problems. Structured sparse representations with locally linear vector spaces are introduced for super-resolution inverse problems and pattern recognition.
8848 en Optimization Algorithms in Support Vector Machines This talk presents techniques for nonstationarity detection in the context of speech and audio waveforms, with broad application to any class of time series that exhibits locally stationary behavior. Many such waveforms, in particular information-carrying natural sound signals, exhibit a degree of controlled nonstationarity, and are often well modeled as slowly time-varying systems. The talk first describes the basic concepts of such systems and their analysis via local Fourier methods. Parametric approaches appropriate for speech are then introduced by way of time-varying autoregressive models, along with nonparametric approaches based on variation of time-localized estimates of the power spectral density of an observed random process, along with an efficient offline bootstrap procedure based on the Wold representation. Several real-world examples are given.
8849 en Unsupervised Learning for Stereo Vision We consider the problem of learning to estimate depth from stereo image pairs. This can be formulated as unsupervised learning - the training pairs are not labeled with depth. We have formulated an algorithm which maximizes conditional likelihood the left image given right image in a model that involves latent information (depth). This unsupervised learning algorithm implicitly trains shape from texture and shape from shading monocular depth cues. The talk will present pragmatic results in the stereo vision problem as well as a general formulation of models and methods for maximizing conditional likelihood in a latent variable model where we wish to interpret the latent information as "labels".
8851 en Theory and Applications of Boosting Boosting is a general method for producing a very accurate classification rule by combining rough and moderately inaccurate "rules of thumb". While rooted in a theoretical framework of machine learning, boosting has been found to perform quite well empirically. This tutorial will introduce the boosting algorithm AdaBoost, and explain the underlying theory of boosting, including explanations that have been given as to why boosting often does not suffer from overfitting, as well as some of the myriad other theoretical points of view that have been taken on this algorithm. Some practical applications and extensions of boosting will also be described.
8852 en Generative Models for Image Analysis A probabilistic grammar for the grouping and labeling of parts and objects, when taken together with pose and part-dependent appearance models, constitutes a generative scene model and a Bayesian framework for image analysis. To the extent that the generative model generates features, as opposed to pixel intensities, the inverse or posterior distribution on interpretations given images is based on incomplete information; feature vectors are generally insufficient to recover the original intensities. I will argue for fully generative scene models, meaning models that in principle generate actual digital pictures. I will outline an approach to the construction of fully generative models through an extension of context-sensitive grammars and a re-formulation of the popular template models for image fragments. Mostly I will focus on the problem of learning template models from image data. Since the model is fully specified (generative), at the pixel level, the templates can be learned by maximum likelihood. A training set of eyes, for example, yields an ensemble of left and right eyes, of familiar and natural character, but not actually coming from any particular individuals in the training set. The upshot is a mixture distribution on image patches, consisting of a set of templates and a set of conditional patch distributions - one for each template. One way to test the model is to examine samples. I will show how to sample from the mixture distribution and I will show sample sets of eyes, mouths, and generic background. Another way to test the model is to use it for detection, recognition, or classification. I will show the results of a test on ethnic classification based on the eye region of faces.
8853 en On Surrogate Loss Functions, f-Divergences and Decentralized Detection In 1951, David Blackwell published a seminal paper - widely cited in economics - in which a link was established between the risk based on 0-1 loss and a class of functionals known as f-divergences. The latter functionals have since come to play an important role in several areas of signal processing and information theory, including decentralized detection. Yet their role in these fields has largely been heuristic. We show that an extension of Blackwell´s programme provides a solid foundation for the use of f-divergences in decentralized detection, as well as in more general problems of experimental design. Our extension is based on a connection between f-divergences and the class of so-called surrogate loss funcions - computationally-inspired upper bounds on 0-1 loss that have become central in the machine learning literature on classification. (Joint work with XuanLong Nguyen and Martin Wainwright.)
8854 en Similarity-Based Classifiers: Problems and Solutions Similarity-based learning assumes one is given similarities between samples to learn from, and can be considered a special case of graph-based learning where the graph is given and fully-connected. Such problems arise frequently in computer vision, bioinformatics, and problems involving human judgment. We will review the field of similarity-based classification and describe the main problems encountered in adapting standard algorithms for this problem, including different approaches to approximating indefinite similarities by kernels. We will motivate why local methods lessen the indefinite similarity problem, and show that a kernelized linear interpolation and local kernel ridge regression can be profitably applied to such similarity-based classification problems by framing them as weighted nearest-neighbor classifiers. Eight real datasets will be used to compare state-of-the-art methods and illustrate the open challenges in this field. nn
8855 en What Do Unique Games, Structural Biology and the Low-Rank Matrix Completion Problem Have In Common We will formulate several data-driven applications as MAX2LIN and d-to-1 games, and show how to (approximately) solve them using efficient spectral and semidefinite program relaxations. The relaxations perform incredibly well in the presence of a large number of outlier measurements that cannot be satisfied. We use random matrix theory to prove that the algorithms almost achieve the information theoretic Shannon bound. The underlying group structure of the different applications (like SO(2), SO(3), GL(n), etc.) is heavily exploited. Applications include: cryo-electron microscopy and NMR spectroscopy for 3D protein structuring, low-rank matrix completion, clock synchronization, and surface reconstruction in computer vision and optics. Partly joint with Yoel Shkolnisky, Ronald Coifman and Fred Sigworth (Yale); Mihai Cucuringu and Yaron Lipman (Princeton); and Yosi Keller (Bar Ilan).
8883 en Bounding Excess Risk in Machine Learning We will discuss a general approach to the problem of bounding the excess risk of learning algorithms based on empirical risk minimization (possibly penalized). This approach has been developed in the recent years by several authors (among others: Massart; Bartlett, Bousquet and Mendelson; Koltchinskii). It is based on powerful concentration inequalities due to Talagrand as well as on a variety of tools of empirical processes theory (comparison inequalities, entropy and generic chaining bounds on Gaussian, empirical and Rademacher processes, etc.). It provides a way to obtain sharp excess risk bounds in a number of problems such as regression, density estimation and classification and for many different classes of learning methods (kernel machines, ensemble methods, sparse recovery). It also provides a general way to construct sharp data dependent bounds on excess risk that can be used in model selection and adaptation problems.
8891 en OSS Tools to Support Collaborative Mobile Work Practices This lecture aims at better understanding the way currently mobile professionals work remotely in collaboration with others both within and beyond the company and matching these current ways of working with the most appropriate open source software (OSS) collaboration tools. This is achieved through addressing the current state of art in the mobility challenges and identifying what is working well or inefficiently in remote collaboration across four case studies. Evidences from the cases show that there is currently a little tool support for mobile work practices and better integration of the tools with companies existing internal systems may improve the productivity of the mobile workers. For researchers, a research lens is proposed to open up discussions and further concepts development with respect to mobile collaboration. For practitioners, a set of technical requirements and a list of corresponding OSS tools are presented to further elaborate the mobile working concept from their own context, with specific focus on the scenario of manager on business trip. The paper concludes in discussing open problems and proposing the research themes within the context of mobile collaboration.
8894 en Productivity in Collaboration-intensive Knowledge Work: The Collaboration Management Imperative Collaboration is a hot issue, and is to an increasing extent recognised as a key driver of overall business performance, innovation capabilities and productivity. However, few companies methodically evaluate how well they perform in the area of collaboration, and few companies have implemented management and leadership principles to systematically improve collaborative performance. This article describes the commonly occurring mismatch between the potential impact of collaboration on business performance and the attention given to collaboration. Furthermore, the article explains why companies should approach collaboration strategically, and identify new ways of fostering and facilitating collaboration in a structured manner. The article proposes a value perspective on collaboration and provides a framework for classifying and managing different factors related to collaboration, and concludes with a list of specific action points for organisations that are interested in improving their collaborative performance and obtaining a higher Return on Investment (ROI) on their collaboration initiatives.
8895 en Collaborative Working Environments as Globalised Inquiry for All With this lecture we are sharing our practical findings in the eSangathan Project, interpreted from the theoretical perspectives of Inquiring Communities and Collaborative Working Environment (CWE). We start by investigating the use of IT and CWE in support of Inquiring Communities among seniors working to create social innovations. We identify five different forms of Inquiring Communities: the Realistic, the Analytic, the Idealistic, the Dialectic and the Pragmatic. These communities we take to be basic and essential for communication and sharing of knowledge among human beings. As there are not very much evidence on the CWE support of all these five communities we use theories on Knowledge Management as a stepping stone, because there are substantial evidence of support for Knowledge Management by CWE, even though Knowledge Management lacks a sound theoretical foundation. As Knowledge Management only can illustrate the Realistic, the Analytic and the Idealistic types of Inquiring Communities we see a lack of support by CWEs for the most crucial aspects of communication and knowledge sharing, that among differences of opinion as in the Dialectical and Pragmatic communities. Finally we explicate findings on good practice in CWE as we have experienced them in the eSangathan project together with some important dilemmas for further investigations.
8896 en Achieving Interoperability in Grid-Enabled Virtual Organisation Grid computing introduces a new paradigm for the realisation of efficient collaborative ICT infrastructures for VOs that has many advantages in comparison to other known approaches. However, due to some shortcomings with regard to industry VO requirements, practical uptake is still quite low, despite of the potential benefits. In this paper, we focus on three important issues regarding the achievement of efficient interoperability in grid-enabled VOs: (1) the appropriate handling of authorisation and authentication, (2) the role-based access to resources and services, and (3) the gridification of existing applications. We suggest an approach to extend Grid functionality by an addition- nal semantic service layer on top of the basic Grid middleware services. The services on this layer are grounded on three inter-related ontologies specified in OWL, in alignment with the Semantic Web paradigm. Reported are findings from the recently finished European project InteliGrid and the ongoing German project BauVOGrid.
8897 en How the idea of a Single European Electronic Market is turning into reality The idea of a Single European Electronic Market (SEEM), described by the European Union and further elaborated within the European Project SEEMseed seems to be a good chance for Small, Medium and Micro Enterprises to seamlessly participate in the electronic market today and in the future. Even individuals may take advantage of the SEEM idea to actively benefit from electronic business within Europe. The overall goal of realizing a Single Electronic European Market is not easy attainable because of high complexity and fragmentation of the areas of interest. Under patronage and sponsorship of European Commission (above all 6th and 7th Framework Programme), several R&D project are currently running or soon to be started in order to foster the SEEM idea. Similar to this, CEN / ISSS has developed the project WS / eCAT. In this paper, we will describe and analyze the current efforts in this area and we will describe a meta-project called Simple On Line Catalog - to connect results of the existing projects, initiatives and studies and to cover them with an integral user interface, accessible to SME and Individuals across the borders and languages.
8898 en Validation of architectural targets in business components identification Component identification is one decisive task in designing software architectures for large component-based information systems, and especially the quality of business components identification plays an important role in concurrent enterprising. Methods that have been developed for this task yield component architectures that conform to these methods target functions. This paper validates the soundness of architectures resulting from systematic component identification with the BCI (business components identification) method. We could derive our results from a real-life industry project where we show that architecture qualities required by stakeholders are directly covered from a target architecture generated through BCI application.
8899 en An intelligent system to business and enterprise management - IDEA IDEA system assists all the specific processes of an enterprise from the meat processing industry to support decision makers to manage performances by implementing the concepts Business Performance Management (BPM) and Business Intelligence (BI). IDEA system transforms data into information and then into knowledge being focused on business, technological and economical aspects specific to the meat processing enterprises helping them to realise an efficient use of their business policies, financial, human and material resources. IDEA system integrates solutions implemented in the software components to decision processes management, customer relation management and enterprise resources planning components. IDEA integrates the BPM organisation's processes with its CRM components and ERP components. IDEA system offers support for an intelligent management to business processes, manufacture flows and the enterprise resources. The tools considered in the development of IDEA system are oriented on business management, business workflow analysis, business performance management, OLAP (Online Analytical Processing), data modelling, data visualisation, report servers, AJAX (Asynchronous JavaScript and XML) technology.
8900 en The effects of various forms of inter-organizational trust on competitiveness CIOPS (Cognitive Inter-organizational Production System) is an agent-based simulation model integrating structural and cognitive aspects of industry competitiveness. The model links firms profitability to the quality of their suppliers, thus firms purpose is to manage their cognitive space in order to find out the best supplier. This work analyses performance of firms using different decision making patterns which define the way clients select their suppliers. Four decision making patterns are observed: when firms make decisions using only their own past experiences, when they consider also others experiences, when they rely on reputations assigned to suppliers, and finally when they make decision randomly. Results show that decision making patterns based on others experiences and reputation are more profitable but that they are extremely sensitive to opportunist behaviours.
8901 en inContext: On Coupling and Sharing Context for Collaborative Teams Present team members have difficulties in keeping the relations between their various, concurrent activities due to the lack of suitable tools supporting context coupling and sharing. Furthermore, collaboration services are hardly aware of related context of team members and their activities. Such awareness is required to adapt to the dynamics of collaborative teams. In this paper, we discuss the context coupling techniques provided by the inContext project. Utilizing the concept of activity-based context and Web services techniques, we can couple individual and team contexts at runtime, thus improving the context-awareness and adaptation of collaboration services such as email, shared calendars, instant messaging and document management.
8902 en Introduction to the Session This session will show the results of the work in the project C@R. C@R project is an Integrated Project, funded by the IST programme of the European Commission's 6th Framework with a budget of EUR 15 million and 33 project partners. C@R aims to boost the use of ICT to promote rural development. According to this strategic goal, C@R will identify, develop and validate technological responses to actual barriers jeopardizing the sustainable development in rural areas. The project works in 7 Living labs in different countries, and manages different kind of activities in the rural environment, including traditional and new economic activities in rural areas. The project is developing products and services in different sectors, This session will present the results and findings in the framework of the project: the products and services developed, but also the methodologies developed in the project.
8903 en Enhancing an Open Service Oriented Architecture with Collaborative Functions for Rural Areas Development in rural European areas face many barriers. The Collaboration@Rural (C@R) EU project aims to remove these barriers through Collaborative Technologies adopted among Rural Living Labs across Europe, and to substantially contribute to the definition of a user-centric Open Collaborative Architecture. The paper presents the threefold C@R Open Service Oriented Architecture approach in System, Software Architecture and Practical Implementation levels. We show that collaborative functions can be orchestrated and instantiated in a tailored process using a service broker that can register and manage them. By introducing control and data planes and a domain concept, the C@R architecture easily addresses different business models in a more natural way compared to other software platforms or service architectures. We also illustrate how the Software Architecture principles and specific components discussed are instantiated in the Spanish and South African Rural Living Labs.
8905 en The multiple side of collaborative working in Foscati Living Lab 
8906 en Living Labs Fostering Open Innovation and Rural Development: Methodology and Results Rural living labs constitute a new and not yet validated approach of enabling user driven ICT-based innovation initiatives geared towards economic and social development in rural areas. At the same time living labs provide a context for open innovation based on partnerships between all stakeholders. This paper discusses methodologies and strategies for developing, launching and operating rural living labs for innovative collaborative working environments, and presents initial results from the C@R Integrated Project. Three living labs cases are presented and compared: Homokháti in Hungary, Sekhukhune in South-Africa, and Cudillero in Spain. The process of establishing the living lab, the involvement of users, the experimentation and innovation processes, and the technical and business innovations and their impacts on the rural environment are being discussed in order to conclude about effective methodologies and strategies. Such methodologies and strategies include the establishment of stakeholder platforms, the creation of user communities, the cyclic and spiral approach to innovation, and the action research style of participative development. Initial results indicate that in order to be successful, such methodologies and strategies must be strongly tailored to the local situation.
8915 en Introduction to the workshop Concurrent enterprising, and especially the new waves of innovation in collaborative networks, are the key issues of the ICE conference 2008. The education of CE-related aspects is an essential topic to prepare the future professionals to the increasing cooperative dimension of every business fields. This workshop addresses the issues related to cooperation in collective design education: methods, IT-tools, sociological aspects and so on. The papers relate different co-design experiments performed across Architecture students in France, Portugal, Germany and Canada. The workshop aims to bring together, discuss, and improve the innovative pedagogical scenarios, and to stimulate a pedagogical network of innovative cooperation-teaching methods.
8916 en Collaborative and Virtual Architectural Design in Second Life: FINC-AV experiment 
8918 en Stimulating Collaborative Behaviour in Design Education 
8919 en Critical Success Factors and Challenges to develop new Sustainable Supply Chains in India based on Swiss Experiences 
8920 en COIN Project Introduction By 2020 enterprise collaboration and interoperability services will become an invisible, pervasive and self-adaptive knowledge and business utility at disposal of the European networked enterprises from any industrial sector and domain in order to rapidly set-up, efficiently manage and effectively operate different forms of business collaborations, from the most traditional supply chains to the most advanced and dynamic business ecosystems.nThe COIN project is developing an ICT integrated solution, in order to make enterprise collaboration and interoperability services available as a business utility for European networked enterprises. The objective of the COIN workshop is to present Enterprises Interoperability and Enterprise Collaboration scenarios, selected among the most promising cases in Europe, with the aim of collecting additional use requirements and initial implementation roadmaps, based on real industrial and socio-economic contexts.
8921 en The COIN Metaphore for EU Industry 
8922 en COIN Results, Market Context, Targets 
8923 en Coin end user - VEN Healthcare 
8924 en Collaboration and Interoperability in the Andalusian Aeronautical Cluster 
8925 en COIN business needs from the perspective of an Innovative IT Cluster at the Hungarian Association of IT Companies (IVSZ) 
8926 en Early requirements collected from the end users 
8928 en Increased Efficiency in Customer Involvement in Configuration Processes: The SWOP Approach Besides a higher complexity of products and services, trends and tendencies in nowadays industries show also a growing customer demand for reliable, fast and cheap as well as individual solutions for existing problems. Nevertheless, optimised engineering and production processes involving multidisciplinary input, dynamic working environments and multi- stakeholder interests across the life-cycle and supply chain of products and services still have not been achieved yet. Especially for complex products, the configuration process is getting more and more important for a successful and efficient sales process. In addition, it is essential for a company to store its technical know-how centrally but make it available cross-departmentally (Ulmer 2005).
8929 en Expected project results and use scenarios 
8957 en Living Labs: new ways to enhance innovativeness in public sector services The public sector is seen to play an important role as a facilitator of innovativeness and competitiveness in the private sector. Besides this important role the innovativeness in the public sector must also be promoted in new ways because of the new challenges facing public sector service production. This paper introduces the ideas of the emerging open innovation paradigm in the public sector context presenting a case for user involvement in the public sector service development context in the Lahti region in Finland. This paper also discusses in more wider terms the emerging role of the public sector as not only a facilitator of innovativeness in the private sector organisations, but also as a target of innovation policies and especially highlights new ways to involve the end-users of public sector services in producing innovations. The implications for the regional innovation system (RIS) are also discussed.
8958 en Best Practices, Innovation and Development: Experiences from Five Living Lab Innovation Environments The Living Lab concept is based on open innovation and serves as a platform for different stakeholders in the innovation system (cities, companies, universities). There is a strong emphasis on user participation. The aim of this paper is to study in depth the experiences of Living Lab innovation environment/platform by qualitative case study method to gain more understanding of the best practices they have found in their activities, areas that need improvement and innovations coming out of these Living Labs. From the 14 Living Labs existing in Finland today, five were chosen for this stud. These represent different areas of industry and have a University of Applied Sciences involved.
8959 en Supporting innovative SME in innovation processes: The role of regional intermediaries Innovation is crucial for growth in highly developed regions. Still, it remains a challenge to foster innovation deliberately. Regional intermediaries can play an important role and provide platforms that induce networks and conversations. Analysis of examples from Central Switzerland and from Extremadura, Spain, shows three patterns typical for the most innovative SME: To take time for creative thought: break and play effect; to mix people with different backgrounds and ideas: diverse people effect; and to support open and trustful communication: conversation effect. These effects can be summarized as the known cafeteria effect and be used on the regional level. Organizations like the Lucerne University of Applied Sciences or Fundecyt of Extremadura or many other organizations can use these effects as ingredients for creating innovation supportive environments where SME can experience the power of break and play, diverse people and open and trustful conversations.
8960 en Adaptability through open innovation, a complexity view on selectivity Today's world is characterized by increasing complexity, uncertainty and change. Several authors have found in Complex Adaptive Systems (CAS) the basis for more suitable management models that should allow firms to adapt and survive under highly complex, uncertain and changing environments. The incorporation of external capabilities and knowledge through open innovation can increase an organization's adaptability because it amplifies significant differences and increases the number of transforming exchanges, through a higher interaction with external system agents. However, too much difference can generate a large number of possibilities, reducing momentum for action, and too many exchanges limits individual behaviour. Therefore, it is important to be selective so that organizations in pursuit of adaptability through open innovation don't fall into chaos. By using the example of the Engineering and Tooling sector, this paper explores the impact of open innovation on adaptability and the importance of selectivity and knowledge brokers.
8961 en Increased Efficiency in Customer Involvement in Configuration Processes: The SWOP Approach Besides a higher complexity of products and services, trends and tendencies in nowadays industries show also a growing customer demand for reliable, fast and cheap as well as individual solutions for existing problems. Nevertheless, optimised engineering and production processes involving multidisciplinary input, dynamic working environments and multi- stakeholder interests across the life-cycle and supply chain of products and services still have not been achieved yet. Especially for complex products, the configuration process is getting more and more important for a successful and efficient sales process. In addition, it is essential for a company to store its technical know-how centrally but make it available cross-departmentally (Ulmer 2005).
8962 en Monitoring and Control of Collaborative Innovation in Small Firms’ networks Collaborative innovations may be generated and well driven by strong visions and the progresses can be optimised by adequate monitoring and control. The paper specifies a verified framework for collaborative innovation for the case of small firms. Using the criticality concept it narrows down the attributes to be observed to a minimum set. The quantifications of these attributes provide for a reliable base for control and decision procedures ensuring the progress as well as the optimisation of collaborative innovations. The value bundles of all observable result in characteristics for control interventions that allow the set up of an efficient process control for collaborative innovation.
8963 en Context recognition in the wearIT@work project 
8964 en The next 6 big things in mobile computing 
8965 en An Approach to Systemic Innovation of Information Technology for Emergency Response 
8966 en Does Wearable Computing Really Empower the Mobile Worker – findings from ethnographic studies 
8969 en The Role of Collaborative Working Environments in Enabling Global Business Tukej je treba dodat ?e folder gareis_rcw kot part dva videa noter... Sta dva avtorja in eno predavanje iz 2 delov!!!nnnnnThe objective of this paper, based on early results of the New Global study, is to explore how globalisation of markets and industries affects the way companies are operating and collaborating, and to investigate the opportunities which global networking and global collaborative working opens up for market players including Europe's large number of SMEs. A core focus is on how ICT-enabled collaborative working environments (CWE) enable global operations and working. Based on an investigation of current trends and developments, this paper explores various types of collaboration settings in which CWEs enable new forms of global collaboration in teams, networks and communities. Initial results of case studies are presented to identify policies and strategies which could be applied to promote global collaboration of European companies.
8984 en Semantic Web Services as Foundation for Enterprise Interoperability 
8987 en What is sustainable innovation and why emerging markets? 
8989 en Applying Serious Games for Supporting Idea Generation in Collaborative Innovation Processes The ideation process often called the fuzzy front-end to innovation is one of the most crucial steps when starting industrial- and especially collaborative innovation processes. There are numerous creativity techniques like brainstorming to be used in this early phase. This paper introduces another approach based on Serious Gaming. A game to structure the ideation process refQuest has been developed and is briefly described. Two early evaluations have been performed at the University of Bremen in order to verify that the approach supports idea generation in a structured approach and to get an overview on strengths and weaknesses for further enhancements. The approach used for this early evaluation of the refQuest game prototype was based on three different types of input: the observation and the exchange of information between the facilitator and the player, questionnaires comprising questions on the functionality, the utility and the usability of the software as well as questions dealing with the idea generation process, and direct observations during the game.
8990 en Computer Related Inventions, in particular Business Methods: Examination at the European Patent Office 
8991 en “Experiences of R&D projects with Emerging Markets for Sustainable Innovation 
8996 en How European SMEs use ICT to engage in Global Virtual Collaboration 
9020 en Automated detection of electrocardiographic diagnostic features through an interplay between Spatial Aggregation and Computational Geometry Within the medical domain, Functional Imaging providesnmethods for effectual visualization of diagnostically relevantnnumeric fields, i.e. of spatially referenced measurements ofnvariables related to organ functions. Unveiling the salientnphysical events that underly a functional image is most appropriatelynaddressed by feature extraction methods that exploitnthe domain-specific knowledge combined with spatialnrelations at multiple abstraction levels and scales. The identificationnof specific patterns that are known to characterizenclasses of pathologies provides an important support to thendiagnosis of disturbances, and the assessment of organ functions.nIn this work we focus on Electrocardiographic diagnosisnbased on epicardial activation fields. This kind of data,nwhich can now be obtained non invasively from body surfacendata through mathematical model-based reconstruction methods,ncan hit electrical conduction pathologies that routine surfacenECGs may miss. However, their analysis/interpretationnstill requires highly specialized skills that belong to few experts.nGiven an epicardial activation field, the automated detectionnof salient patterns in it, grounded on the existing interpretationnrationale, would represent a major contributionntowards the clinical use of such valuable tools whose diagnosticnpotential is still largely unexplored. We focus on epicardialnactivation isochronal maps, which convey informationnabout the heart electric function in terms of the depolarizationnwavefront kinematics. An approach grounded on the integrationnof a Spatial Aggregation (SA) method with conceptsnborrowed from Computational Geometry provides a computationalnframework to extract, from the given activation data,na few basic features that characterize the wavefront propagation,nas well as a more specific set of diagnostic featuresnthat identify an important class of heart rhythm pathologies,nnamely reentry arrhythmias due to block of conduction.nKeywords: Biomedical imaging; functional imaging; imagenbased diagnosis; spatial aggregation; computational geometry;nelectrocardiography; cardiac electrical function.
9021 en Computing Human-Like Qualitative Topological Relations via Visual Routines A core problem in spatial reasoning is finding an appropriate set of relationships to compute. This paper proposes that humans represent topological relationships between 2D regions using three basic, qualitative relations: contains, intersects, and overlaps-with. We show how these relations can be computed from sketched inputs using a model of mid-level perception. Results from a pilot experiment indicate that these three relationships suffice to explain people?s judgments on four English spatial terms (“intersects”, “overlaps”, “connects to”, and “contains”), although a combination of the three is generally required for each term.
9022 en Commonsense Inference in Dynamic Spatial Systems “Phenomenal and Reasoning Requirements” Spatial changes within an environment are typicallyna result of interaction— actions and events—noccurring within. Reasoning about such changes,nwhen dealt with formally within the context ofnqualitative spatial calculi and logics of action andnchange, poses several difficulties along multiple dimensions:n(a) phenomenal requirements stemmingnfrom the dynamic nature of the spatial system (e.g.,nappearing and disappearing objects), (b) reasoningnrequirements (e.g., abductive explanation), (c)ndomain-independent or epistemological (e.g., persistence,nramification), and (d) aspects concerningnthe need to satisfy the intrinsic (axiomatic) propertiesnof the spatial calculi (e.g., compositional consistency)nbeing modelled. This paper, encompassingnthe phenomenal and reasoning aspects in (a)nand (b) respectively, presents some instances thatndemonstrate the role of commonsense reasoningnand the non-monotonic inference patterns it necessitatesnwhilst representing and reasoning about dynamicnspatial systems in general.
9023 en Factored Envisioning Envisioning has been used extensively to model behaviornof physical systems. Envisioning generatesnthe qualitatively distinct possible behaviors withoutnnumerically simulating every possible set ofninput conditions and model parameters. This papernapplies envisioning to analyze course of actionn(COA) diagrams to determine the qualitativelyndistinct outcomes of military operations. In ordernto avoid the combinatorial explosion of possiblenstates, this envisioner factors non-interactingnunits into separate envisionment threads. The envisionernuses Assumption-Based Truth Maintenancento further limit combinatorial explosion and estimatenprobability of outcomes.We illustrate the performancenof the factored envisioner on a variety ofnexamples provided by military experts. We analyzenits scaling performance and demonstrate its abilitynto track operations from sparse observations.
9024 en Automated Critique of Sketched Designs in Engineering Designers often use a series of sketches to explain how their design goes through different states or modes to achieve its intended function. Learning how to create such explanations turns out to be a difficult problem for engineering students. An au-tomated ?crash test dummy? to let students practice explanations would be desirable. This paper de-scribes how to carry out a core piece of the reason-ing needed in such system. We show how an open-domain sketch understanding system can be used to enter many aspects of such explanations, and how qualitative mechanics can be used to check the plausibility of the intended state transitions. The system is evaluated using a corpus of sketches based on designs from an engineering school de-sign & communications course.
9025 en Learning and Reasoning with Qualitative Models of Physical Behavior Building models of the physical world from examples is an important challenge for qualitative reasoning systems. We describe a system that can learn intuitive models of physical behaviors from a corpus of multimodal, multi-state stimuli, consisting of sketches and text. The system extracts and temporally encodes exemplars from the stimuli and uses analogical generalization to abstract prototypical behaviors. Using statistical analysis, the system parameterizes these abstractions into qualitative representations for reasoning. We show that the explanations the system provides for new situations are consistent with those given by naïve students. Keywords: Cognitive modeling; conceptual change; misconceptions; naïve physics; qualitative reasoning
9026 en Intelligent Authoring of ’Graph of Microworlds’ for Adaptive Learning with Microworlds In science education, it is important to sequence a set ofnmicroworlds (which means a system and its model limitednfrom educational viewpoint) of various complexitynadaptively to the context of learning. We previouslynproposed Graph of Microworlds (GMW), a frameworknfor indexing a set of microworlds based on their models.nBy using GMW, it is possible to adaptively selectnthe microworld a student should learn next, and to assistnhim in transferring between microworlds. However,nit isn’t easy to describe GMW because an author mustnhave the expertise in the process of modeling. In thisnresearch, we propose a method for semi-automating thendescription of GMW by introducing the compositionalnmodeling mechanism. Our method assists an authornin generating a set of indexed microworlds and also innconsidering educational meanings of the relations betweennthem. We present how to design such a functionnand also illustrate how it works. A preliminary testnwith a prototype system showed the effectiveness of ournmethod.
9027 en Application of qualitative reasoning models in the scientific education of deaf students Regarding the education of deaf students (in Brazil), threenconditions have to be met in order to bring qualitativenreasoning (QR) models into the classroom: (a) a bilingualneducation should be provided, the Brazilian Sign Languagen(LIBRAS) being the first and Portuguese the secondnlanguage; (b) in the absence of scientific vocabulary innLIBRAS, it has to be created; (c) given the auralnimpairment, which is cognitively compensated through annover-developed visual ability, a visually oriented pedagogynis needed. This paper describes how qualitative reasoningnmay provide an adequate scenario to create a vocabulary innsign language for representing scientific concepts whilenoffering support for the integration of visually-orientednmodels and simulations, and written Portuguese inneducational activities.nKey words: qualitative models, deaf, science education
9028 en Closeness and Distance Relations in Order of Magnitude Qualitative Reasoning via PDL The syntax, semantics and an axiom system for an extensionnof Propositional Dynamic Logic (PDL) for order of magnitudenqualitative reasoning which formalizes the concepts ofncloseness and distance is introduced in this paper. In doingnthis, we use some of the advantages of PDL: firstly, we exploitnthe possibility of constructing complex relations fromnsimpler ones for defining the concept of closeness and othernprogramming commands such as while . . . do and repeat . . .nuntil; secondly, we employ its theoretical support in order tonshow that the satisfiability problem is decidable. Moreover,nthe specific axioms of our logic have been obtained from thenminimal set of formulas needed in our definition of qualitativensum of small, medium and large numbers. We also presentnsome of the advantages of our approach on the basis of annexample.
9029 en Incorporating Qualitative Equations in Process-Based Models This paper explores the possibility of extending Processbasednmodels with Qualitative differential equations.nProcess-based modeling is a modeling technique that usesntwo-level approach for modeling dynamical systems. Itnmodels systems on a purely qualitative level in terms ofnentities and processes that involve those entities on onenhand, and a quantitative level on which all entities andnprocesses are given a quantitative formulation which is thatnautomatically translated into a set of ordinary differentialnequations. This paper aims to illustrate that this formalismncan be extended with an intermediate level of modelingnwhich consists of qualitative equations.
9030 en Using Qualitative Reasoning in Modelling Consensus in Group Decision-Making Ordinal scales are commonly used in rating and evaluationnprocesses. These processes usually involve group decisionnmaking by means of an experts’ committee. In this paper anmathematical framework based on the qualitative model ofnthe absolute orders of magnitude is considered. The entropynof a qualitatively described system is defined in this framework.nOn the one hand, this enables us to measure the amountnof information provided by each evaluator and, on the othernhand, the coherence of the evaluation committee. The newnapproach is capable of managing situations where the assessmentngiven by experts involves different levels of precision.nThe use of the proposed measures within an automatic systemnfor group decision making will contribute towards avoidingnthe potential subjectivity caused by conflicts of interestsnof the evaluators in the group.
9031 en Model Building Experiences using Garp3: Problems, Patterns and Debugging Capturing conceptual knowledge in QR models is becomingnof interest to a larger audience of domain experts.nConsequently, we have been training severalngroups to effectively create QR models during the lastnfew years. In this paper we describe our teaching experiences,nthe issues the modellers encountered and thensolutions to solve them in the form of reusable patterns,nand finally a structured way to debug models.
9032 en Dark Knowledge in Qualitative Reasoning: A Call to Arms While people do qualitative reasoning, there is amplenevidence that they do not always do it well. Two currentncrises, human-induced climate change and the financialnmeltdown, can be traced in part to faulty mental models.nThe QR community has formalisms that can potentially helpnwith public education about such problems, but so far wenhave not been very successful in doing so. We claim thatnpart of the reason is that current QR accounts do notnadequately incorporate experiential knowledge. We arguenthat it is important to find better ways to improve publicnqualitative reasoning abilities, in part by helping peoplenenlist their experience-based models via analogy.
9033 en Order-of-Magnitude Based Link Analysis for False Identity Detection Combating identity fraud is crucial and urgent as false identitynhas become the common denominator of all seriousncrime, including mafia trafficking and terrorism. Typical approachesnto detecting the use of false identity rely on thensimilarity measure of textual and other content-based characteristics,nwhich are usually not applicable in the case of deceptivenand erroneous description. This barrier can be overcomenthrough link information presented in communicationnbehaviors, financial interactions and social networks. Quantitativenlink-based similarity measures have proven effectivenfor identifying similar problems in the Internet and publicationndomains. However, these numerical methods only concentratenon link structures, and fail to achieve accurate andncoherent interpretation of the information. Inspired by thisnobservation, this paper presents a novel qualitative similaritynmeasure that makes use of multiple link properties to refinenthe underlying similarity estimation process and consequentlynderive semantic-rich similarity descriptors. The approachnis based on order-of-magnitude reasoning. Its applicabilitynand performance are experimentally evaluated over anterrorism-related dataset, and further generalized with publicationndata.
9034 en QCM: A QP-Based Concept Map System Qualitative representations have proven to be usefulnformalisms for capturing human mental models. As a result,nqualitative modeling could become an important tool forncognitive science. Specifically, an environment in whichnqualitative representations can be used to explore mentalnmodels and different type of reasoning and simulations cannbe performed on these models can be a useful tool forncognitive scientists. In this paper, we introduce thenQualitative Concept Map system, designed for cognitivenscientists, for building and simulating qualitative andnBayesian models using qualitative process theory andnBayesian inference.
9035 en Qualitative approximation to Dynamic TimeWarping similarity between time series data Dynamic time warping (DTW) is a method for calculatingnthe similarity between two time series which cannoccur at different times or speeds. Although its effectivenessnmade it very popular in several disciplines, itsntime complexity of O(N2) makes it useful only fornrelatively short time series. In this paper, we proposena qualitative approximation Qualitative Dynamic TimenWarping (QDTW) to DTW. QDTW reduces a time seriesnlength by transforming it to qualitative time series.nDTW is later calculated between qualitative time series.nAs qualitative time series are normally much shorternthan their corresponding numerical time series, time toncompute their similarity is significantly reduced. Experimentalnresults have shown improved running time of upnto three orders of magnitude, while prediction accuracynonly slightly decreased.
9036 en A qualitative model of the salmon life cycle in the context of river rehabilitation A qualitative model was developed in Garp3 to capture andnformalise knowledge about river rehabilitation and thenmanagement of an Atlantic salmon population. The modelnintegrates information about the ecology of the salmon lifencycle, the environmental factors that may limit the survivalnof key life stages and links with human activities such asnagriculture, habitat rehabilitation and fishing. The overallnaim of the model was to explore the effects of rehabilitationnin the context of a complete life cycle scenario. Thenscenarios and simulations produced were able to explorenthese processes in the context of a complete life cycle, but atnthis scale the simulations were time consuming. Therefore,nin addition to these scenarios a series of smallerndemonstrator scenarios were developed that succinctlynexplored individual concepts within the system.
9037 en Evaluating the potential of Qualitative Reasoning to capture and communicate knowledge on sustainable catchment management This paper presents the potential use of QualitativenReasoning (QR) to capture and communicate knowledge onnsustainable catchment management. Based on a case study,nqualitative models dealing with issues of a sustainablendevelopment of riverine landscapes were developed andnimplemented using the Garp3 software following a generalnmodeling framework. The evaluation of the models and thenQR approach by students and experts revealed the highnpotential of QR models to capture and communicatencomplex knowledge in an understandable and interestingnmanner, mainly due to the ability of the presented approachnto capture qualitative system dynamics and integrate ‘hard’nand ‘soft’ facts in a structured way. In the future a library ofnexpert models might serve as an important source ofninformation for both, education and management.
9038 en Assessing the Ecological Impacts of Agriculture Intensification Through Qualitative Reasoning How to feed the world without loosing what is left ofnbiodiversity? Two answers for this question are found in thenliterature. On the one hand, the “Land Sparing” paradigmnsuggests that increasing yield by means of intensive agriculturalnsystems would fulfill the needs of human population and savennatural landscapes. On the other hand, “Biodiversity FriendlynFarming” argues that agricultural intensification has deep impactsnon both biodiversity and ecosystem properties and suggests thatnnon-intensive farming practices keep the ecological balance andnstill may produce large quantities of high quality food (foodnsecurity). This work presents a Qualitative Reasoning (QR) modelnthat compares the impacts of intensive and non-intensivenagriculture on water resources, biodiversity and productivity. Thensimulations show the inefficiency of intensive agriculture innprotecting water resources and biodiversity, and the efficiency ofnnon-intensive approach in terms of food production andnecosystem conservation.
9082 en Learning in Hierarchical Architectures: from Neuroscience to Derived Kernels Understanding the processing of information in our cortex is a significant part of understanding how the brain works, arguably one of the greatest problems in science today. In particular, our visual abilities are computationally amazing: computer science is still far from being able to create a vision engine that imitates them. Thus, visual cortex and the problem of the computations it performs may well be a good proxy for the rest of the cortex and for intelligence itself. nI will briefly review our work on developing a hierarchical feedforward architecture for object recognition based on the anatomy and the physiology of the primate visual cortex. These architectures compete with state-of-the-art computer vision systems; they mimic human performance on a specific but difficult natural image recognition task. I will sketch current work aimed at extending the model to the recognition of behaviors in time sequences of images and to accounting for attentional effects inhuman vision. nI will then describe a new attempt (with S. Smale, L. Rosasco and J. Bouvrie) to develop a mathematics for hierarchical kernel machines centered around the notion of a recursively defined "derived kernel" and directly suggested by the model and the underlying neuroscience of the visual cortex.
9083 en More Data Less Work: Runtime As A Monotonically Decreasing Function of Data Set Size We are used to studying runtime as an increasing function of the data set size, and are happy when this increase is not so bad (e.g. when the runtime increases linearly, or even polynomiall, with the data set size). Traditional runtime analysis of learning is also viewed this way, and studies how training runtime increases as more data is available. However, considering the true objective of training, which is to obtain a good predictor, I will argue that training runtime should actually be studied as a *decreasing* function of training set size. Focusing on training Support Vector Machines (SVMs) and combining ideas from optimization, statistical learning theory, and online methods. I will then present both theoretical and empirical results demonstrating how a simple stochastic subgradient descent approach indeed displays such monotonic decreasing behavior. I will also discuss a similar phenomena in the context of Gaussian mixture clustering, where it appears that excess data turns the problem from computationally intractable to computationally tractable. Joint work with Shai Shalev-Shwartz, Karthik Sridharan, Yoram Singer, Greg Shakhnarovich and Sam Roweis.
9084 en On Finding Low Error Clusterings There has been substantial work on approximation algorithms for clustering data under distance-based objective functions such as k-median, k-means, and min-sum objectives. This work is fueled in part by the hope that approximating these objectives well will indeed yield more accurate solutions. That is, for problems such as clustering proteins by function, or clustering images by subject, there is some unknown correct "target" clustering and the implicit assumption is that clusterings that are approximately optimal in terms of these distance-based measures are also approximately correct in terms of error with respect to the target. In this work we show that if we make this implicit assumption explicit - that is, if we assume that any c-approximation to the given clustering objective Phi is epsilon-close to the target - then we can produce clusterings that are O(epsilon)-close to the target, even for values c for which obtaining a c-approximation is NP-hard. In particular, for the k-median, k-means, and min-sum objectives, we show that we can achieve this guarantee for any constant c > 1. Our results shows how for these clustering objectives one can get much better guarantees on accuracy than those implied by results obtained so far in the approximation literature, by wisely using all the available information for the problem at hand.
9086 en Analysis of Clustering Procedures Clustering procedures are notoriously short on rigorous guarantees. In this tutorial, I will cover some of the types of analysis that have been applied to clustering, and emphasize open problems that remain. Part I. Approximation algorithms for clustering Two popular cost functions for clustering are k-center and k-means. Both are NP-hard to optimize exactly. (a) Algorithms for approximately optimizing these cost functions. (b) Hierarchical versions of such clusterings. (c) Clustering when data is arriving in a streaming or online manner. Part II. Analysis of popular heuristics (a) How good is k-means? How fast is it? (b) Probabilistic analysis of EM. (c) What approximation ratio is achieved by agglomerative heuristics for hierarchial clustering? Part III. Statistical theory in clustering What aspects of the underlying data distribution are captured by the clustering of a finite sample from that distribution? (a) Consistency of k-means. (b) The cluster tree and linkage algorithms. (c) Rates for vector quantization.
9087 en The Stability of the Contour of an Orientable 2-Manifold Think of the view of the boundary of a solid shape as a projection of a 2-manifold to R^2. Its apparent contour is the projection of the critical points. Generalizing the projection to smooth mappings of a 2-manifold to R^2, we get the contour as the image of the points at which the derivative is not surjective. Measuring difference with the erosion distance (the Hausdorff distance between the complements), we prove that the contour is stable. Along the way, we introduce the by now well established method of persistent homology, including the stability of its diagrams, as well as an extension using zigzag modules. Joint work with Dmitriy Morozov and Amit Patel.
9088 en On a Theory of Similarity Functions for Learning and Clustering Kernel methods have become powerful tools in machine learning. They perform well in many applications, and there is also a well-developed theory of what makes a given kernel useful for a given learning problem. However, this theory requires viewing kernels as implicit (and often difficult to characterize) maps into high-dimensional spaces. In this talk I will describe work on developing a theory that just views a kernel as a measure of similarity between data objects, and describes the usefulness of a given kernel (or more general similarity function) in terms of fairly intuitive, direct properties of how the similarity function relates to the task at hand, without need to refer to any implicit spaces. I will also talk about an extension of this framework to learning from purely unlabeled data, i.e., clustering. In particular, one can ask how much stronger the properties of a similarity function should be (in terms of its relation to the unknown desired clustering) so that it can be used to cluster well: to learn well without any label information at all. We find that if we are willing to relax the objective a bit (for example, allow the algorithm to produce a hierarchical clustering that we will call successful if some pruning is close to the desired clustering), then this question leads to a number of interesting graph-theoretic and game-theoretic properties that are sufficient to cluster well. This work can be viewed defining a kind of PAC model for clustering. (This talk based on work joint with Maria-Florina Balcan, Santosh Vempala, and Nati Srebro).
9089 en A Bahadur Type Representation of the Linear Support Vector Machine and its Relative Efficiency The support vector machine has been used successfully in a variety of applications. Also on the theoretical front, its statistical properties including Bayes risk consistency have been examined rather extensively. Taking another look at the method, we investigate the asymptotic behavior of the linear support vector machine through Bahadur type representation of the coefficients established under appropriate conditions. Their asymptotic normality and statistical variability are derived on the basis of the representation. Furthermore, direct theoretical comparison is made with likelihood based approach to classification such as linear discriminant analysis and logistic regression in terms of the asymptotic relative efficiency, where the efficiency of a classification procedure is defined using the excess risk from the Bayes risk.
9090 en Fitting a Graph to Vector Data We ask "What is the right graph to fit to a set of vectors?" We propose one solution that provides good answers to standard Machine Learning problems, that has interesting combinatorial properties, and that we can compute efficiently. Joint work with Jonathan Kelner and Samuel Daitch.
9091 en An Overview of Compressed Sensing and Sparse Signal Recovery via L1 Minimization In many applications, one often has fewer equations than unknowns. While this seems hopeless, the premise that the object we wish to recover is sparse or nearly sparse radically changes the problem, making the search for solutions feasible. This lecture will introduce sparsity as a key modeling tool together with a series of little miracles touching on many areas of data processing. These examples show that finding *that* solution to an underdetermined system of linear equations with minimum L1 norm, often returns the ''right'' answer. Further, there is by now a well-established body of work going by the name of compressed sensing, which asserts that one can exploit sparsity or compressibility when acquiring signals of general interest, and that one can design nonadaptive sampling techniques that condense the information in a compressible signal into a small amount of data - in fewer data points than were thought necessary. We will survey some of these theories and trace back some of their origins to early work done in the 50's. Because these theories are broadly applicable in nature, the tutorial will move through several applications areas that may be impacted such as signal processing, bio-medical imaging, machine learning and so on. Finally, we will discuss how these theories and methods have far reaching implications for sensor design and other types of designs.
9092 en Spectral Graph Theory, Linear Solvers and Applications We discuss the development of combinatorial methods for solving symmetric diagonally dominate linear systems. Over the last fifteen years the computer science community has made substantial progress in fast solvers for SDD systems. For general SDD systems the upper bound is $0(m \log^k n)$ for some constant $k$, where $m$ is the number of non-zero entries, due to Spielman and Teng. Newer methods, combinatorial multigrid, have linear time guarantee for the planar case and work very well in practice. Critical to the use of these new solvers has been the reduction of problems to the solution of SDD systems. We present some of these reductions, including several from image processing.
9093 en Cheeger Cuts and p-Spectral Clustering Spectral clustering has become in recent years one of the most popular clustering algorithm. In this talk I discuss a generalized version of spectral clustering based on the second eigenvector of the graph p-Laplacian, a non-linear generalization of the graph Laplacian. The clustering obtained for 1<=2 can be seen as an interpolation of the relaxation of the normalized cut (p=2) and the Cheeger cut (p=1). However, the main motivation for p-spectral clustering is the fact, that one can show that the cut value obtained by thresholding the second eigenvector of the p-Laplacian converges towards the optimal Cheeger cut as p tends to 1. I will also present an efficient implementation which allows to do p-spectral clustering for large scale datasets.
9094 en Multiscale Geometry and Harmonic Analysis of Data Bases We describe a method for geometrization of databases such as, questionnaires, or lists of sensor outputs. Interlacing multiscale diffusion geometries of rows and columns of a data matrix, results in a pair of language ontologies which are mutually supportive (certain words are used in certain contexts). This mutual geometry serves a structure of Harmonic Analysis and signal processing on the database. We will illustrate, on databases of audio (music), psychological questionnaires, science documents, images and many others. Joint work with Mata Gavish, Yale University.
9095 en Graphical Models for Speech Recognition: Articulatory and Audio-Visual Models Since the 1980s, the main approach to automatic speech recognition has been using hidden Markov models (HMMs), in which each state corresponds to a phoneme or part of a phoneme in the context of the neighboring phonemes. Despite their crude approximation of the speech signal, and the large margin for improvement still remaining, HMMs have proven difficult to beat. In the last few years, there has been increasing interest in more complex graphical models for speech recognition, involving multiple streams of states. I will describe two such approaches, one modeling pronunciation variation as the result of the "sloppy" behavior of articulatory variables (the states of the lips, tongue, etc.) and the other modeling the audio and visual states in audio-visual speech recognition (i.e. recognition enhanced by "lipreading"). nn
9096 en Semi-Supervised Learning This tutorial covers classification approaches that utilize both labeled and unlabeled data. We will review self-training, Gaussian mixture models, co-training, multiview learning, graph-transduction and manifold regularization, transductive SVMs, and a PAC bound for semi-supervised learning. We then discuss some new development, including online semi-supervised learning, multi-manifold learning, and human semi-supervised learning.
9097 en Matrix Completion via Convex Optimization: Theory and Algorithms This talk considers a problem of considerable practical interest: the recovery of a data matrix from a sampling of its entries. In partially filled out surveys, for instance, we would like to infer the many missing entries. In the area of recommender systems, users submit ratings on a subset of entries in a database, and the vendor provides recommendations based on the user's preferences. Because users only rate a few items, we would like to infer their preference for unrated items (this is the famous Netflix problem). Formally, suppose that we observe m entries selected uniformly at random from a matrix. Can we complete the matrix and recover the entries that we have not seen? We show that perhaps surprisingly, one can recover low-rank matrices exactly from what appear to be highly incomplete sets of sampled entries; that is, from a minimally sampled set of entries. Further, perfect recovery is possible by solving a simple convex optimization program, namely, a convenient semidefinite program. A surprise is that our methods are optimal and succeed as soon as recovery is possible by any method whatsoever, no matter how intractable; this result hinges on powerful techniques in probability theory. Time permitting, we will also present a very efficient algorithm based on iterative singular value thresholding, which can complete matrices with about a billion entries in a matter of minutes on a personal computer.
9098 en Drifting Games, Boosting and Online Learning Drifting games provide a new and useful framework for analyzing learning algorithms. In this talk I will present the framework and show how it is used to derive a new boosting algorithm, called RobustBoost and a new online prediction algorithm, called NormalHedge. I will present two sets of experiments using these algorithms on synthetic and real world data. The first set demonstrates that RobustBoost can learn from mislabeled training data. The second demonstrating an application of NormalHedge to the tracking moving objects.
9099 en Recent Progress in Combinatorial Statistics I will discuss some recent progress in combinatorial statistics. In particular, I will describe progress in the areas of reconstructing graphical models, ML estimation of the Mallows model and diagnostics of MCMC.
9100 en MAP Estimation with Perfect Graphs Efficiently finding the maximum a posteriori (MAP) configuration of a graphical model is an important problem which is often implemented using message passing algorithms and linear programming. The optimality of such algorithms is only well established for singly-connected graphs such as trees. Recently, along with others, we have shown that matching and b-matching also admit exact MAP estimation under max product belief propagation. This leads us to consider a generalization of trees, matchings and b-matchings: the fascinating family of so-called perfect graphs. While MAP estimation in general loopy graphical models is NP, for perfect graphs of a particular form, the problem is in P. This result leverages recent progress in defining perfect graphs (the strong perfect graph theorem which has been resolved after 4 decades), linear programming relaxations of MAP estimation and recent convergent message passing schemes. In particular, we convert any graphical model into a so-called nand Markov random field. This model is straightforward to relax into a linear program whose integrality can be established in general by testing for graph perfection. This perfection test is performed efficiently using a polynomial time algorithm. Alternatively, known decomposition tools from perfect graph theory may be used to prove perfection for certain graphs. Thus, a general graph framework is provided for determining when MAP estimation in any graphical model is in P, has an integral linear programming relaxation and is exactly recoverable by message passing.
9101 en Inference for Networks A great deal of attention has recently been paid to determining sub-communities on the basis of relations, corresponding to edges, between individuals, corresponding to vertices out of an unlabelled graph (Neman, SIAM Review 2003; Airoldi et al JMLR 2008; Leskovec & Kleinberg et al SIGKDD 2005) for probabilistic ergodic models of infinite unlabelled graphs. We drive consistency properties of the Newman-Girvon index, and develop an index with better consistency properties and better performance on simulated data sets. This is joint work with Aiyou Chen.
9102 en Cocktail Party Problem as Binary Classification Speech segregation, or the cocktail party problem, has proven to be extremely challenging. Part of the challenge stems from the lack of a carefully analyzed computational goal. While the separation of every sound source in a mixture is considered the gold standard, I argue that such an objective is neither realistic nor what the human auditory system does. Motivated by the auditory masking phenomenon, we have suggested instead the ideal time-frequency (T-F) binary mask as a main goal for computational auditory scene analysis. Ideal binary masking retains the mixture energy in T-F units where the local signal-to-noise ratio exceeds a certain threshold, and rejects the mixture energy in other T-F units. Recent psychophysical evidence shows that ideal binary masking leads to large speech intelligibility improvements in noisy environments for both normal-hearing and hearing-impaired listeners. The effectiveness of the ideal binary mask implies that sound separation may be formulated as a case of binary classification, which opens the cocktail party problem to a variety of pattern classification and clustering methods. As an example, I discuss a recent system that segregates unvoiced speech by supervised classification of acoustic-phonetic features.
9103 en Machine Learning in Acoustic Signal Processing This tutorial presents a framework for understanding and comparing applications of pattern recognition in acoustic signal processing. Representative applications will be delimited by two binary features: (1) regression vs. (2) classification (inferred variables are continuous vs. discrete), (A) instantaneous vs. (B) dynamic. (1. Regression) problems include imaging and sound source tracking using a device with unknown properties, and inverse problems, e.g., articulatory estimation from speech audio. (2. Classification) problems include, e.g., the detection of syllable onsets and offsets in a speech signal, and the classification of non-speech audio events. (A. Instantaneous) inference is performed using a universal approximator (neural network, Gaussian mixture, kernel regression), constrained or regularized, if necessary, to reduce generalization error (resulting in a support vector machine, shrunk net, pruned tree, or boosted classifier combination). (B. Dynamic) inference methods apply prior knowledge of state transition probabilities, either in the form of a regularization term (e.g., using Bayesian inference) or in the form of set constraints (e.g., using linear programming) or both; examples include speech-to-text transcription, acoustic-to-articulatory inversion using a switching Kalman filter, and computation of the query presence probability in an audio information retrieval task.
9104 en Nonlinear Dimension Reduction by Spectral Connectivity Analysis and Diffusion Coarse-Graining For naturally occurring data, the dimension of the given input space is often very large while the data themselves have a low intrinsic dimensionality. Spectral kernel methods are non-linear techniques for transforming data into a coordinate system that efficiently reveal the geometric structure in particular, the connectivity of the data. In this talk, we will focus on one particular technique diffusion maps and diffusion coarse-graining; the construction is based on a Markov random walk on the data and offers a general scheme of simultaneously reorganizing and quantizing graphs and arbitrarily shaped data sets in high dimensions using intrinsic geometry. We show that clustering in embedding spaces is equivalent to compressing operators and that the quantization distortion in diffusion space bounds the error of compression of the operator, thus giving a rigorous justification and a precise measure of performance of k-means clustering in spectral embedding spaces. We will discuss two particular applications of diffusion coarse-graining: One application is choosing an appropriate set of prototype similar stellar population (SSP) spectra for parameter estimation of star formation history in galaxies. The other example is texture discrimination by a novel geometry-based metric on distributions. (Part of this work is joint with R.R. Coifman, S. Lafon, J. Richards and C. Schafer.)
9314 en Warm-Up for BCI Physiology: Basic Concepts in the Transition from Cellular Microrecordings to Noninvasive Large-Scale EEG/MEG Signals Macroscopic brain sigals, e.g., the noninvasively measured scalp EEG/MEG, correlate with mental states, such as movement intentions. This opens a new communication channel for paralyzed patients as their intentions can be 'read' by computers and utilised for triggering technical devices. This entry-level tutorial will provide an intuitive introduction into the relevant brain anatomy and physiology and describe typical procedures as well as potential pitfalls when obtaining and analysing multichannel EEG/MEG data.
9315 en Machine Learning and Signal Processing Tools for BCI We will first provide a brief overview of Brain-Computer Interface from a machine learning and signal processing perspective. In particular showing the wealth, the complexity and the difficulties of the data available, a truly enormous challenge: In real-time a multi-variate very strongly noise contaminated data stream is to be processed and neuroelectric activities are to be accurately differentiated. We will then in detail discuss the components of the data analysis chain employed in modern BCI systems, spanning all aspects from preprocessing and feature extraction, adaptive vs. fixed classification and feedback design.
9316 en About Learning, Predictions and Adaptivity of Brains and Machines Using useful signals from the brain, and useful computer algorithms to improve brain machine interfaces.nI will talk about the physiology of motor cortex and the nature of activity of population of neurons in motor cortex during Sensorimotor learning, movement preparation and execution. I will present the approach of internal models of the brain as the basis for learning and perception and use all of the above to show how our current knowledge can facilitate approaches to adaptive brain machine interfaces.
9317 en Theory and Application of Electrocorticographic (ECoG) Signals in Humans Brain-computer interfaces (BCIs) convert brain signals into outputs that communicate a user's intent. BCIs can be used by people to communicate and interact with their environment. However, the prevailing non-invasive and invasive sensor methods have important limitations. Electrocorticographic (ECoG) recordings from the surface of the brain could be a robust and high-fidelity alternative to existing sensor methods. This tutorial will provide an overview of the history of ECoG recordings; describe the types of signals present in ECoG and their relationship to signals detected using EEG and intracortical microelectrodes; and finally give examples of successful use of these signals in real time for BCI purposes and also for diagnosis.
9318 en What's the Plan? - Movement-Goal Representations in the Frontoparietal Reach Network When planning goal-directed arm movements the sensorimotor system needs to integrate the current behavioral context with given spatial constraints to define and maintain motor-goals. We are interested in how this integration is achieved within the sensorimotor network. Specifically we tested the respective roles of the dorsal premotor cortex (PMd) and the parietal reach region (PRR) in defining and planning context-specific reach goals.
9319 en Brain-Machine Interfaces Based on Neuronal Ensemble Recordings Brain-machine interfaces (BMIs) have experienced an explosive development during the last decade. Current state of the art BMIs convert neuronal ensemble activity recorded in nonhuman primates or human subjects into reaching and grasping movements performed by artificial actuators. BMIs that enact movements of lower extremeties are less explored. Additionally, most BMI implementations do not have somatosensory feedback from the actuator. I will review our recent experiments in which we (i) extracted bipedal locomotion patterns from monkey cortical activity and (ii) used spatiotemporal patterns of intracortical microstimulation to deliver information back to the brain. These results bring us closer to building clinical neuroprosthetic devices for restoration of both sensory and motor functions in paralyzed people.
9320 en Plasticity at the Brain-Computer Interface Next generation recurrent Brain-Computer Interfaces will not only extract signals from cortical activity but also deliver feedback to the nervous system via electrical stimulation. For example, stimulation of cervical spinal segments can produce functional arm and hand movements such as reaching and grasping. We are developing new technologies including chronic electrodes and implantable electronic circuitry to control stimulation from cortical recordings, constituting an artificial corticospinal connection which could replace injured motor pathways. I will present evidence that the motor system can readily acquire the novel neuromotor transformations required to incorporate these connections into motor system function. In separate experiments we have shown that operation of artificial connections can potentiate new motor pathways via activity-dependent plasticity mechanisms. Together, these results suggest that recurrent BCIs have application not only as prostheses to replace function, but also as tools for manipulating plastic reorganisation to restore nervous system function following injury.
9321 en BCI in Paralysis: An Unfulfilled Promise EEG and ECoG (Electrocorticogram) can be used successfully to initiate direct brain communication with locked-in patients but fail in completely locked-in patients. Possible reasons are explained and some new solutions with first data presented. In chronic stroke the author's team together with L. Cohen's group at NIH have shown motor restoration of paralysed hand in chronic stroke without residual movement using non-invasive MEG/BCI. However generalization from the BCI-clinic to the social reality was poor. A new strategy for invasive and non-invasive BCI in chronic stroke is demonstrated and first data presented.
9322 en The Hybrid BCI There are several different BCI approaches, which may or may not depend on external stimulation. Slow cortical potential (SCP)-, event-related desynchronisation (ERD)- and sensorimotor rhythm (SMR)-BCIs do not require external stimulation, while P300-BCIs and steady state visual evoked potential (SSVEP)-BCIs do. Dependent means in this respect that the user has to focus attention and/or gaze to flickering/flashing lights or and can therefore not completely freely decide to perform an action. Each type of BCI system has advantages and disadvantages. SSVEP-BCIs need minimal training time and can achieve a high information transfer rate (ITR), but have a relatively high false positive rate during rest. In contrast, an asynchronous brain switch based on the post-imagery beta ERS has a low ITR, but can be set up quickly and easily with a low false position rate (Pfurtscheller and Solis-Escalante, Clin. Neurophysio. 2009). It is therefore a challenge to use the advantages of different BCI systems and create a “hybrid” BCI system by switching e.g. a battery of flickering lights (SSVEP-BCI) on or off by using a brain switch (ERD-BCI). Another type of “hybrid” BCI can analyse motor imagery related EEG changes and SSVEP amplitudes simultaneously. It was shown recently that such a “hybrid” strategy results in a better classification accuracy relative to either an ERD or SSVEP classification alone (Allison et al submitted 2009).
9323 en Feedback-Regulated Mental Imagery in BCI Applications: Using Non-Invasive EEG and NIRS Signals An important issue of brain-computer interface (BCI) development is to detect changes in brain signals that are related to specific intentions or thought processes. For example, mental motor imagery modulates the sensorimotor brain activity, and the detected changes can be used to operate a computer-controlled device. Clinical applications of this technology include the restoration of movement, such as control of grasping with the help of a neuroprosthesis, in severely paralyzed individuals. The motor imagery based BCI training may further be useful as a complementary therapeutic tool to facilitate functional recovery after stroke. To date, the majority of BCI systems rely on EEG recordings. However, near-infrared spectroscopy (NIRS) has recently attracted attention of BCI researchers due to its noninvasiveness, portability, short preparation time, and relatively low cost. In this talk I will shortly introduce the NIRS technique for BCI development and present data on how characteristic hemodynamic responses during motor imagery can be modulated by real-time NIRS feedback. Based on recent results I will finally discuss how simultaneous NIRS and EEG recordings might combine advantages of both approaches.
9324 en EEG?Based Brain?Computer Interface for Communication and Control: Independent Home Use People affected by severe motor disorders such as amyotrophic lateral sclerosis (ALS), brainstem stroke, cerebral palsy, and spinal cord injury need alternative methods for communication and control. They may not be able to use even the most basic conventional assistive technologies, which all rely in one way or another on muscles. Studies from this and other laboratories have shown that humans, including those with severe motor disabilities, can learn to control sensorimotor rhythms and other features of scalp?recorded electroencephalographic (EEG) activity and that they can use this control to select letters or icons, or move a cursor in up to three dimensions. Such multidimensional control could be used to control a prosthesis or a robotic arm. Currently, we are showing that people with ALS can use EEG?based brain?computer interfaces (BCIs) for communication and control independently in their homes.
9325 en Tackling Increasing Opthalmologic Problems in Patients with a New Auditory Multi-Class BCI Paradigm Most P300 BCI approaches use the visual modality for stimulation and feedback. Due to increasing sight deterioration, this might not be the preferable choice for ALS patients in late stages. To tackle this general problem, a multi-class brain-computer interface paradigm is proposed that uses fast spatially distributed auditory cues for ERP paradigms.
9328 en A Robust Spelling Device for Locked-In Patients Based on Real-Time fMRI Several medical conditions (e.g., brain injury, stroke, progressive neurological diseases) can lead to complete paralysis while largely preserving sensory and cognitive functions and associated brain activation. We investigated whether healthy subjects are able to "write" solely on the basis of voluntary control of the fMRI (BOLD) signal. Using a guided display technique, we show that subjects can learn in less than half an hour to produce reliably any letter of the alphabet in a single trial. To achieve this performance, subjects use three mental strategies to modulate spatio-temporal properties of the fMRI signal in three different brain areas. While the transmitted information (BOLD time courses from regions-of-interest) has been initially decoded offline by human raters, we have recently implemented a fully automatized real-time "brain reading" technique.The developed paradigm and decoding technique might be applied in locked-in patients to let them communicate their wishes and thoughts reliably without extensive pre-training.
9330 en Modeling fMRI Dynamics Functional MRI modeling is challenged by long-range coupling, non-linearity, and lack of detailed physiological information. I will review our progress in modeling the spatio-temporal dynamics of fMRI including hemodynamic deconvolution, blind deconvolution, and brain state decoding based on spatio-temporal kernel methods.
9332 en Bernstein Focus: Neurotechnology Berlin The Bernstein Focus: Neurotechnology Berlin (BFNT-B) posits that neuroscientific results can be exploited for developing robust ‘real-world’ applications that have a major potential for (also non-medical) industry. Similar to the new paradigm of medical research ‘from bench to bedside and back’, the center brings together a multidisciplinary faculty with the aim of directly applying insights from basic neuroscience to relevant applications (‘from bench to desktop and back’). The major aim of the BFNT-B is to foster novel noninvasive ‘brain reading’ techniques to enhance man-machine interactions. Their contributions will be evaluated, e.g. in the future-oriented field of usability studies for telecommunications systems and services, or driver-assisted measures for vehicle safety.
9333 en Bernstein Focus: Neurotechnology Freiburg In spite of considerable progress towards prosthetic devices controlled by neuronal signals, brain-machine interfaces and other neurotechnological devices, however, user-friendly, neurotechnical devices for everyday use remain a vision of the future, with numerous fundamental biological, technical, computational, clinical, and ethical problems still to be solved. The aim of the BCNT-FT consortium is the development of bidirectional hybrid neurotechnical devices for human usage. This will be implemented in three research clusters, each consisting of projects organized around a common goal: to understand the principles, to advance technology, and to explore and extend clinical applications. Each project addresses issues central to neurotechnology, from basic questions on decoding neuronal signals, interfacing biological neuronal networks to technical devices, actuators and real-time feedback systems, via the stable recording and interpretation of neuronal signals, to the clinical testing and application of new technologies. Research in the BCNT-FT will be supported by a matching, interdisciplinary training program for neurotechnology. In collaboration with industrial, applied and clinical partners neuroprosthetical devices for biomedical application will be developed.
9490 en Business Cases for Enterprise Interoperability - The Andalusian Aeronautics Business Case 
9492 en eBIZ-TCF: An Initiative to Improve eAdoption in European Textile/Clothing and Footwear Industry 
9493 en Debate on Business Cases for Enterprise Interoperability 
9494 en Business Cases for Enterprise Interoperability Collaborative Demand Capacity Planning (CDCP) 
9495 en iSURF – Piacenza Knitwear Business Case 
9496 en Collaboration and Interoperability in Production Management of Ship-Building Industry 
9497 en The eGov Financial Reporting Business Case 
9498 en Welcome to the COIN4LL Workshop - "Unleashing Open Innovation with Enterprise Collaboration & Interoperability Services, the Living Lab Way" 
9499 en Unleashing Open Innovation potential in Living Labs by Enterprise Interoperability and Collaboration Services: the COIN project 
9500 en Living Labs in Open Innovation Functional Regions 
9502 en Methodologies for Engaging Users into Research & Innovation: The Living Lab way as an Open Innovation Ecosystem 
9504 en EI Science Base initiative: a state-of-play from the Commission 
9505 en Task Forceon Enterprise Interoperability Science Base 
9506 en On the Scientific Basis of Enterprise Interoperability 
9507 en Towards Semantic Interoperability in an Evolving Environment 
9508 en Dealing with Interoperability:An Agent-Oriented Perspective 
9510 en System Theory to support Enterprise Interoperability Science Base 
9511 en Towards EI as a science: Considerations and points of view 
9512 en Breeding Business Success - West midlawos collaborative commerce marketplace 
9533 en Multi-Strategy Trading Utilizing Market Regimes This video considers the problem of dynamically allocating capital to a portfolio of trading strategies.nThe allocation should be robust, and the capital allocated to a trading strategy should reflectnthe confidence in the expected profit that the strategy will make in current market conditions.nGood trading strategies exploit recurring market dynamics that can be more prevalent in some timenperiods than in others. Indeed, the concept of regimes is fundamental to financial markets, and muchnresearch has focused on the detection of regime shifts. In this paper, we consider a regime as definednby a set of trading strategies that exhibit similar performance in a given time period.nWe consider different parameterizations of the same strategy as distinct in our ground set of strategies.nThe trading problem is to pick a distribution over the ground set that will achieve good performancenin the current time period. That we typically choose a distribution of support greater thannone reflects uncertainty on many levels, and allows diversification of risk and return drivers.nWe provide a simple algorithm that empirically picks distributions that often approximate the performancenof an oracle that picks the best trading strategy in each period from the ground set. Tonthis end, we explicitly define regimes as subsets of strategies. An initial phase is to rule out a largennumber of regimes as irrelevant to counter the combinatorial explosion of dealing with subsets.nIn the training phase of our algorithm, we pick random time windows and learn two functions: thenfirst, classifyMarket, is for (probabilistic) regime classification and takes as input the market datanand produces a distribution over regimes; the second, stFuncDist, produces for each regime a distributionnover strategies, where strategies believed to be good in that regime are assigned highernprobability. The main tools we use are Monte Carlo permutation tests and incremental re-weightingnof probabilities. In the trading phase we use a standard “walk-forward” approach. In the in-samplenperiod we use the trading results for regime classification, and in the out-of-sample period we allocatencapital according to the combination classifyMarket determined from the in-sample period andnthe current stFuncDist. This is a simple algorithm, but an empirically successful one - an indicationnof which we report. The approach bears some similarity to Sequential Monte Carlo methods [3] innthat it sequentially re-weights hypotheses (in our case, regarding suitability of strategies).nIn the final section, we discuss an approach to modelling the time evolution of strategy fitnessesnwith a view towards characterizing regimes. This could be used to guide our choice of in-samplenof out-of-sample periods in the existing setup. We present preliminary results in this direction. Inncurrent work, we are trying to extend the basic algorithm in such a way that we can more directlynmake use of the Sequential Monte Carlo method, such as particle filter based estimation of strategynfitness that might parsimoniously accomplish what is done above with permutation tests.
9534 en The More Rating: New Model -a comparative analysis of different companies from different countries-" This video discusses a specific model for assessing credit risk of Industrialncompanies by using financial statement data and industry-specific information.nIn particular, the model permits each enterprise to associate a fundamental creditnrating giving an indication of the creditworthiness of industrial companies.
9537 en Dynamic Portfolio Management with Transaction Costs We develop a recurrent reinforcement learning (RRL) system that directly inducesnportfolio management policies from time series of asset prices and indicators,nwhile accounting for transaction costs. The RRL approach learns a direct mappingnfrom indicator series to portfolio weights, bypassing the need to explicitlynmodel the time series of price returns. The resulting policies dynamically optimizenthe portfolio Sharpe ratio, while incorporating changing conditions and transactionncosts. A key problem with many portfolio optimization methods, includingnMarkowitz, is discovering ”corner solutions” with weight concentrated on just anfew assets. In a dynamic context, naive portfolio algorithms can exhibit switchingnbehavior, particularly when transaction costs are ignored. In this work, we extendnthe RRL approach to produce better diversified portfolios and smoother assetnallocations over time. The solutions we propose are to include realistic transactionncosts and to shrink portfolio weights toward the prior portfolio. The methodsnare assessed on a global asset allocation problem consisting of the Pacific, NorthnAmerica and Europe MSCI International Equity Indices.
9538 en Modelling decimalisation in the Nasdaq stockmarket 
9540 en The Effect of Reinforcement Learning Agents in Double-Auction Markets Several time series models such as ARCH and GARCH have been developed to forecast volatilitynusing asset returns data. However, these methods ignore one key source of market volatility: financialnnews. Similarly, asset pricing models often describe the arrival of novel information by a jumpnprocess, but the characteristics of the underlying jump process are only coarsely, if at all, related tonthe underlying news source. Our objective in this paper is to show that recent advances in statisticalnlearning allow a much more refined analysis of the impact of news on asset prices.nIn this paper, we demonstrate that information from press releases can be used to predict intradaynabnormal returns with relatively high accuracy. We form a text classification problem wherenpress releases are labeled positive if the absolute return jumps at some (fixed) time after the newsnis made public. First, abnormal returns are predicted using support vector machines in similar fashionnto [1]. Given a press release, we predict whether or not an abnormal return will occur in thennext 10, 20, ..., 250 minutes using either text or past absolute returns. Our experiments analyze predictabilitynat many horizons and demonstrate significant initial intraday predictability that decreasesnthroughout the trading day. Second, we optimally combine text information with asset price timenseries to significantly enhance classification performance using multiple kernel learning (MKL).Wenuse an analytic center cutting planemethod (ACCPM) to solve the resultingMKL problem. ACCPMnis particularly efficient on problems where the objective function and gradient are hard to evaluatenbut whose feasible set is simple enough so that analytic centers can be computed efficiently. Furthermore,nbecause it does not suffer from conditioning issues, ACCPM can achieve higher precisionntargets than other first-order methods.
9541 en Machine Learning Based Prediction in Financial Markets Forecasting models generally do a very poor job in predicting future returns in financial markets whichnare characterized by high levels of noise. It is commonly known that predictions of typical autoregressiventime series forecasting models, for example, are usually very conservative – meaning close to the mean –nbecause of the inherent difficulty of making accurate forecasts. This situation is particularly discouragingnsince it is the large future values that we especially care about predicting accurately. Machine learningnmethods provide a potential solution to this problem by providing multiple models that each of whichnmakes forecasts of different magnitudes. The obvious problem with this approach is overfitting,nespecially if the learner has no constraints on how complex its models can be. In this talk, I present thentradeoffs between increased the model complexity and forecast accuracy based on a mix of trading resultsnand current research.
9542 en Modelling Financial Time Series using Grammatical Evolution The traditional models of price, and its statistical signatures are often based onnlimiting assumptions, such as linearity. Moreover, the model developer is facednwith the model selection problem, and model uncertainty. In this paper we introducena method based on Grammatical Evolution (GE) to evolve models fornpredicting financial returns, and we examine the profitability of these models. Ournempirical analysis demonstrates that for some securities our method is able to producenmodels of return that are lead to more profitable trading compared with annAutoregressive model picked using Aikake Information Criterion (AIC), under thenassumption of frictionless markets.
9544 en Modeling Dependence in Financial Data with Semiparametric Archimedean Copulas Copulas are useful tools for the construction of multivariate models because theynallow to link univariate marginals into a joint model with arbitrary dependencenstructure. While non-parametric copula models can have poor generalization performance,nstandard parametric copulas often lack expressive capacity to capturenthe dependencies present in financial data. In this work, we propose a novelnsemiparametric bivariate Archimedean copula model that is expressed in termsnof a latent function. This latent function is approximated using a basis of naturalnsplines and the model parameters are selected by maximum penalized likelihood.nExperiments on financial data are used to evaluate the accuracy of the proposednestimator with respect to other benchmark methods: Two flexible estimators ofnArchimedean copulas previously introduced in the literature, two approaches forncopula estimation that allow for more general dependencies and three parametricncopulas models. The proposed semiparametric copula model has excellent in andnout-of-sample performance, which makes it a useful tool for modeling multivariatenfinancial data.
9548 en Empirical Portfolio Selection Dark pools are a relatively recent type of equities exchange in whichntransparency is deliberately limited in order to minimize the marketnimpact of large-volume trades. The success and proliferation of dark npools has also led to a challenging and interesting problem innalgorithmic trading --- namely, optimizing the distribution of a large ntrade over multiple competing dark pools. In this work we formalizenthis as a problem of multi-venue exploration from censored data, andnprovide a provably efficient and near-optimal algorithm for its solution.nThis algorithm and its analysis has much in common with well-studiednalgorithms for exploration-exploitation in reinforcement learning, andnis evaluated on dark pool execution data from a large brokerage.n
9549 en Dynamic Asset Allocation for Bivariate Enhanced Index Tracking using Sparse PLS Index tracking is a popular portfolio management strategy which involves creating a portfolio whosenreturns track very closely those achieved by a benchmark index. There are two interconnected problemsnassociated with index tracking: asset selection and asset allocation. Asset selection involvesnselecting a subset of p out of n available assets, whereas asset allocation involves investing a proportionnof the total available capital in each one of the p assets with the objective of reproducing thenperformance of the index. The capital invested in asset i is in proportion i of the total capital so thatnPpni=1 i = 1. These portfolio weights are generally estimated by minimzing the tracking error, thatnis the error between the index returns yt and the portfolio returns ˆy, given by T?1PTnt=1(yt ? ˆyt)2.nFollowing this setting, the problem of asset allocation becomes a standard regression problem withnthe portfolio weights being the parameters to be estimated.nIn the literature, only a few attempts have been made to tackle both the asset selection and allocationnproblems at the same time; for instance, [8] use a quadratic programming approach and the methodnin [3] is based on genetic algorithms. Our interest lies in taking a unified approach which simultaneouslynselects a subset of assets in the available basket and minimizes the tracking error. We take anregularized regression approach. In a full index replication scenario, asset selection can be thoughtnof as assigning certain assets a zero weight, so that those assets are not included in the portfolio,nwhereas the selected one should be able to reproduce the index. These ideas have recently beennexploited in the context of minimum variance portfolios and L1-penalized least squares have beennproved to be a promising method for creating robust portfolios [4]; see also the related work by [5].nWe extend on these ideas in three main ways. Firstly, we consider a multivariate version of the indexntracking problem, where the selected portfolio is expected to reproduce the performance of multiplenindices. Secondly, we are interested in enhanced versions of the indices to be tracked so that thenportfolio is also expected to overperform each index by a given annual percentage return (say, plusn15%). Thirdly, we propose a methodology that works well in real-time and is fully adaptive, in thensense that both the asset allocation and optimization solutions can be updated in a recursive manner,nkeeping the number of computations low, every time new data points are made available. Thisnlast feature makes the methodology more robust against non-stationarities presented in the data andnyelds superior tracking results, before transaction costs.
9551 en Are markets efficient? A view from micro-structural data Efficient market theory posits that market prices reflect at any instant of time the fundamental value of assets, and can only change because of unpredictable news or other information items that affect this fundamental value. If true, well, systematic quantitative strategies should not work. But of course this picture cannot strictly hold – for one thing someone should process information and push the price towards its putative true value. There should be at least some kind of tatonnement and arbitrage opportunities at high frequencies. In order to dissect these possible mispricing mechanisms and to devise profitable high frequency trading and execution models, the detailed study of order flow and order books has become mandatory. nnMuch as in physics, where the detailed understanding of the microscopic world provides invaluable insight on macroscopic phenomena, we believe that a consistent picture of the microstructure mechanisms will help put in perspective some of the traditional questions about markets and prices, such as: “Are prices in equilibrium?”, “What is the information content of these prices?”, or “Why is the volatility so high?”. It will also allow one to optimize execution costs, which for large AUMs is mostly due to impact.nnEmpirical data reveals an unexpectedly subtle price formation mechanism. Order flow turns out to be a highly persistent, long memory process, both in sign and volume. This reflects the fact that even on very liquid markets, the revealed liquidity is in fact extremely small (typically 0.001% of the market cap of a stock). Large orders to buy or sell can only be traded incrementally, over periods of time as long as months. Hence prices cannot be instantaneously in equilibrium, and cannot instantaneously reflect all available information. There is nearly always a substantial offset between latent offer and latent demand that only slowly gets incorporated in prices. nnMaintaining compatibility with market efficiency has profound consequences on price formation, on the dynamics of liquidity, and on the nature of impact. On anonymous, electronic markets, there cannot be any distinction between “informed” trades and “uninformed” trades. The average impact of all trades must be the same, which means that impact must have a mechanical origin: if everything is otherwise held constant, the appearance of an extra buyer (seller) must on average move the price up (down).nnnA body of theory that makes detailed quantitative predictions about the volume and lag dependence of market impact, the bid-ask spread, order book dynamics,nand volatility has been recently put forth [1,2,3,4]. This framework allows one to make quantitative models of execution costs in terms of a time dependent (non-local) friction kernel. nnIt also suggests a novel interpretation of financial information. The theory of rational expectations and efficient markets has increasingly emphasized information and belittled the role of supply and demand, in contradiction with the intuition of traders and of the layman. Our recent work on the role of news on price jumps [5] also shows that information in the traditional sense is not the main driver of market volatility. Rather, we highlight the role of fluctuations in supply and demand, which may or may not be exogenous, and may or may not be informed – it does not really matter. Attempts to estimate ex-post the fraction of truly informed trades actually leads to very small numbers, at least judged on a short time basis, meaning that the concept of informed trades is not very useful to understand what is going on in markets at high frequencies. But still, prices manage to be almost perfectly unpredictable, even on very short time scales. The conclusion is that any useful notion of information must be internal to the market: trades, order flow, cancellations are information, whatever the final cause of these events may be.n
9553 en Predicting Abnormal Returns From News Using Text Classification 
9554 en Modeling the S&P 500 Index using the Kalman Filter and the LagLasso This video introduces a method to predict upward and downward monthly variationsnof the S&P 500 index by using a pool of macro-economic and financialnexplicative variables. The method is based on the combination of a denoisingnstep, performed by Kalman filtering, with a variable selection step, performednby a Lasso-type procedure. In particular, we propose an implementation of thenLasso method called LagLasso which includes selection of lags for individual factors.nWe provide promising backtesting results of the prediction model based on annaive trading rule.
9579 en QCA and fuzzy sets This course examines the family of 'configurational comparative methods' (CCM). First, the course spells out the fundamental concepts that underlie the configurational comparative approach. In the framework of the general literature on comparative empirical social research, participants are made familiar with issues such as concept formation, truth tables, basic Boolean algebra, ideal types, and property spaces. Then participants are trained to use the most widely used of the CCM so far: dichotomous Qualitative Comparative Analysis (csQCA). The practical steps and best practices of csQCA (including software use: TOSMANA and fs/QCA) are taught: first the basic procedures, then various refinements. The course is concluded with an overview of linked developments such as fuzzy set QCA (fsQCA) and multi-value QCA (mvQCA) and the combination of QCA with other methods. Real-life, published applications are used throughout the course; participants are also encouraged to bring their own data, if available. Some basic quantitative or qualitative methodological training is probably useful to get more out of the course, but participants with little methodological training should find no major obstacles to follow the course. Above all, participants should be motivated to engage in rigorous comparative analysis.
9580 en Multiple regression analysis The course starts with a discussion of the logic of the multivariate regression model and the central assumptions underlying the ordinary least squares approach. Then it proceeds with testing for the adequacy of the assumptions and suitable corrections and extensions to the estimation techniques in the context of cross-sectional data. Particular emphasis will be laid on multicollinearity and heteroskedasticity. The second part of the course focuses on functional form. Models that are nonlinear in variables but linear in parameters, dummy variables, and interaction terms will be covered. In the third part, various topics arising with special data are covered. Firstly, the analysis of binary dependent variables is introduced. Secondly, problems involved with the analysis of longitudinal data, i.e. time series and panel data, are discussed, with special emphasis on autocorrelation. The course assumes proficiency with descriptive and inferential statistics at the level of test theory and bivariate regression analysis.
9581 en Mixed methods research This interactive course provides new and seasoned researchers with a framework in a step-by-step manner for using quantitative and qualitative research approaches within the same study. The instructor will provide many published examples and illustrate how to conduct mixed methods research using both statistical software such as SPSS and qualitative software such as QDA Miner. Participants will be able to understand the historical underpinnings of mixed methods research, define and explain mixed methods research in its current form, describe the major steps in the mixed methods research process, identify goals and objectives for mixed methods research studies, identify mixed methods techniques for conducting literature reviews, identify the rationale and purpose for mixing quantitative and qualitative approaches, design mixed methods research questions, describe the role of sampling in the mixed methods research process, compare and contrast several mixed methods research designs, describe several ways of collecting data in mixed methods research studies, conduct mixed methods data analyses, link research questions to mixed methods data analysis techniques, identify how to make quality meta-inferences, explain the major legitimation types in mixed methods research, and understand issues and standards involved in conducting, reporting, and publishing mixed methods research.
9582 en Creative Environment and Young Researchers Performance: The Keys to PhD Students Success The results of the study (also conducted by Hajdeja Igli?, Franc Mali, Uro? Mateli?nand Petra Ziherl) presented in this lecture confirm other empirical results obtained bynsociological research on the activities of research groups (e.g., Hemlin, Allwood in Martinn2004): creativity or research productivity of young researchers is strongly influenced by intrasocialnfactors such as autonomy, flexibility, cooperation, etc. More precisely, this studynconfirms that, at the micro level, the relationships among the researchers, especially betweennthe young researcher and his/her mentor, play an important role for the young researcher'snscientific performance. This is especially true in the phase of the socialisation andnprofessionalisation of young scientists (during the Ph.D. research).
9583 en ‘Creative Environment and Young Researchers’ Performance: The Keys to PhD After the lecture and a reaction from the discussant, an open debate will be held with thenaudience, both on substantive and methodological issues.
9586 en Computer Verified Exact Analysis This tutorial will illustrate how to use the Coq proof assistant to implement effective and provably correct computation for analysis. Coq provides a dependently typed functional programming language that allows users to specify both programs and formal proofs.nnWe will introduce dependent type theory and show how it can be used to develop both mathematics and programming. We will show how to use dependent type theory to implement constructive analysis. Specifically we will cover how to implement effective real numbers and effective integration.nnThis work will be done using the Coq proof assistant. The tutorial will cover how to use the Coq proof assistant. Attendees are encouraged to download and install Coq 8.2 from http://coq.inria.fr/download and also download and make the full system of C-CoRN from http://c-corn.cs.ru.nl/download.html beforehand.
9664 en The Evolutions of Cybercrime: Is the Email of the Species still more Deadlier than the Mail? This talk will discuss the implications of technology for criminal behaviour and its control. The first part will briefly outline how networked technology has transformed /is transforming crime, crime control, policing and surveillance. It will chart the development of cybercrime through three generational changes from discrete computers systems to dial-in modem access to broadband. It will then map out the significant transformations and the challenges they create, especially the need to deal with informationalized, networked and globalised small impact bulk victimisations that do not fall within the routine activities and experiences of criminal justice systems and the professionals who work within them. nnThe second part will explore recent developments in cybercrimes that are continuing to challenge criminal justice systems. For example, as dial in computer networking became common place during the late 1990s the practice of bulk spamming, through their content and also attachments, were arguably the most prevalent form of online victimisation. The prevalence of spamming proliferated dramatically from mid 2003 onwards following the advent of broadband and the growth of botnets (robot networks of remotely administered infected computers). However, in more recent years we have experienced the emergence of new forms of victimization online which are showing interesting and distinct signs of evolution from their predecessors. nnBotnets and the threats from spams remain prevalent, but the explosion in social networking sites has added potential opportunities for online victimisation. Not only are the there now many more opportunities for obtaining personal information about individuals who live out large parts of their lives online, but these social networks can also be exploited by socially engineering (persuading) participants to pass on information to the various nodes in their personal networks. These information flows can be used to perform more advanced forms of phishing expeditions. In deed, in the worst case scenario they can contribute to mass panics through the intentionally or accidental flow of misinformation. Not only is email being surpassed as the primary form of threat but it will be suggested in this talk that these new victimisations are potentially more invasive. nnThe third part of the talk will seek to identify some of the new developments in networked technology that could be used to initiate future online attacks in the future. These include ambient technologies, new generations of Radio Frequency Identity Tags, but also the impact of governmental and commercial policies which increase our reliance upon technology. Whilst some of these predictions may be speculative, one thing remains certain, cybercrime is not an absolute concept – it will not be eradicated, rather it is relative to online business and social opportunities and it is therefore a function of that opportunity. As a consequence cybercrime will never be eradicated; the best that we can hope to achieve is that it can be prevented through social information as quickly as potential exploits in operating and financial systems can be identified. To this end we need to develop new methodologies for understanding change as it happens and also to explore the various relationships between technology and law in the process of regulating harmful behaviour online.
9672 en Online Child Pornography: European and Hungarian Perspective 
9824 en Privacy in Web Search Query Log Mining Web search engines have changed our lives - enabling instant access to information about subjects that are both deeply important to us, as well as passing whims. The search engines that provide answers to our search queries also log those queries, in order to improve their algorithms.nAcademic research on search queries has shown that they can provide valuable information on diverse topics including word and phrase similarity, topical seasonality and may even have potential for sociology, as well as providing a barometer of the popularity of many subjects. At the same time, individuals are rightly concerned about what the consequences of accidental leaking or deliberate sharing of this information may mean for their privacy. In this talk I will cover the applications which have benefited from mining query logs, the risks that privacy can be breached by sharing query logs, and current algorithms for mining logs in a way to prevent privacy breaches.
9825 en Theory-Practice Interplay in Machine Learning – Emerging Theoretical Challenges Theoretical analysis has played a major role in some of the most prominent practical successes of statistical machine learning. However, mainstream machine learning theory assumes some strong simplifying assumptions which are often unrealistic. In the past decade, the practice of machine learning has led to the development of various heuristic paradigms that answer the needs of a vastly growing range of applications. Many useful such paradigms fall beyond the scope of the currently available analysis. Will theory play a similar pivotal role in the newly emerging sub areas of machine learning?nIn this talk, I will survey some such application-motivated theoretical challenges. In particular, I will discuss recent developments in the theoretical analysis of semi-supervised learning, multi-task learning, “learning to learn”, privacy-preserving learning and more.
9826 en Highly Multilingual News Analysis Applications The publicly accessible Europe Media Monitor (EMM) family of applications (http://press.jrc.it/overview.html) gather and analyse an average of 80,000 to 100,000 online news articles per day in up to 43 languages. Through the extraction of meta-information in these articles, they provide an aggregated view of the news, they allow to monitor trends and to navigate the news over time and even across languages. EMM-NewsExplorer additionally collects historical information about persons and organisations from the multilingual news, generates co-occurrence and quotation-based social networks, and more. All EMM applications were entirely developed at, and are being maintained by, the European Commission’s Joint Research Centre (JRC) in Ispra, Italy.nnThe applications make combined use of a variety of text analysis tools, including clustering, multi-label document classification, named entity recognition, name variant matching across languages and writing systems, topic detection and tracking, event scenario template filling, and more. Due to the high number of languages covered, linguistics-poor methods were used for the development of these text mining components. See the site http://langtech.jrc.it/ for technical details and a list of publications.nnThe speaker will give an overview of the various applications and will then explain the workings of selected text analysis components.
9827 en The Growing Semantic Web From its beginnings in 2004, the data available on the web in Semantic Web formats has typically been both eclectic and relatively small, and closely linked the interests of particular researchers. In the past year, however, the quantity and scope of data published on the public semantic web has exploded, and the size of the semantic web is now measured in the billions of assertions. It is a significant and growing resource for applications which depend on web-based resources for some or all of their knowledge. With this massive increase in quantity and scope come many opportunities, as well as the usual issues of scale on the web: inconsistency, mapping problems, incompleteness and data variability. This talk will cover the history and current state of the Semantic Web and the Linked Data Cloud, describe some of the uses to which web-based semantic data is currently put, and discuss prospects for the ECML/PKDD community to leverage this growing web of data.
9828 en Are We There Yet? Statistical approaches to Artificial Intelligence are behind most success stories of the field in the past decade. The idea of generating non-trivial behaviour by analysing vast amounts of data has enabled recommendation systems, search engines, spam filters, optical character recognition, machine translation and speech recognition. As we celebrate the spectacular achievements of this line of research, we need to assess its full potential, its limitations and its position within the larger scheme of things. What are the next steps to take towards machine intelligence?
9983 en Artificial Business Intelligence: Scaling Beyond the Real World with Cyc and LarKC In the last few years significant advancement has been achieved in semantic, knowledge and context technologies as well as in methods for knowledge management. These technologies are becoming especially effective when applied to the capture, formalization and automated reuse of knowledge. In particular, these techniques have been demonstrated by Cycorp in specific intelligence and medical domains. Equally, though they may be applied to problems of managing business complexity to provide ABI - Artificial Business Intelligence.nThe explosion of availability of free and open information resources following the emergence of the Web2.0 paradigm has widened the prospects for constructing real Artificial Intelligence solutions that are able to learn, to reason and to speculate.nnIn my talk I'll discuss the general class of problems that should be solvable in the near term, in part by exploiting available knowledge, and in part by collaboration between people and machines. I'll show some examples of partial solutions, and describe in some detail the components of a more complete solution. The discussion will focus on the issue of scaling AI techniques up to real applications, both in terms of very large, inferentially sophisticated knowledges bases, like Cyc, and in terms of techniques for web scale inference - the goal of the FP7 LarKC project.
9984 en Link Mining Statistical machine learning is in the midst of a "relational revolution". After many decades of focusing on independent and identically-distributed (iid) examples, researchers are now studying problems in which the examples are linked together into complex networks. These networks can be a simple as sequences and 2-D meshes (such as those arising in part-of-speech tagging and remote sensing) or as complex as the collaboration structures produced by knowledge workers performing a variety of tasks in different contexts within an enterprise. In this talk, I will give an overview of this newly emerging research, focusing specifically on link mining tasks and algorithms.
9985 en Text Mining and Light Weight Semantics 
9986 en Informal Knowledge Processes: The Long Tail of Business Processes 
9987 en Lizzon: A Professional Messaging Utility for Distributing Relevant Information within Enterprise in Real-Time 
9988 en Taking Anysite.com to the Next Level with SEO, Web Analytics, Testing, Data Mining, Targeting, Forecasting and Web Personalization 
9989 en Introduction to the ACTIVE Knowledge Workspace SDK This session will provide a short introduction into the ACTIVE knowledge workspace with online demonstration of the basic Workspace features.nWorkspace software architecture will be overviewed and then the ACTIVE Software Developer Kit will be presented. As an SDK usage example the scenario for developing an ACTIVated application will be explained.
9990 en A Cabinet of Web 2.0 Scientific Curiositics 
9991 en On Future Internet, Cloud Computing and Semantics - You Name It This presentation will present various developments in the Internet of Services. Research in the Internet of Services is put in the context of ongoing research on the Future Internet. The role of semantics in this research domain will also be illustrated. In addition, this presentation will consider industry relevance by focusing on well-known business models based on Software-as-a-Service and Cloud Computing. A number of business strategies will be discussed as well. Some glimpses of future research will also be given.
9992 en Semantic Technology in Business Systems: Status and Prospects 
9994 en Research Directions in Enterprise Knowledge Management 
9995 en The Intelligent Cargo Concept in the European Project EURIDICE 
9996 en KM at the Customer Front-Line: The BT Case Study in ACTIVE 
10001 en Opening speech / Pozdravni nagovor 
10005 en The Emergence of Life on Earth: Mystery or Scientific Problem? / Nastanek ?ivljenja na Zemlji: misterij ali znanstveni problem? Until the end of the 19th century, the origin of life on Earth was explained either as the creative act of god or as the result of "spontaneous generation" – the repeated formation of complete organisms from inanimate matter. Only within an evolutionary conception, following Darwin's theory, could the origin of life on the ancient Earth begin to be studied scientifically. Scientists still didn't find an empirical answer to this difficult question and are divided on the nature of the first systems to emerge. They agree however that natural selection was actively involved in the process. They are also united in rejecting the creationistic claims and in supporting the naturalistic worldview.n----nIzvor ?ivljenja so vse do konca 19. stoletja razlagali ali kot bo?je stvarstvo ali kot rezultatnspontanega nastanka, torej kot ponovni nastanek celotnih organizmov iz ne?ive snovi. ?ele znevolucijskim konceptom, ki je sledil Darwinovi teoriji, so ga za?eli preu?evati znanstveno.nRaziskovalci ?e vedno niso na?li empiri?nega odgovora na to te?ko vpra?anje in se delijo glede nanprepri?anje o pojavu prvih sistemov. Kljub vsemu se strinjajo, da je bil naravni izbor aktivno vklju?ennv ta proces. Enakega mnenja so tudi pri zavra?anju kreacionisti?nih razlag in pri podpiranjunnaturalisti?nega svetovnega nazora.
10006 en The Major Transitions in Evolution / Veliki prehodi v evoluciji Some major transitions in evolution (such as the origin of multicellular organisms or that of social animals) occurred a number of times, whereas others (the origin of the genetic code, or language) seem to have been unique events. One must be cautious with the word ‘unique’, however. Due to a lack of the ‘true’ phylogeny of all extinct and extant organisms, one can give it only an operational definition. If all the extant and fossil species, which possess traits arising from a particular transition, share a last common ancestor after that transition, then the transition is said to be unique. Obviously, it is quite possible that there had been independent “trials”, as it were, but we do not have comparative or fossil evidence for them.
10010 en The Gene in Context: from Developmental Plasticity to Plastic Heredity / Gen v kontekstu: od razvojne plasti?nosti do plasti?nega dedovanja The gene-centered view, according to which genes are the most important determinants of development, the sole stuff of biological heredity, and the guide for the understanding of all aspects of evolution, has dominated biological thinking for the last 50 years. This view is now changing. In this lecture I point to the perils of “genetic astrology” – predicting the future and analyzing personality on the basis of DNA sequencing. I present a developmental approach to biology and evolution which stresses the importance of phenotypic plasticity and which incorporates epigenetic inheritance. This approach points to the many sources of developmental and hereditary variations, answers some of the puzzles that the gene-centered approach failed to solve, and leads to a more satisfying and fertile biological outlook and research.n----nZadnjih petdeset let v biolo?kem razmi?ljanju prevladuje genocentri?en pogled, po kateremnso geni najpomembnej?i dejavnik, ki vpliva na ontogenetski razvoj organizmov, edina osnova zanbiolo?ko dedovanje in vodilo za razumevanje vseh vidikov evolucije. Ta pogled se v zadnjem ?asunspreminja. V tem prispevku bom izpostavila nevarnosti "genetske astrologije", ki napovedujenprihodnost in analizira osebnost na podlagi zaporedja DNA. Predstavila bom razvojni pogled nanbiologijo in evolucijo, ki poudarja pomen fenotipske plasti?nosti in vklju?uje epigenetsko dedovanje.nTak pristop poka?e na mnogo vzrokov za ontogenetske in dedne raznolikosti, odgovori na nekaterenuganke, ki jih genocentri?ni pogled ne re?i, in obeta zadovoljivej?e in plodnej?e raziskovanje vnprihodnosti biologije.
10011 en Cooperation – A Successful Principle in the Living World / Sodelovanje med organizmi kot princip v ?ivih sistemih Iz dana?njega zornega kota lahko re?emo, da nobena ?ival ne pre?ivi brez posebnih sodelujo?ihnpartnerjev. V nekaterih primerih so taka sodelovanja celo spremenila Zemljo in prinesla novenpokrajine. Torej niso bile vpletene samo populacije in vrste, temve? veliko ve?ji sistemi, biocenoze,nekosistemi in pokrajine, vklju?no z obse?nimi obmo?ji oceanskega dna.nNadalje, sodelovanje omogo?a tudi prebavo. To sodelovanje je posebno izrazito pri termitih, ki jihnnaseljujejo enoceli?ni bi?karji, ki se nahajajo v posebnem delu ?revesja. Ti bi?karji v sodelovanju znbakterijami omogo?ajo razgradnjo lignina. ?e umrejo bi?karji, tudi termiti ne pre?ivijo. Podobnonpre?vekovalci razgrajujejo celulozo v hrani. Migetalkarji, ki ?ivijo v njihovih ?elodcih, omogo?ajo – snpomo?jo bakterij – prebavljivost hrane. Danes je splo?no znano, da v prehranjevalnem traktu ?lovekan?ivi ve? prokariontov, kot je vseh celic v telesu. Lahko bi na?teli ?e veliko podobnih primerov.nSklenemo lahko, da vse ?ivali za prebavlajnje potrebujejo prisotnost in sodelovanje drugihnorganizmov v njihovih prebavilih.nV mnogih primerih sodelovanje vodi v ?isto nov razvoj. Najbolj prepri?ljiv primer so li?aji, ki sonsestavljeni iz gliv in enoceli?nih alg ali cianobakterij. Li?aji ?ivijo skoraj v vseh kopenskihnekosistemih, vklju?no z Antarktiko, uspevajo na morski obali, lahko so tudi vodni.nRaznolikost dana?njih cvetnic ne bi bila mogo?a brez opra?itve, posebno z ?u?elkami kot tudi s pti?inin sesalci. Cvet in opra?evalec sta ?la skozi dolgo skupno evolucijo. Veliko cvetov potrebuje zelonposebne opra?evalce. ?e teh opra?evalcev ni, lahko rastline izumrejo.nPrenos (transport) je ?e en primer za obse?no sodelovanje. Mrtva telesa in iztrebki se morajo razkrojiti, do kon?nega produkta, ki so minerali, tako da ekosistemi ostanejo nepo?kodovani. Mrtva telesa in iztrebki privla?ijo mnogo ?u?elk, ki imajo druge pomembne organizme, ki so nujno potrebni za razkroj, obenem pa jih pripeljejo do naslednjega izvora hrane. Hro??i prena?ajo gliste in pr?ice kot tudi bakterije in tako omogo?ajo zaprtje biokemijskega kroga .nNa koncu pa ?e podmena: Nobena ?ival, vklju?no z dana?njim ?lovekom, ni sposobna pre?iveti breznmedvrstnega sodelovanja. ?e je le-to v kriti?nem stanju, se vsi lahko soo?imo z globalnimi okoljskiminproblemi.
10014 en Evolution and How Microbes See It / Evolucija z vidika mikrobov With the beginning of life under early Earth condition the microorganisms developed metabolic pathways for energy regeneration and carbon assimilation. Due to stepwise improvements new developments were introduced resulting finally in microbial life forms of today, more or less stable geochemical cycles and a remarkable changed world. For humankind a very minor fraction of today’s microorganisms can cause medical problems. Most of them are essential members of the ecosystem World and for human beings directly or indirectly of great importance. Nevertheless, there are several new habitats introduced by us which are invaded by them and where they cause major problems.n----nZ za?etkom ?ivljenja, v pogojih na zgodnji Zemlji, so mikroorganizmi razvili metabolnenpoti za pridobivanje energije in asimilacijo ogljika. S postopnimi izbolj?avami so razvijali novenlastnosti, ?emur so sledili razmah mikrobnih oblik ?ivljenja, kot ga poznamo danes, bolj ali manjnstabilni geokemi?ni cikli in mo?no spremenjen svet. Za zdravje ?love?tva je nevaren le zelo majhenndel dana?njih mikroorganizmov. Ve?ina jih je pomembnih gradnikov svtovnega ekosistema in sonneposredno ali posredno izjemno pomembni za ?loveka. Ne nazadnje, naselili so nekatera novanprebivali??a, ki smo jih razvili mi, in tam lahko povzro?ajo resne te?ave
10020 en Urban Ecology - Why Is It an Increasingly Important Topic? / Urbana ekologija – zakaj se njen pomen pove?uje? Land-use changes due rapidly increasing urbanization pose major threat on ecosystems and their functions worldwide. By the end of this decade, more than half of the world human population will be living in cities and other urbanized areas. Human-induced habitats, loss and fragmentation of the landscape as well as accumulation of trace gases and other pollutants serve as the best-known examples of negative impacts of dense human population on the surrounding environment. Land-use changes can be seen analogous to soil use, as well-developed old soils are replaced by functionally altered soils or even by completely new substrates, called “made lands”. Such conversion of land and soils to urban use is likely to translate into changes in soil biota, thereby distorting life-supporting ecosystem services, such as decomposition of organic matter, cycling of nutrients and detoxification of harmful substances. Urban soils are traditionally described as being highly artificial and disturbed, although soils in urban parks and gardens can share many features typical to agricultural, even natural soils. One of the key factors distinguishing urban soils from natural soils is their high spatial heterogeneity in the various ecological patterns and processes. This high heterogeneity manifests as parks, cemeteries, vacant lots, streams and lakes, gardens and yards, campus areas, golf courses, bridges, air ports and landfills. These habitats are highly dynamic, influenced by both biophysical and ecological drivers on the one hand and social and economic drivers on the other. Active management of green spaces is vital but seldom sufficient, there is also a need to protect, restore and manage surrounding ecosystems in order to maintain ecosystem services of value for human well-being and build resilience in the urban landscape. nRecent climate models predict precipitation to increase under various climatic conditions, especially in heavily urbanized areas. Combined with increasing proportion of sealed, impermeable surfaces in urban settings, the growing trend of rain events are likely to cause anomalies in hydrological cycles. A typical example of it is the increased quantity and worsened quality of urban runoff waters (“street waters”). In this presentation urban runoff waters will be dealt as an indicator of the ecological health of urbanized habitats. Furthermore, the typicalities of urban vs. natural ecosystems are compared and some examples of the ongoing urban ecological studies in Finland will be presented.n----nZaradi hitro nara??ajo?e urbanizacije prihaja do sprememb v rabi zemlji??, kar predstavljanpomembno gro?njo ekosistemom in njihovemu delovanju ?irom po svetu. Do konca tega desetletja bonve? kot polovica svetovne populacije ?ivela v mestih in drugih urbaniziranih obmo?jih. Habitati, ki jihnspreminja ?lovek, izguba in fragmentacija pokrajine kot tudi spro??anje plinov v sledovih in drugihnonesna?evalcev slu?ijo kot najbolj znani primeri negativnih vplivov goste ?love?ke populacije nanokolje. Spremembe v rabi zemlji?? lahko gledamo kot izrabo tal, ko se dobro razvita stara prstnnadome??a s funkcionalno spremenjeno prstjo ali celo s povsem novim substratom, tako imenovanon"umetno zemljo". Tak?no spreminjanje zemlji?? za potrebe urbane uporabe najverjetneje vodi vnspremembe v ?ivljenju v prsti in tako poru?i ?ivljenjske podporne mehanizme ekosistema, kot sonrazgradnja organskih snovi, kro?enje hranil in razstrupljanje ?kodljivih snovi. Zemljo v urbanihnobmo?jih po navadi opisujejo kot umetno in mo?no prizadeto, vendar pa je prst v mestnih parkih innvrtovih lahko zelo podobna kmetijski ali celo naravni prsti. Eden glavnih dejavnikov, po katerih senzemlja v mestih lo?i od tiste v naravi zunaj mest, je velika prostorska heterogenost razli?nih ekolo?kihnvzorcev in procesov. Visoko heterogenost ka?ejo okolja kot so parki, pokopali??a, gradbi??a, potoki innjezera, vrtovi in dvori??a, univerzitetna naselja, igri??a za golf, mostovi, letali??a in smeti??a. Tinhabitati so zelo dinami?ni, nanje po eni strani vplivajo biofizikalni in ekolo?ki dejavniki, po druginstrani pa socialne in ekonomske te?nje. Aktivno upravljanje zelenih prostorov je zelo pomembno,nvendar redko zadostuje; treba je ??ititi, obnavljati in upravljati tudi okoli?nje ekosisteme, zato da senvzdr?ujejo podporne storitve v ekosistemih, ki so pomembne za blagostanje ljudi in omogo?ajonpro?nost urbane krajine.nZadnji klimatski modeli napovedujejo pove?anje koli?ine padavin v razli?nih klimatskih pogojih, ?enposebno v mo?no urbaniziranih obmo?jih. V povezavi z vedno ve?jim dele?em zatesnjenih,nneprepustnih povr?in v urbanih okoljih je zelo verjetno, da bo nara??ajo?i trend de?ja povzro?ilnanomalije v hidrolo?kih ciklih. Tipi?ni primer tega je pove?ana koli?ina in poslab?ana kakovostnurbanih povr?insko odtekajo?ih voda ("uli?ne vode"). V tej predstavitvi bodo urbane odto?ne vodenuporabljene kot kazalnik ekolo?kega zdravja urbanih ?ivljenjskih okolij. Nadalje bodo predstavljeninprimerjava zna?ilnosti urbanih in naravnih ekosistemov ter nekaj primerov potekajo?ih urbanihnekolo?kih ?tudij na Finskem.
10022 en The Young Charles Darwin - Student, Naturalist and Gardener / Mladi Charles Darwin – ?tudent, naravoslovec in vrtnar Charles Darwin was one of the most important scientists who ever lived. He was born in 1809 (200 years ago) in Shrewsbury, Shropshire, England. He was fortunate to have a wealthy father who was a doctor and be part of an intelligent and educated family. He lived in a large house called ‘The Mount’ with servants and a large garden, on the edge of the town and open countryside. He had developed a strong interest in natural history before he attended his first school at the age of eight, having been taught previously by his mother, brother and sisters. Many poor children of his age never had the opportunity to attend school. However, he was unfortunate because his mother died soon after he started at day school. He was sent to Shrewsbury Grammar School as a boarder for seven years where he studied Latin and Greek but little that interested him. Science was not taught in schools at that time but Charles developed his knowledge by collecting natural history specimens and helping his older brother with Chemistry experiments at home. He was not an exceptional child at school but his father taught him about observation, recording and analysis of information in the garden and surgery, assisting with his patients and diagnosing illnesses. This prepared him for Edinburgh University where he studied to become a doctor but he ‘dropped out’ because he did not like operations or dissections and was sent to Cambridge University to train for The Church (to become a priest). However, both in Edinburgh and Cambridge he disliked lectures, preferring to read about the subjects in books and spent most of his time having a good time with his friends and collecting beetles. However, he worked hard enough to pass examinations and achieved a good Bachelor of Arts Degree as a first step to becoming a clergyman. That career path was cut short because he had impressed influential professors and lecturers who admired his scientific prowess at a time when the study of natural history (studying the wonders of God’s Creation) was considered an appropriate pastime for a clergyman! He was recommended for the role of ship’s naturalist on the survey ship ‘The Beagle’ because of his natural history skills and because he was ‘a gentleman’ of the right social background to be the captain’s companion. His self-motivated study of plants, animals and rocks commenced as a child in Shrewsbury and encouraged at last by perceptive teachers, propelled him into a new direction. However, his five year voyage on The Beagle around the world and his subsequent life of research would have been impossible if his father had not supported him financially and later, when he married, invested a large sum of money, the income from which supported his research, wife and family. He had been raised in a one-parent family and did not do well at school where his interests were dismissed as a waste of time. However, he had the support of his family and eventually, at university, found the support of teachers who recognised his potential. n----nCharles Darwin je bil eden najpomembnej?ih znanstvenikov na?e zgodovine! Rodil se je letan1809 (pred 200 leti), v Shrewsburyju, v okraju Shropshire, v Angliji. Dobro popotnico so munpredstavljali premo?en o?e, po poklicu zdravnik, ter izobra?ena in razgledana dru?ina. ?iveli so nanobrobju mesta, v veliki hi?i, imenovani »The Mount« (Gora) s slu?abniki in velikim vrtom.nNaravoslovje je Charlesa za?elo zanimati, ?e ko so ga pou?evali mati, brat in sestre, ?e preden je prinosmih letih za?el obiskovati v ?olo. Mnogi revnej?i otroci tedaj do ?olanja sploh niso pri?li. A tudinmalemu Charlesu ni bilo lahko, saj mu je ?e kmalu po vstopu v ?olo umrla mati. Sedem let je prebil nangimnaziji v Shrewsburyju, kjer je bival v internatu. Tam se je u?il latin??ino in gr??ino, a ga to ninpreve? zanimalo. Naravoslovnih predmetov tedaj v ?olah niso pou?evali, a Charles je svojennaravoslovno znanje razvijal sam, med drugim tudi s tem, ko je doma starej?emu bratu pomagal prinkemijskih poizkusih. V ?oli ni bil izjemen u?enec, a ga je o?e na doma?em vrtu ter v ordinaciji prinkirur?kih operacijah in diagnosticiranju bolezni u?il opazovati, bele?iti in analizirati podatke. Tako senje pripravil na ?tudij medicine na Edinbur?ki univerzi. Ker pa ni maral operacij in seciranja, ?tudija nindokon?al. Poizkusil je ?e v Cambridgeu, s ?tudijem teologije. Tako kot v Edinburgu je tudi tu ugotovil,nda ga zelo odbijajo vsa predavanja, in je raje ?tudiral iz knjig, veliko ?asa pa posvetil tudi zabavi snprijatelji in zbiranju hro??ev. Kljub vsemu je uspe?no opravil izpite in bil na pravi poti donduhovni?kega poklica. Na vplivne profesorje je s svojim naravoslovnim znanjem naredil mo?an vtis,nin to v ?asu, ko je veljalo naravoslovje (ali »preu?evanje ?udes bo?jega stvarjenja«) za primernonprosto?asno dejavnost duhovnikov. Njegova nesojena duhovni?ka kariera je bila prekinjena, ko so ga,nzaradi naravoslovnega znanja in ker je izhajal iz »primerne« dru?ine, predlagali za »ladijskegannaravoslovca« in kapitanovega spremljevalca na raziskovalni ladji Beagle. Vztrajno samostojnonprou?evanje rastlin, ?ivali in kamnin, za?eto v otro?tvu in nazadnje vzpodbujeno s strani dojemljivihnu?iteljev, ga je pognalo v novo smer. Vendar njegovo petletno potovanje z ladjo Beagle okoli sveta innnjegovo kasnej?e raziskovalno delo ne bi bila mo?na brez o?etove izdatne finan?ne pomo?i. Tudi ponCharlesovi poroki je o?e vlo?il veliko denarja v investicije, s prihodki katerih je Charles pla?eval svojenraziskovalno delo in vzdr?eval dru?ino. Odrasel je v enostar?evski dru?ini, v ?olah, kjer so njegovonzanimanje za naravoslovje ozna?ili za izgubo ?asa, pa ni bil preve? uspe?en. Kljub temu je vselej imelnpodporo dru?ine in kasneje na univerzi vendarle na?el tudi razumevanje in podporo u?iteljev, ki sonprepoznali njegove zmo?nosti.
10026 en Biological Education for the 21st Century: Educating the Next Generation for Tomorrow’s Society / Biolo?ka izobrazba za 21. stoletje: Izobra?evanje nove generacije za dru?bo prihodnosti Against the background of a changing world and major developments in biological sciences, this paper addresses two overarching questions: What can biological education contribute to the general education of young people in 21st Century? and How might we approach biological education in order to meet the future demands of society? It argues that, by teaching about and through biology, we can contribute to the general education of young people enabling them to become successful learners, confident individuals and responsible citizens. Furthermore it outlines some principles to guide the development of the biology curriculum, emphasises the importance of young peoples’ own ideas in teaching and learning and highlights the key role of formative assessment. In considering biological education for 21st Century the paper emphasises: the need for different sectors (government, education, science and industry) to work together, the need to develop creative approaches to pedagogy and, importantly, the need to hold high expectations for quality and achievement.n----nOb spreminjajo?em se svetu in velikem napredku biolo?ke znanosti si v ?lanku zastavljamnkrovni vpra?anji: Kaj lahko biolo?ko izobra?evanje prispeva k splo?ni izobrazbi mladih v 21. stoletju?nin Kako naj zasnujemo biolo?ko izobra?evanje, da bo ustrezalo prihodnjim potrebam dru?be? Snpou?evanjem o biologiji in skozi biologijo lahko prispevamo k splo?ni izobrazbi mladih, da se bodonuspe?no u?ili ter postali samozavestne osebe in odgovorni dr?avljani. Poleg tega predstavljam nekaterensmernice za razvoj u?nih na?rtov za biologijo, pomen lastnih predstav mladih pri pou?evanju in u?enjunter klju?no vlogo sprotnega preverjanja znanja. Za biolo?ko izobra?evanje v 21. stoletju so pomembninsodelovanje med razli?nimi sektorji (vlado, izobra?evanjem, znanostjo in industrijo), razvojnustvarjalnih pristopov k pedagogiki ter visoka pri?akovanja o kakovosti in dose?kih u?encev.
10096 en Crime, Conflict and the Racialization of Criminal Law The scars from riots in 47 American cities in the late 1960s remain visible today not onlynin the physical landscape of a few stubbornly poor cities, but in a philosophy and jurisprudencenof criminal law that has instantiated the disparate fates of racial minoritiesnin the criminal justice system. The riots took place in the midst of profound social andneconomic restructuring of the nation’s cities, and at the outset of an epidemic of risingnrates of crime and disorder that framed both a new political order and a profoundntransformation of American criminal law and criminal procedure. Within the decade, anpolitical and legal mobilization – fueled by racial and cultural conflict – led to an abruptnreversal in the substance and philosophy of criminal law. The ceding of rights to criminalndefendants and the increasing regulation of police in the early 1960s gave way throughna series of cascading court decisions and new laws to a punitive regime that, over threendecades, has expanded the authority of police, curtailed the procedural rights of criminalndefendants, and supported policies that have sustained a widening racial gap innincarceration. A new body of criminal laws, though facially race-neutral, have had profoundnracial consequences that are durable and sustainable even in a low crime era. PartnI of this paper discusses the antecedents, contexts and dynamics of this turn in criminalnlaw, focusing on the racial dynamics that exploded in a wave of riots. Part II examinesnthe development of the new legal order, analyzing the reversal in law through a series ofncourt decisions over two decades. Part III examines how the use of race-neutral laws tonachieve crime control in the absence of complementary models of social regulation hasnproduced disparate racial impacts that have become endogenous to the political order.nPart IV discusses the challenges to legal and institutional reform to restore racial equalitynin criminal justice. The American experience is a cautionary tale of the limits and dangersnof criminal law to manage diversity and social conflict.
10097 en Slovenia: Crime Policy in Time of Change Traditionally Slovenia had a comparatively low level of violent and organized crime andncomparatively low imprisonment rate. In addition, it successfully resisted to punitivenessnin all its aspects, starting on the level of prescribed sentences and incarcerationnrates. Recently, however, we can witness several new developments in its crime policy.nFirst, we can observe the growing interest of populist politics in the crime policy issues.nThe fear of and the fight against crime has risen higher among the priorities of the electionsncampaigns and every day politics. Connected to that, the potential of academianand research institutions to influence crime policy decision making has been declining,nas has been notably demonstrated in the haste passing of the new Criminal Code inn2008. Finally, as the EU is becoming more and more interested in harmonizing criminalnlaw and crime policy, the extending criminal law powers of the EU erode Slovenia’s sovereigntynin this respect. In my presentation I will discuss the relevance of these developmentsnfor the future.
10098 en Redeeming Redemption as a Criminological Concept In the past two decades, the science of criminology has focused considerable attentionnon the topic of desistance from crime or how and why individuals active in crime “gonstraight.” This research has been instrumental in the design and assessment of strategiesnfor reducing recidivism through resettlement or reintegration support. Criminologists,nhowever, have had little to say about the issue of “redemption” or what it should requirenfor individuals to be officially forgiven of their crimes and have their “good names” restored.nIn this talk, I will outline the need for a discussion of secular redemption in societynand discuss the implications of criminological research in this regard.
10099 en Slovenian Criminology: Its Beginnings, Development and State of the Art The early beginnings of the literature on Slovenian Criminology go back to the earlyn1920s when Fran Mil?inski, a judge with enough literary talent to depict complex issuesnof law and crime, published the first Slovenian novel on juvenile delinquency. Innthe 1930s, Aleksander Vasiljevi? Maklecov, a Russian criminal lawyer and refugee fromnRussia, joined the Faculty of Law in Ljubljana and became well known for his reflectionsnupon several criminological issues and problems, of which many are still topicalntoday: alcohol abuse and juvenile delinquency prevention, women and crime, crimenin newspapers, etc. He is also the author of the first Slovenian textbook on criminology,n‘Introduction to criminology’, (1948). In 1950, the Secretariat of the Internal Affairs,nhaving established a research unit to study crime and delinquency, started publishingn‘Criminal Investigation Topics’, a professional journal to gradually evolve into ‘Journal ofnCriminal Investigation and Criminology’, now an SSCI journal. In addition, there are twonmore recent journals dealing with criminological topics: ‘Social Education’ and ‘Journalnof Criminal Justice and Security’. The Institute of Criminology was established in 1954,nand its first director was Hinko Lu?ovnik. The first research project, finished in 1957, hasnso far been followed by more than 150 research projects. The most fertile developmentnera of the Institute’s (positivistic) research endeavours was that under Katja Vodopivec,nMaklecov’s PhD graduate (1943) and the second director of the Institute. She establishednSlovenian criminology as a science on equal footing with other disciplines and headedna huge number of research projects. Her successors, as directors of the Institute, werenJanez Pe?ar, Alenka ?elih and Matja? Jager. In addition to the Institute of Criminology,nother institutions have developed in this field of expertise, studying crime, delinquency,nsafety and security issues and other crime related problems (Social Education Depart72nment at the Faculty of Education, UL; Faculty of Law, UM, and Faculty of Criminal Justicenand Security, UM). Slovenian criminology developed in several stages making significantncontributions and enjoying international recognition. The leading perspectives on thisndiscussion relate to crime control and prevention, the impact of Slovenian criminologicalnthought on policy making, international projects, publications, as well as contributionsnto global criminology.
10100 en Caught between crime control and human rights? The fall of the Berlin wall in November 1989 has not brought the “end of history” butnthe beginning of a new era of instability, uncertainty and conflict. One of the outputsnof the opening of the borders in Europe has been the rapid increase of trafficking in humannbeings in the 1990s, coupled with the rise of research on the phenomenon of humanntrafficking. Despite all research done it remains very difficult to get a good grip onnthe phenomenon. Albrecht, using a variety of sources, mentions estimates of betweenn200,000–500,000 women who are trafficked to Western Europe for the purpose of prostitutionnon an annual basis, many of them coming from the former socialist countries ofnCentral and Eastern Europe. Since the 1990s as well the issue of trafficking has becomenthe object of systems of crime control worldwide, both through criminal legislation andncriminal justice system enforcement. Both national states and international organisationsnhave adopted a variety of legal instruments that make trafficking a criminal offencenand have reinforced the investigation and the prosecution of such offences. Enforcement,nhowever, remains a problem in most countries, partly because of the close relationshipnwith organised crime, partly because victims are not always willing to testify outnof fear for revenge. At the same time it is interesting to note a shift from conceiving humanntrafficking as a law enforcement problem relating to offenders to understanding itnas a serious violation of human rights and putting the emphasis on the human dignity ofnthe victims. This shift has also facilitated the development of protection mechanisms fornvictims, including access to justice for them, which at the same time may strengthen thenmodel of law enforcement. In this contribution I will focus both on the phenomenon ofntrafficking, its antecedents and its manifestations, as well as on the policies designed toncombat trafficking. In doing so, I will also highlight fundamental conceptual paradigmsnused to view the phenomenon, such as ‘moral panics’ (Cohen), left realism (Young et al.)nand crime and human rights (Parmentier and Weitekamp).
10101 en Smuggling and Trafficking of Human Beings in the Balkan Area The paper intends to give and overview and critical analyses of causes and recent trendsnin smuggling and trafficking of human beings in the Balkan area, as well as of criminalnpolicies and mechanisms for protection of victims. First, the problems related to definitionsnand distinction between smuggling and trafficking will be analysed in relation tongeographical position of Balkan, and attention will be drown to the scarce serious academicnresearch in this field, as well as to overall low quality of data about victims, perpetratorsnand policies. Then, main contributing factors and recent trends will be explored,nsuggesting large parts of population being involved in smuggling and trafficking “business”,nwhile large portions of victims are invisible and stay out of any support system.nParticular attention will be paid to both victims and perpetrators coming from marginalisedngroups, as well as to victims who are invisible because they do not fit widespread stereotypes about victims (illegal migrants, men, sexual workers, street children, childrennused by organised crime for guiding victims etc.). Criminal policies and mechanisms fornprotection of victims will then be analysed in relation to the role and social position ofnperpetrators, and socio-economic status, social visibility and stereotypes related to bothnvictims and perpetrators. I will argue that, in spite of obvious positive developmentsnrecent years, comprehensive and holistic approach to smuggling and trafficking in humannbeings, which goes beyond stigmatization, “otherness” and social exclusion, is notnestablished neither in Balkan, nor in Western countries. In the concluding part, the papernwill suggest possible directions toward policy which would be oriented toward preventionnand social inclusion of both victims and perpetrators, with more victims assistednand human rights of both protected.
10102 en Crime and Globalisation Post-industrial societies are facing changes in the way people internalize social norms,nwhat they feel guilty about, when they experience shame and how they perceive punishment.nIdentification with traditional authorities which have in the past transmittednsocial norms has been declining for some time. Individualism has been pushed to itsnlimits. And transgression of norms which comes from global capital, international financialninstitution and state governments has often been cherished as a matter of progress.nUnder the veil of ideology of perpetual economic growth on the societal level and advancementnof self-fulfilment on the individual level, the definition of what counts asntransgression has been globally altered. Redefinition of what counts as a limit, what isnthe nature of the prohibition and what are publicly acceptable forms of remorse as wellnas individually experienced anxieties in regard to prohibitions also underwent a change.nFeeling of guilt and shame often accompanies individual’s striving towards creating annimage of perfect life and not so much transgression of moral rules and the legal order. Innthis context the definition of crime has radically changed, too. How can criminology respondnto these changes? As an interdisciplinary discipline it needs to in a new way assessnthe way malaise of the civilization affects the malaise of the individual and vice versa. Inntrying to understand this connection, some lessons from contemporary psychoanalyticnknowledge might be of help, especially the reasoning that utilitarianism ultimately failed in its perception that people work towards advancement of their well being and minimalizationnof pain. Current economic crisis, for example, cannot be explained throughnthis framework – rather we need to look at it through the prism of an enjoyment in selfdestructionnwhich has always been the hidden underside of progress.
10103 en Crime Policy between Effective Crime Control and Human Rights Protection Crime policy has been facing the dilemma of whether human rights standards shouldnbe upheld or the effectiveness strengthened for quite some time. For the Central andnEast European countries this dilemma is of particular importance because human rightsnhave been the motivating factor in the processes of the 1990ies democratization. Socialnchanges that occurred after the 1990ies have coincided with an ongoing change in crimenpolicy in the “free world”: the liberal and humane crime policy has been slowly replacednby just desert, law and order and other more punitive policies in this field. Crime policyndevelopments in the last decade, especially after 9/11, have been especially orientedntowards achieving greater effectiveness – also at the cost of human rights. Crime policiesnin different European countries have adopted measures that either diminished humannrights standards; in some cases, zone of criminality has been extended beyond criminalnoffences; in others, law enforcements measures can be applied to persons very remotelynconnected or not connected at all to the criminal offence. Policies and measures, clearlyndisregarding human rights standards, have been taken by legislative as well as executivenauthorities at the European as well as at national levels. The economic crisis may be thentesting point for future orientation of crime policy: is it going into a fully punitive direction,nor may it be oriented towards its earlier goals – effectiveness by respecting humannrights standards – in new ways?
10105 en What Are Patterns? **//Disclaimer:// Videolectures.Net emphasises that the audio quality of this video could not be improved,ndue to poor audio quality conditions provided in the lecture auditorium.**
10106 en Pattern Analysis and Scientific Method, Privacy and Intelligent Machines **//Disclaimer:// Videolectures.Net emphasises that the audio quality of this video could not be improved,ndue to poor audio quality conditions provided in the lecture auditorium.**
10110 en Three Hours on Multiple Classififier Systems Motivations and basic concepts Motivationsnof multiple classifier systems. The “worst” case and “best” case motivations.nPractical and theoretical motivations. Basic concepts. Architectures fornmultiple classifier systems. Ensemble types, combiner types. The concept ofnclassifier “diversity”. The design cycle of a multiple classifier system.nnCreating multiple classifiers Systematicnmethods for creating classifier ensembles. Methodsnbased on training data manipulation: data splitting methods, Bagging andnBoosting. Methodsnbased on input and output feature manipulation: feature selection, the RandomnSubspace method, noise injection, and error-correcting codes.nnCombining multiple classifiers Methodsnfor combining multiple classifiers at the “abstract” level (voting methods, thenBehaviour Knowledge Space method, etc.) Methodsnfor combining multiple classifiers at the “rank” level (the Borda count method,netc.) Methodsnfor combining multiple classifiers at the “measurement” level (linearncombiners, the product rule, etc.) Basicnconcepts on dynamic classifier selection methods.
10111 en Statistical Significance and Stability Analysis for Patterns 
10112 en Pattern Analysis over Graphs, and Bioinformatics Applications 1. Classification and regression over graphs. Overview:npositive definite graph kernels based on walk, subtrees etc.., as wellnas other non p.d. similarity functions (eg from graph matching) thatncan be used to compare graphs and do classification/regression withnkernel methods. Applications: QSAR in chemistry, image classificationnn2. Detecting patterns in the context of regression or classification with a graph as prior knowledge over the features. nOverview:nin a classical regression/classification problem over high-dimensionalnvectors. Control the complexity, by using priors that can be derivednfrom the graph over the vectors, and how they can be used as penaltynfunctions for classification and regression. This will cover diffusionnkernels and other kernels over graphs, fused lasso, structured groupnlasso. Application in bioinformatics.
10114 en Patterns: Significant vs. Accidental 
10116 en Analysis of Patterns for Computer Security 
10118 en IFA for the Diagnosis of a Railway Infrastructure Device in a Semi-Supervised Learning 
10783 en Welcome and Introduction by Organizer and Partners 
10784 en Leveraging New Technologies for Process Flexibility in Logistics Service Providers 
10785 en ICT - An Enabler for Efficient and Sustainable Transport Logistics 
10786 en ICT Research to Support Freight and Logistics 
10787 en The Intelligent Cargo Concept in the European Project EURIDICE 
10789 en Panel Discussion: Challenges Met in the Logistic Sector 
10790 en Hardware and Related Software Technologies (Track and Trace, Monitoring, Data Aquisition) 
10791 en Frameworks for a Cargo Centric Approach 
10792 en The Unexploited Potential of Savings in the European Road Transport Industry 
10793 en Impact of Current and Future Technologies – Visions Beyond Existing 
10794 en Results of the KOMODA Delphi Survey on e-Logistics 
10796 en Impact Assessment Study on Intelligent Cargo 
11099 en Furniture scenario - Delocalisation with production networks to countries with cheaper human efforts, or skill competencies 
11141 en Relative Entropy An overview of relative entropy (aka Kulback-Leibler divergence, etc.) and its multiple appearances in information theory, probability, and statistics, including recent results by the speaker.
11142 en Deep Learning with Multiplicative Interactions Deep networks can be learned efficiently from unlabeled data. The layers of representation are learned one at a time using a simple learning module that has only one layer of latent variables. The values of the latent variables of one module form the data for training the next module. The most commonly used modules are Restricted Boltzmann Machines or autoencoders with a sparsity penalty on the hidden activities. Although deep networks have been quite successful for tasks such as object recognition, information retrieval, and modeling motion capture data, the simple learning modules do not have multiplicative interactions which are very useful for some types of data. The talk will show how a third-order energy function can be factorized to yield a simple learning module that retains advantageous properties of a Restricted Boltzmann Machine such as very simple exact inference and a very simple learning rule based on pair-wise statistics. The new module contains multiplicative interactions that are useful for a variety of unsupervised learning tasks. Researchers at the University of Toronto have been using this type of module to extract oriented energy from image patches and dense flow fields from image sequences. The new module can also be used to allow the style of a motion to blend auto regressive models of motion capture data. Finally, the new module can be used to combine an eye-position with a feature-vector to allow a system that has a variable resolution retina to integrate information about shape over many fixations.
11143 en The Rat Vibrissal Array as a Model Sensorimotor System The rat whisker (vibrissal) system has neural processing pathways that are analogous to human tactile pathways through the spinal cord. Rats use rhythmic (5-25 Hz) movements of their whiskers to actually extract object features, including size, shape, orientation, and texture. In this talk, I will describe our efforts to characterize whisker mechanical properties and to determine the set of mechanical variables that could potentially be sensed by the rat. We have also developed a hardware array of artificial whiskers that makes use of these variables to successfully extract 3-dimensional object features. On the behavioral side, we have developed a laser light sheet to visualize whisker-object contact patterns during natural exploratory behavior and have used this technique to examine the relative roles of head and whisker movements in generating patterns of mechanosensory input across the array during natural exploratory sequences. Finally, we are developing a simulation environment that integrates our knowledge of whisker mechanics with our knowledge of head and whisker motion to enable the user to model the rat's interactions with various objects in the environment. Our goal is to predict the contact patterns – and resulting forces and moments – at each whisker base for a given exploratory sequence, and then to predict the resulting responses of trigeminal ganglion neurons.
11144 en Learning and Inference in Low-Level Vision Low level vision addresses the issues of labeling and organizing image pixels according to scene related properties - such as motion, contrast, depth and reﬂectance. I will describe our attempts to understand low-level vision in humans and machines as optimal inference given the statistics of the world. If time permits, I will discuss my favorite NIPS rejected papers.nYair Weiss is an Associate Professor at the Hebrew University School of Computer Science and Engineering. He received his Ph.D. from MIT working with Ted Adelson on motion analysis and did postdoctoral work at UC Berkeley. Since 2005 he has been a fellow of the Canadian Institute for Advanced Research. With his students and colleagues he has co-authored award winning papers in NIPS (2002),ECCV (2006), UAI (2008) and CVPR (2009).nnSlide presentation contains animation videos which can be found at [[http://www.cs.huji.ac.il/~yweiss/movies.tar.gz]].
11145 en Bayesian Analysis of Markov Chains Suppose we observe data and want to test if it comes from a Markov chain. If it does, we may want to estimate the transition operator. Working in a Bayesian way, we have to specify priors and compute posteriors. Interesting things happen if we want to put priors on reversible Markov chains. There are useful connections with reinforced random walk (work with Silke Rolles). On large-scale application to protein folding will be described. More generally, these problems arise in approximating a dynamical system by a Markov chain. For continuous state spaces, the usual conjugate prior analysis breaks down. Thesis work of Wai Liu (Stanford) gives useful families of priors where computations are "easy." These seem to work well in test problems and can be proved consistent.
11147 en Making Very Large-Scale Linear Algebraic Computations Possible Via Randomization The demands on software for analyzing data are rapidly increasing as ever larger data sets are generated in medical imaging, in analyzing large networks such as the World Wide Web, in image and video processing, and in a range of other applications. To handle this avalanche of data, any software used must be able to fully exploit modern hardware characterized by multiple processors and capacious but slow memory. The evolution of computer architecture is currently forcing a shift in algorithm design away from the classical algorithms that were designed for single-processor computers with all the data available in Random Access Memory (RAM), towards algorithms that aggressively minimize communication costs. This tutorial will describe a set of recently developed techniques for standard linear algebraic computations (such as computing a partial singular value decomposition of a matrix) that are very well suited for implementation on multi-core or other parallel architectures, and for processing data stored on disk, or streamed. These techniques are based on the use of randomized sampling to reduce the effective dimensionality of the data. Remarkably, randomized sampling does not only loosen communication constraints, but does so while maintaining, or even improving, the accuracy and robustness of existing deterministic techniques.
11148 en Understanding Visual Scenes One remarkable aspect of human vision is the ability to understand the meaning of a novel image or event quickly and effortlessly. Within a single glance, we can comprehend the semantic category of a place, its spatial layout as well as identify many of the objects that compose the scene. Approaching human abilities at scene understanding is a current challenge for computer vision systems. The field of scene understanding is multidisciplinary, involving a growing collection of works in computer vision, machine learning, cognitive psychology, and neuroscience. In this tutorial, we will focus on recent work on computer vision attempting to solve the tasks of scene recognition and classification, visual context representation, object recognition in context, drawing parallelism with work in psychology and neuroscience. Devising systems that solve scene and object recognition in an integrated fashion will lead towards more efficient and robust artificial visual understanding systems.
11149 en Model-Based Reinforcement Learning In model-based reinforcement learning, an agent uses its experience to construct a representation of the control dynamics of its environment. It can then predict the outcome of its actions and make decisions that maximize its learning and task performance. This tutorial will survey work in this area with an emphasis on recent results. Topics will include: Efficient learning in the PAC-MDP formalism, Bayesian reinforcement learning, models and linear function approximation, recent advances in planning.
11150 en Sparse Methods for Machine Learning: Theory and Algorithms Regularization by the L1-norm has attracted a lot of interest in recent years in statistics, machine learning and signal processing. In the context of least-square linear regression, the problem is usually referred to as the Lasso or basis pursuit. Much of the early effort has been dedicated to algorithms to solve the optimization problem efficiently, either through first-order methods, or through homotopy methods that leads to the entire regularization path (i.e., the set of solutions for all values of the regularization parameters) at the cost of a single matrix inversion. A well-known property of the regularization by the L1-norm is the sparsity of the solutions, i.e., it leads to loading vectors with many zeros, and thus performs model selection on top of regularization. Recent works have looked precisely at the model consistency of the Lasso, i.e., if we know that the data were generated from a sparse loading vector, does the Lasso actually recover the sparsity pattern when the number of observations grows? Moreover, how many irrelevant variables could we consider while still being able to infer correctly the relevant ones? The objective of the tutorial is to give a unified overview of the recent contributions of sparse convex methods to machine learning, both in terms of theory and algorithms. The course will be divided in three parts: in the first part, the focus will be on the regular L1-norm and variable selection, introducing key algorithms and key theoretical results. Then, several more structured machine learning problems will be discussed, on vectors (second part) and matrices (third part), such as multi-task learning, sparse principal component analysis, multiple kernel learning and sparse coding.
11151 en Deep Learning in Natural Language Processing This tutorial will describe recent advances in deep learning techniques for Natural Language Processing (NLP). Traditional NLP approaches favour shallow systems, possibly cascaded, with adequate hand-crafted features. In constrast, we are interested in end-to-end architectures: these systems include several feature layers, with increasing abstraction at each layer. Compared to shallow systems, these feature layers are learnt for the task of interest, and do not require any engineering. We will show how neural networks are naturally well suited for end-to-end learning in NLP tasks. We will study multi-tasking different tasks, new semi-supervised learning techniques adapted to these deep architectures, and review end-to-end structured output learning. Finally, we will highlight how some of these advances can be applied to other fields of research, like computer vision, as well.
11152 en Sequential Monte-Carlo Methods Over the last fifteen years, sequential Monte Carlo (SMC) methods gained popularity as powerful tools for solving intractable inference problems arising in the modelling of sequential data. Much effort was devoted to the development of SMC methods, known as particle filters (PFs), for estimating the filtering distribution of the latent variables in dynamic models. This line of research produced many algorithms, including auxiliary-variable PFs, marginal PFs, the resample-move algorithm and Rao-Blackwellised PFs. It also led to many applications in tracking, computer vision, robotics and econometrics. The theoretical properties of these methods were also studied extensively in this period. Although PFs occupied the center-stage, significant progress was also attained in the development of SMC methods for parameter estimation, online EM, particle smoothing and SMC techniques for control and planning. Various SMC algorithms were also designed to approximate sequences of unnormalized functions, thus allowing for the computation of eigen-pairs of large matrices and kernel operators. Recently, frameworks for building efficient high-dimensional proposal distributions for MCMC using SMC methods were proposed. These allow us to design effective MCMC algorithms in complex scenarios where standard strategies failed. Such methods have been demonstrated on a number of domains, including simulated tempering, Dirichlet process mixtures, nonlinear non-Gaussian state-space models, protein folding and stochastic differential equations. Finally, SMC methods were also generalized to carry out approximate inference in static models. This is typically done by constructing a sequence of probability distributions, which starts with an easy-to-sample-from distribution and which converges to the desired target distribution. These SMC methods have been successfully applied to notoriously hard problems, such as inference in Boltzmann machines, marginal parameter estimation and nonlinear Bayesian experimental design. In this tutorial, we will introduce the classical SMC methods and expose the audience to the new developments in the field.
11179 en Towards Brain Computer Interfacing: Algorithms for on-line Differentiation of Neuroelectric Activities Brain Computer Interfacing (BCI) aims at making use of brain signalsnfor e.g. the control of objects, spelling, gaming and so on. This talknwill first provide a brief overview of Brain Computer Interfacenfrom a machine learning and signal processing perspective. Innparticular it shows the wealth, the complexity and the difficulties ofnthe data available, a truly enormous challenge: In real-time anmulti-variate very strongly noise contaminated data stream is to benprocessed and neuroelectric activities are to be accuratelyndifferentiated in real time.nnFinally, I report in more detail about the Berlin Brain Computern(BBCI) Interface that is based on EEG signals and take the audiencenall the way from the measured signal, the preprocessing and filtering,nthe classification to the respective application. Â BCI as a newnchannel for man-machine communication is discussed in a clinicalnsetting and for gaming.
11180 en Machine Learning for Brain-Computer Interfaces Brain-computer interfaces (BCI) aim to be the ultimate in assistive technology: ndecoding a user's intentions directly from brain signals without involving any nmuscles or peripheral nerves. Thus, some classes of BCI potentially offer hope nfor users with even the most extreme cases of paralysis, such as in late-stage nAmyotrophic Lateral Sclerosis, where nothing else currently allows communication nof any kind.  Other lines in BCI research aim to restore lost motor function in as nnatural a way as possible, reconnecting and in some cases re-training motor-cortical nareas to control prosthetic, or previously paretic, limbs. Research and development nare progressing on both invasive and non-invasive fronts, although BCI has yet to nmake a breakthrough to widespread clinical application.nnThe high-noise high-dimensional nature of brain-signals, particularly in non-invasivenapproaches and in patient populations, make robust decoding techniques a necessity. nGenerally, the approach has been to use relatively simple feature extraction techniques, nsuch as template matching and band-power estimation, coupled to simple linear classifiers. nThis has led to a prevailing view among applied BCI researchers that (sophisticated) nmachine-learning is irrelevant since "it doesn't matter what classifier you use once you've ndone your preprocessing right and extracted the right features." I shall show a few examples nof how this runs counter to both the empirical reality and the spirit of what needs to be done nto bring BCI into clinical application. Along the way I'll highlight some of the interesting nproblems that remain open for machine-learners.
11181 en An Efficient P300-based Brain-Computer Interface with Minimal Calibration Time Brain-Computer Interfaces (BCI) are communication systems that enable subjects to send commandsnto computers by using only their brain activity [1]. Most existing BCI are based on ElectroEncephaloGraphyn(EEG) as the measure of brain activity [1]. So far, BCI have been proven to benvery promising communication and control tools for disabled people [1]. A promising brain signalsnused in the design of assistive BCI for disabled people is the P300, a positive waveform occuringnroughly 300 ms after a rare and relevant stimulus [1, 2]. In order to use a P300-based BCI, subjectsnhave to focus their attention on a given stimulus randomly appearing among many others, eachnstimulus corresponding to a given command. The appearance of the desired stimulus being rare andnrelevant, it is expected to trigger a P300 in the subject’s brain activity. As such, detecting the P300nenables the system to identify the desired stimulus and hence the desired command. Interestinglynenough, P300-based BCI have been successfully used to control a wheelchair (see, e.g., [3]) or tonenable severely disabled users to spell words [2, 4].nnHowever, current P300-based BCI as well as other BCI systems still suffer from several limitationsnwhich prevent them from being widely used [1]. One of these limitations is that to use a BCI, manynexamples of the subject’s EEG signals must be recorded in order to calibrate the BCI, which isnunconvenient and time consuming. Moreover, this calibration process generally has to be repeatednat regular intervals (e.g., from one day to the other) in order to accomodate sources of variationsnsuch as changes in electrode positions or changes in the subject’s mental state and fatigue level.nTherefore, the calibration time should be maintained as brief as possible. Until now, reducing thencalibration time of P300-based BCI has been scarcely addressed by the literature. Exceptions arenthe works of Li et al [5] and Lu et al [6]. Li et al suggested to use initially a BCI calibrated withnfew training samples, and then to incrementally adapt this BCI online, thanks to semi-supervisednlearning [5]. Lu et al proposed to use a subject-independent BCI, previously learnt from the datanof many other subjects, also followed by online adaptation [6]. However, the main limitiation ofnthese two approaches is that such BCI would have initially poor detection performances, becomingnefficient only after adaptation. An ideal P300-based BCI would have initially high performances,neven if trained with very few examples. In this paper, we propose a new P300-based BCI designnwhich can be trained using much fewer examples than current BCI designs, without sacrifying thendetection performances.
11182 en Extracting Gait Spatiotemporal Properties from Parkinson's Disease Patients The Parkinson’s disease (PD) is a frequent chronic progressive syndrome in the elderly population.nCurrent available PD treatments either stimulate brain dopamine receptors or increase dopaminensynthesis. In the long-term, especially from the fifth year of disease on, onset of motor complicationsnis often observed. Such motor complications arise as a consequence of a reduction in the durationnof the effect of the medication. This motor complications affects spatiotemporal properties duringnthe gait of patients [8]. Length and speed of the steps change due to the effects known as dyskinesianand or akinesia, thus, and on-line measurement of these properties during gait may help to predictnthese situations and therefore warn the patient (e.g., minimizing risk of falling) or a remote healthncare system. Recently several approaches using inertial sensors have been developed with the aimnof measure these types of gait properties [1, 2, 4, 5, 6]. The gait characteristics may be obtainednusing gyroscopes tied at legs using a double inverted pendulum model [1, 5], nevertheless wearingnthese devices on the legs during daily life activity seems a drawback, leaving the application scopenof this method to clinical environments. In the case of accelerometers, they are usually positionednat the dorsal side of the trunk near the L3 region of the subject, since it is the CoM location. In thisnposition, 3D CoM acceleration, velocity and displacement can be estimated [2, 4]. However, to thenbest of our knowledge, there is no a user-friendly wearable device/location, that patients may usenoutside the hospital. Here we propose a method, based on SVM-regression, to extract spatiotemporalnproperties from accelerations obtained from a single accelerometer positioned at the lateral side ofnthe waist, with the advantage of being a wearable system the patients may use during their daily life,nwithout danger of hurt or damaging the device.
11183 en Machine Learning Applied to Multi-Modal Interaction, Adaptive Interfaces and Ubiquitous Assistive Technologies The presentation will describe the challenge of eInclusion in the technological designnprocess, which impedes the complete integration of people with disabilities and elderlynin Information Society. To face this challenge, the INREDIS project aims to face nindividual needs of users instead of addressing the needs of the average user, by nproposing basic technologies that enables the creation of personalized channels forncommunication and interaction with the technological environment. For this purpose, nMachine Learning can help constructing effective methods to reflect user needs, npreferences and expectations (and their evolution over time) on user interfaces,nconsequently improving satisfaction and performance. In particular, academia and nindustry within the INREDIS consortium explore together the potential of Machine nLearning on multimodal services and ubiquitous assistive technologies, as well as nadaptive user interfaces according to user and technological capabilities.
11185 en Human-Centered Machine Learning in a Social Interaction Assistant for Individuals with Visual Impairments Over the last couple of decades, the increasing focus on accessibility has resulted in the design andndevelopment of several assistive technologies to aid people with visual impairments in their dailynactivities. Most of these devices have been centered on enhancing the interaction of a user whonis blind or visually impaired with objects and environments, such as a computer monitor, personalndigital assistant, cellphone, road traffic, or a grocery store. Although these efforts are very essentialnfor the quality of life of these individuals, there is also a need (which has so far not been seriouslynconsidered) to enrich the interactions of individuals who are blind, with other individuals.nnNon-verbal cues (including prosody, elements of the physical environment, the appearance of communicatorsnand physical movements) account for as much as 65% of the information communicatednduring social interactions [1]. However, more than 1.1 million individuals in the US who are legallynblind (and 37 million worldwide) have a limited experience of this fundamental privilege of socialninteractions. These individuals continue to be faced with fundamental challenges in coping withneveryday interactions in their social lives. The work described in this paper is based on the designnand development of a Social Interaction Assistant that is intended to enrich the experience of socialninteractions for individuals who are blind, by providing real-time access to information aboutnindividuals and their surrounds. The realization of a Social Interaction Assistant device involvesnsolving several challenging problems in pattern analysis and machine intelligence such as personnrecognition/tracking, head/body pose estimation, gesture recognition, expression recognition, etc onna wearable real-time platform. A list of eight significant daily challenges faced by these individualsnwas identified in our initial focus group studies conducted with 27 individuals who are blind or visuallynimpaired. Each of these problems raises unique machine learning challenges that need tonbe addressed.
11186 en Toward Text-to-Picture Synthesis It is estimated that more that 2 million people in the United States have significant communicationnimpairments that result in them relying on methods other than natural speech alone for communicationn[2]. One type of commonly used augmentative and alternative communication (AAC) system isnpictorial communication software such as SymWriter [8], which uses a lookup table to transliterateneach word (or common phrase) in a sentence into an icon. This is an example of converting informationnbetween modalities. However, the resulting sequence of icons can be difficult to understand.nWe have been developing general-purpose Text-to-Picture (TTP) synthesis algorithms [10, 5] tonimprove understandability using machine learning techniques. Our goal is to help users with specialnneeds, such as the elderly or those with disabilities, to rapidly browse documents through pictorialnsummaries (e.g., Figure 5). Our TTP system targets general English. This differs from other pictorialnconversion systems that require hand-crafted narrative descriptions of a scene [1, 9], 3D models [3],nor special domains [6]. Instead, we use a concatenative or “collage” approach. In this talk, wendiscuss how machine learning enables the key components of our TTP system.
11187 en Fast and Flexible Selection with a Single Switch In single-switch communication, user input consists of repeated clicks, distinguished only by timingninformation; these clicks might be generated by pressing a button or blinking. For instance, thenrange of movement of individuals with severe motor impairments may be limited to a single muscle.nAlternatively, a crowded or jostled mobile technology user may be able to click precisely whilenother actions are difficult or sloppy. A single switch may also be useful when information conveyed,nsuch as a PIN, is sensitive and hand location on a normal keyboard might betray this content. Ournmethod, “Nomon” (e.g. Figure 1), expands the application scope of existing methods and facilitatesnfaster writing than the most common single-switch writing interface.nnExisting single-switch communication methods include scanning [1, 2] and One-Button Dasher [3,n4]. (Morse Code, in contrast, requires either click duration information or multiple switches.) Thesenmethods require options to be arranged in a particular configuration. By contrast, traditional operatingnsystems, web browsers, and free-form applications such as drawing place options at arbitrarynpoints on the screen. We seek a single-switch selection method that is not limited to certain forms ofnoption placement. We want our method to work for any number of options; to be able to effectivelynreorder the set of selections without imposing additional cognitive load; and to allow the user tonattend only to the desired target. Our method, Nomon, accomplishes these objectives. It can furthernautomatically adapt to individuals’ clicking abilities and incorporate prior beliefs about optionnselection frequency.nnTo test our method, we developed a writing application, the Nomon Keyboard (Figure 1), and comparednits performance with a popular commercial scanning interface, The Grid 2 [5] (Figure 2). Wenexamined study participants’ writing speeds, error rates, and number of clicks made per character asnwell as the subjective ratings of their experiences. We found that novice users wrote 35% faster withnthe Nomon interface than with the scanning interface. An experienced user (author TB, with > 10nhours practice) wrote at speeds of 9.3 words per minute with Nomon, using 1.2 clicks per characternand making no errors in the final text.
11188 en Data Mining Based User Modeling Systems for Web Personalization Applied to People with Disabilities This position paper tackles the problem of automatic web personalization using machine learningntechniques to model the users' behavior. The target population is people with physical, sensory orncognitive restrictions. In this paper we present our plans to study the possibility of creating usernmodels using the information extracted from web navigation logs by means of data mining methods.nWe discuss the expected advantages of adopting data mining to generate information about the user,nin comparison with traditional methods.
11189 en Perspective on the Goals and Complexities of Inclusive Design This presentation will discuss goals, methodology, and research for creating accessible user experiences in today's cutting-edge software technology. Using a framework for mapping user requirements to technology solutions, we will discuss the complex market for accessible technology and the opportunity for innovation in technology solutions. Demos will be given on new accessibility solutions in Windows and on the Internet. The presentation will highlight the complexities of the accessible technology space, with the hope of inspiring research and application of machine learning.
11191 en Granger Causality and Dynamic Structural Systems Using a generally applicable dynamic structural system of equations, we give natural definitions of direct and total structural causality applicable to both structural VARs and recursive structures representing time-series natural experiments. These concepts enable us to forge a previously missing link between Granger (G-) causality and structural causality by showing that, given a corresponding conditional form of exogeneity, G- causality holds if and only if a corresponding form of structural causality holds. Of importance for applications is the structural characterization of finite-order G-causality, which forms the basisfor most empirical work. We show that conditional exogeneity is necessary for valid structural inference and prove that in the absence of structural causality, conditional exogeneity is equivalent to G non-causality. We provide practical new G-causality and conditional exogeneity tests and describe their use in testing for structural causality.
11192 en Time Series Causality Inference Using the Phase Slope Index A method recently introduced by Nolte et. al (Phys Rev Lett 100:23401, 2008) estimates the causal direction of interactions robustly with respect to instantaneous mixtures of independent sources with arbitrary spectral content, i.e. in observations which are dominated by non-white spatially correlated noise and in which dynamic structural interaction plays little part. The method, named Phase Slope Index (PSI), is unlikely to assign causality in the case of lack of dynamic interaction among time series, unlike Granger causality for linear systems. Results show that PSI does not yield false positives even in the case of nonlinear interactions. The meaning of instaneous noise mixtures in different data domains will be discussed in the context of correct correlation vs. causation inference, and the theoretical relationship of PSI to other time-series causality inference methods will be expanded upon.n
11193 en Causality in Brain Connectivity Studies Using Functional Magnetic Resonance Imaging (fMRI) Data This talk will discuss the application of Granger causality to fMRI data in the form of Granger causality mapping (GCM), which is used to explore directed influences between neuronal populations (effective connectivity) in fMRI data. The method does not rely on a priori specification of a model that contains pre-selected regions and connections between them. This distinguishes it from other fMRI effective connectivity approaches that aim at testing or contrasting specific hypotheses about neuronal interactions. Instead, GCM relies on the Granger causality concept to define the existence and direction of influence from temporal information in the data. The problems of limited temporal resolution in fMRI, and the hemodynamic source of the signal that makes direct interpretation of fMRI Granger causality as neuronal influence difficult, will be discussed. n
11194 en Graphical Causal Models for Time Series Econometrics: Some Recent Developments and Applications Structural vector-autoregressive models are potentially very useful tools for guiding economic policy. I present a recently developed method to estimate and identify the causal structure underlying the data generating process. The method, which is based on graphical models, exploits conditional independence tests among estimated VAR residuals to infer the causal relationships among contemporaneous variables. I first show how this method works in the Gaussian linear setting. Then I present some developments for both the linear non-Gaussian and nonlinear settings.n
11195 en Open-Access Datasets for Time Series Causality Discovery Validation The Causality Workbench project provides an environment to test causal discovery algorithms. Via a web portal (http://clopinet.com/causality), we provide a number of resources, including a repository of datasets, models, and software packages, and a virtual laboratory allowing users to benchmark causal discovery algorithms by performing virtual experiments to study artificial causal systems. We regularly organize competitions. Our repository already includes several time dependent datasets from a variety of domains: system biology, neurosciences, physics, manufacturing, and marketing. We will invite new contributions and present our plan for upcoming evaluations of causal models for time series applications. nn
11196 en Understanding Gene Regulatory Networks and Their Variations A key biological question is to uncover the regulatory networks in a cellular system and to understand how this network varies across individuals, cell types, and environmental conditions. In this talk I will describe work that uses machine learning techniques to reconstruct gene regulatory networks from gene expression data. Specifically, we exploit novel forms of Bayesian regularized regression to enable transfer between multiple related learning problems, such as between different individuals or between different cell types. We demonstrate applications in two domains: understanding the effect of individual genetic variation on gene regulation and its effect on phenotypes including human disease; and understanding the regulatory mechanisms underling immune system cell differentiation.
11197 en Time Varying Graphical Models: Reverse Engineering and Analyzing Rewiring Networks A plausible representation of the relational information among entities in dynamic systems such as a social community or a living cell is a stochastic network that is topologically rewiring and semantically evolving over time. While there is a rich literature in modeling static or temporally invariant networks, until recently, little has been done toward modeling the dynamic processes underlying rewiring networks, and on recovering such networks when they are not observable. In this talk, I will present a new formalism for modeling network evolution over time based on time-evolving probabilistic graphical models, such as TV-GGM, TV-MRF, and TV-DBN, and several new algorithms for estimating the structure of such models underlying nonstationary time-series of nodal attributes. I will show some promising results on recovering the latent sequence of evolving social networks in the US Senate based it voting history, and the gene networks over more than 4000 genes during the life cycle of Drosophila melanogaster from microarray time course, at a time resolution only limited by sample frequency. I will also sketch some theoretical results on the asymptotic sparsistency of the proposed methods.
11198 en Novel Applications of Computational Biology in Infectious Disease Interventions Interventions in infectious diseases are increasingly relying on computational biology and genomic methods. Estimating changes in viral genetic diversity in a population could be a new potential method to evaluate vaccination strategies in populations. Transgenic mosquitoes immune to a pathogen are being developed to replace the native mosquito vector of a number of vector-borne diseases. High throughput methods are being used to elucidate mechanisms of immune memory with the promise of developing better vaccines. Large-scale computer simulation models are useful for exploring interventions and could benefit from input from network and graph theory. In this talk, we discuss a few novel applications of computational biology in understanding infectious diseases and interventions.
11205 en Direct Maximization of Protein Identifications from Tandem Mass Spectra 
11206 en Exploiting Physico-Chemical Properties in String-Kernels 
11207 en A Bayesian Method for 3D Reconstruction of Macromolecular Structure Using Class Averages from Single Particle Electron Microscopy 
11208 en vbFRET: A Bayesian Approach to Single-Molecule Forster Resonance Energy Transfer Analysis 
11209 en Leveraging Joint Test Status Distribution for an Optimal Significance Testing 
11210 en Statistical Methods for Ultra-Deep Pyrosequencing of Fast Evolving Viruses 
11211 en A Machine Learning Pipeline for Phenotype Prediction from Genotype Data 
11212 en Association Mapping of Traits over Time Using Gaussian Processes 
11213 en Learning Graphical Model Structure with Sparse Bayesian Factor Models and Process Priors 
11214 en Scalable Hierarchical Multitask Learning in Sequence Biology 
11268 en Geostatistics for Gaussian Processes Gaussian process methodology has inspired a number of stimulating new ideas in the area of machine learning. Kriging has been introduced as a statistical interpolation method for the design of computer experiments twenty years ago. However, some aspects of the geostatistical methodology originally developed for natural resource estimation have been ignored when switching to this new context. This talk reviews concepts of geostatistics and in particular the estimation of components of spatial variation in the context of multiple correlated outputs.
11269 en Gaussian Processes and Process Convolutions from a Bayesian Perspective 
11270 en Prior Knowledge and Sparse Methods for Convolved Multiple Outputs Gaussian Processes One approach to account for non-trivial correlations between outputs employs convolution processes. Under a latent function interpretation of the convolution transform it is possible to establish dependencies between output variables. Two important aspects in this framework are how can we introduce prior knowledge and how can we perform efficient inference. Relating the convolution operation with dynamical systems, we can specify richer covariance functions for multiple outputs. We also present different sparse approximations for dependent output Gaussian processes in the context of structured covariances. Joint work with Neil Lawrence, David Luengo and Michalis Titsias.
11271 en Multi-Task Learning and Matrix Regularization Multi-task learning extends the standard paradigm of supervised learning. In multi-task learning, samples for multiple related tasks are given and the goal is to learn a function for each task and also to generalize well (transfer learned knowledge) on new tasks. The applications of this paradigm are numerous and range from computer vision to collaborative filtering to bioinformatics while it also relates to vector valued problems, multiclass, multiview learning etc. I will present a framework for multi-task learning which is based on learning a common kernel for all tasks. I will also show how this formulation connects to the trace norm and group Lasso approaches. Moreover, the proposed optimization problem can be solved using an alternating minimization algorithm which is simple and efficient. It can also be "kernelized" by virtue of a multi-task representer theorem, which holds for a large family of matrix regularization problems and includes the classical representer theorem as a special case.
11272 en Learning Vector Fields with Spectral Filtering We present a class of regularized kernel methods for vector valued learning, which are based on filtering the spectrum of the kernel matrix. The considered methods include Tikhonov regularization as a special case, as well as interesting alternatives such as vector valued extensions of L2 boosting. While preserving the good statistical properties of Tikhonov regularization, some of the new algorithms allows for a much faster implementation since they require only matrix vector multiplications. We discuss the computational complexity of the different methods, taking into account the regularization parameter choice step. The results of our analysis are supported by numerical experiments.
11273 en Borrowing Strength, Learning Vector Valued Functions and Supervised Dimension Reduction We study the problem of supervised dimension reduction from the perspective of learning vector valuednfunctions and multi-task or hierarchical modeling in ai regularization framework. An algorithm is speciednand empirical results are provided. In the second part of the talk the same problem of supervised dimensionnreduction for a hierarchical model is revisted from a non-parametric Bayesian perspective.
11274 en Approximate Inference in Natural Language Processing I'll start out by presenting an idealized version of the natural language processing problem of parsing. I will brazenly suggest that most of NLP is reducible to variations on parsing problems. I'll show how dynamic programming solves the idealized version of the problem, both for calculating modes and marginals over parse trees, exploiting some key independence assumptions about the structure of natural language sentences. nnI will then discuss two approximate inference methods that let us build more powerful models of parsing. Neither comes with strong theoretical guarantees, but both are demonstrated to perform strongly in experiments on real NLP data. The first method builds on the dynamic programming representation, combining max-product and sum-product methods to produce, approximately, the k-best parses and a residual sum over the rest of the parses, useful when incorporating features that violate the usual independence assumptions. Experiments validate the approach with a discriminative model for machine translation. nnThe second method turns a parsing problem instance into a concise integer linear program. Approximate inference is then accomplished using well-known linear program relaxation. This is embedded in a new online learning algorithm that tries to penalize uninterpretable fractional solutions (and therefore inference cost at evaluation time). We show that this approach leads to state-of-the-art parsing performance on seven languages, with improved speed for both exact and approximate inference and no significant performance loss.
11276 en Joint Max Margin and Max Entropy Learning of Graphical Models Inferring structured predictions based on correlated covariates remains a central problem in many fields, including NLP, computer vision, and computational biology. Popular paradigms for training structured input/output models include the maximum (conditional) likelihood estimation, which leads to the well-known CRF; and the max-margin learning, which leads to the structured SVM (a.k.a. M3N), each enjoys some advantages, as well as weaknesses. In this talk, I present a new general framework called Maximum Entropy Discrimination Markov Networks (MEDN), which integrates the margin-based and likelihood-based approaches and combines and extends their merits. This new learning paradigm naturally facilitates integration of the generative and discriminative principles under a unified framework, and the basic strategies can be generalized to learn arbitrary graphical models, such as the generative Bayesian networks or models with structured hidden variables. I will discuss a number of theoretical properties of this model, and show applications of MEDN to learning fully supervised structured i/o model, max-margin structured i/o models with hidden variables, and a max-margin LDA model for jointly discovering discriminative latent topic representations and predicting document label/score of text documents, with compelling performance in each case.
11277 en Large-Scale Learning and Inference: What We Have Learned with Markov Logic Networks Markov logic allows very large and rich graphical models to be compactly specified. Current learning and inference algorithms for Markov logic can routinely handle models with millions of variables, billions of features, thousands of latent variables, and strong dependencies. In this talk I will give an overview of the main ideas in these algorithms, including weighted satisfiability, MCMC with deterministic dependencies, lazy inference, lifted inference, relational cutting planes, scaled conjugate gradient, relational clustering and relational pathfinding. I will also discuss the lessons learned in developing successive generations of these algorithms and promising ideas for the next round of scaling up.
11278 en Parameter Learning Using Approximate MAP Inference In recent years, machine learning has seen the development of a series of algorithms for parameter learning that avoid estimating the partition function and instead, rely on accurate approximate MAP inference. Within this framework, we consider two new topics. nnIn the first part, we discuss parameter learning in a semi-supervised scenario. Specifically, we focus on a region-based scene segmentation model that explains an image in terms of its underlying regions (a set of connected pixels that provide discriminative features) and their semantic labels (such as sky, grass or foreground). While it is easy to obtain (partial) ground-truth labeling for the pixels of a training image, it is not possible for a human annotator to provide us with the best set of regions (those that result in the most discriminative features). To address this issue, we develop a novel iterative MAP inference algorithm which selects the best subset of regions from a large dictionary using convex relaxations. We use our algorithm to "complete" the ground-truth labeling (i.e. infer the regions) which allows us to employ the highly successful max-margin training regime. We compare our approach with the state of the art methods and demonstrate significant improvements. nnIn the second part, we discuss a new learning framework for general log-linear models based on contrastive objectives. A contrastive objective considers a set of "interesting" assignments and attempts to push up the probability of the correct instantiation at the expense of the other interesting assignments. In contrast to our approach, related methods such as pseudo-likelihood and contrastive divergence compare the correct instantiation only to nearby instantiations, which can be problematic when there is a high-scoring instantiation far away from the correct one. We present some of the theoretical properties and practical advantages of our method, including the ability to learn a log-linear model using only (approximate) MAP inference. We the theoretical properties and practical advantages of our method, including the ability to learn a log-linear model using only (approximate) MAP inference. We also show results of applying our method to some simple synthetic examples, where it significantly outperforms pseudo-likelihood.
11279 en Training Structured Predictors for Novel Loss Functions As a motivation we consider the PASCAL image segmentation challenge. Given an image and a target class, such as person, the challenge is to segment the image into regions occupied by objects in that class (person foreground) and regions not occupied by that class (non-person background). At the present state of the art the lowest pixel error rate is achieved by predicting all background. However, the challenge is evaluated with an intersection over union score with the property that the all-background prediction scores zero. This raises the question of how one incorporates a particular loss function into the training of a structured predictor. A standard approach is to incorporate the desired loss into the structured hinge loss and observe that, for any loss, the structured hinge loss is an upper bound on the desired loss. However, this upper bound is quite loose and it is far from clear that the structured hinge loss is an appropriate or useful way to handle the PASCAL evaluation measure. nnThis talk reviews various approaches to this problem and presents a new training algorithm we call the good-label-bad-label algorithm. We prove that in the data-rich regime the good-label-bad-label algorithm follows the gradient of the training loss assuming only that we can perform inference in the given graphical model. The algorithm is structurally similar to, but significantly different from, stochastic subgradient descent on the structured hinge loss (which does not follow the loss gradient).
11280 en Some Machine Learning Problems that We in the Computer Vision Community Would Like to See Solved From a user's perspective, I'll describe what solutions I'd like to see regarding the learning of large scale graphical models. Also, at a recent vision workshop, I asked (one by one) numerous leading researchers in computer vision what results they would like to see from computer scientists/machine learning folks. I'll present those responses.
11281 en Covariate Shift by Kernel Mean Matching Given sets of observations of training and test data, we consider the problem of re-weighting the training data such that its distribution more closely matches that of the test data. We achieve this goal by matching covariate distributions between training and test sets in a high dimensional feature space (specifically, a reproducing kernel Hilbert space). This approach does not require distribution estimation. Instead, the sample weights are obtained by a simple quadratic programming procedure.nWe first describe how distributions may be mapped to reproducing kernel Hilbert spaces. Next, we review distances between such mappings, and describe conditions under which the feature space mappings are injective (and thus, distributions have a unique mapping). Finally, We demonstrate how a transfer learning algorithm can be obtained by reweighting the training points such that their feature mean matches that of the (unlabeled) test distribution. Our correction procedure yields its greatest and most consistent advantages when the learning algorithm returns a classifier/regressor that is "simpler" than the data might suggest. On the other hand, even an ideal sample reweighting may not be of practical benefit given a sufficiently powerful classifier (if available).
11284 en Kernel Learning and Meta Kernels for Transfer Learning 
11286 en No-Free-Lunch Theorems for Transfer Learning I will present a formal framework for transfer learning and investigate under which conditions is it possible to provide performance guarantees for such scenarios. I will address two key issues:n*1) Which notions of task-similarity suffice to provide meaningful error bounds on a target task, for a predictor trained on a (different) source task?n*2) Can we do better than just train a hypothesis on the source task and analyze its performance on the target task? Can the use of unlabeled target samples reduce the target prediction error?
11319 en Learning a Region-based Scene Segmentation Model In recent years, machine learning has seen the development of a series of algorithms for parameter learning that avoid estimating the partition function and instead, rely on accurate approximate MAP inference. Within this framework, we consider two new topics. nnIn the first part, we discuss parameter learning in a semi-supervised scenario. Specifically, we focus on a region-based scene segmentation model that explains an image in terms of its underlying regions (a set of connected pixels that provide discriminative features) and their semantic labels (such as sky, grass or foreground). While it is easy to obtain (partial) ground-truth labeling for the pixels of a training image, it is not possible for a human annotator to provide us with the best set of regions (those that result in the most discriminative features). To address this issue, we develop a novel iterative MAP inference algorithm which selects the best subset of regions from a large dictionary using convex relaxations. We use our algorithm to "complete" the ground-truth labeling (i.e. infer the regions) which allows us to employ the highly successful max-margin training regime. We compare our approach with the state of the art methods and demonstrate significant improvements. nnIn the second part, we discuss a new learning framework for general log-linear models based on contrastive objectives. A contrastive objective considers a set of "interesting" assignments and attempts to push up the probability of the correct instantiation at the expense of the other interesting assignments. In contrast to our approach, related methods such as pseudo-likelihood and contrastive divergence compare the correct instantiation only to nearby instantiations, which can be problematic when there is a high-scoring instantiation far away from the correct one. We present some of the theoretical properties and practical advantages of our method, including the ability to learn a log-linear model using only (approximate) MAP inference. We the theoretical properties and practical advantages of our method, including the ability to learn a log-linear model using only (approximate) MAP inference. We also show results of applying our method to some simple synthetic examples, where it significantly outperforms pseudo-likelihood.
11337 en SINDBAD and SiQL: An Inductive Database and Query Language in the Relational Model The speaker presented the concepts and implementation of SINDBAD, a prototype of an inductive database - as proposed by Imielinski and Mannila - in the relational model. The goal is to support all steps of the knowledge discovery process on the basis of queries to a database system. The query language SiQL (structured inductive query language), an SQL extension, offers query primitives for feature selection, discretization, pattern mining, clustering, instance-based learning and rule induction.nn
11351 en Retrospective Change-point Approaches and Sequential Modelling We propose for the analysis of array-CGH data, a new stochastic segmentation model and an associatednestimation procedure that has attractive statistical and computational properties. An important benefit ofnthis Bayesian segmentation model is that it yields explicit formulas for posterior means, which can be usednto estimate the signal directly without performing segmentation. Other quantities relating to the posteriorndistribution that are useful for providing confidence assessments of any given segmentation can also benestimated by using our method. We propose an approximation method whose computation time is linear innsequence length which makes our method practically applicable to the new higher density arrays. Simulationnstudies and applications to real array-CGH data illustrate the advantages of the proposed approach.
11352 en A Dynamic HMM for Online Segmentation We propose a novel method for the analysis of sequential datanthat exhibits an inherent mode switching. In particular, the datanmight be a non-stationary time series from a dynamical systemnthat switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and withoutnany training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of thenViterbi algorithm that performs an unsupervised segmentation andnclassification of the data on-the-y, i.e. the method is able to process incoming data in real-time. The main idea of the approach isnto track and segment changes of the probability density of the datanin a sliding window on the incoming data stream. The usefulnessnof the algorithm is demonstrated by an application to a switchingndynamical system.
11353 en Distributed Detection and Localization of Network Anomalies using Rank Tests We propose an efficient and decentralized method for detecting change-points in high-dimensional data. This issue is of growing concern to the network security community since, in this context, network anomalies such as denial of service (DoS) attacks are likely to lead to statistical changes in Internet traffic. Our method proposes a way of distributing a centralized approach called TopRank, which consists of a data reduction stage based on record filtering, followed by a nonparametric change-point detection test based on U-statistics. The key point is to aggregate censored time series built locally and to perform a nonparametric test for doubly censored time series resulting from this aggregation. With this new approach, called distributed TopRank in the following, we can address massive data streams and perform network anomaly detection and localization on the fly while limiting the quantity of data exchanged within the network.
11354 en Product Partition Models for Modelling Changing Dependency Structure in Time Series We show how to apply the efficient Bayesian changepoint detection techniques of Fearnhead in thenmultivariate setting. We model the joint density of vector-valued observations using undirected Gaussianngraphical models, whose structure we estimate. We show how we can exactly compute the MAPnsegmentation, as well as how to draw perfect samples from the posterior over segmentations, simultaneouslynaccounting for uncertainty about the number and location of changepoints, as well as uncertainty about thencovariance structure. We illustrate the technique by applying it to financial data and to bee tracking data.
11356 en Hierarchical-Dirichlet-Process-based Hidden Markov Models We consider the problem of speaker diarization, the problem of segmenting an audio recording of a meetingninto temporal segments corresponding to individual speakers. The problem is rendered particularly dicultnby the fact that we are not allowed to assume knowledge of the number of people participating in the meeting.nTo address this problem, we take a Bayesian nonparametric approach to speaker diarization that builds onnthe hierarchical Dirichlet process hidden Markov model (HDP-HMM) of Teh et al. (2006). Although thenbasic HDP-HMM tends to over-segment the audio data{creating redundant states and rapidly switchingnamong them{we describe an augmented HDP-HMM that provides effective control over the switching rate.nWe also show that this augmentation makes it possible to treat emission distributions nonparametrically.nTo scale the resulting architecture to realistic diarization problems, we develop a sampling algorithm thatnemploys a truncated approximation of the Dirichlet process to jointly resample the full state sequence,ngreatly improving mixing rates. Working with a benchmark NIST data set, we show that our Bayesiannnonparametric architecture yields state-of-the-art speaker diarization results.
11357 en Quickest Change Detection This work examines the problem of sequential change detection in the constant drift of a Brownian motion in the case of multiple alternatives. As a performance measure an extended Lordenas criterion is proposed.nWhen the possible drifts, assumed after the change, have the same sign, the CUSUM rule, designed to detectnthe smallest in absolute value drift, is proven to be the optimum. If the drifts have opposite signs, thenna specific 2-CUSUM rule is shown to be asymptotically optimal as the frequency of false alarms tends toninfinity.
11359 en Adaptive Sequential Bayesian Change-point Detection Nonstationarity, or changes in the generative parameters, are often a key aspect of real world timenseries, which comprise of many distinct parameter regimes. An inability to react to regime changesncan have a detrimental impact on predictive performance. Change point detection (CPD) attemptsnto reduce this impact by recognizing regime change events and adapting the predictive model appropriately.nAs a result, it can be a useful tool in a diverse set of application domains includingnrobotics, process control, and finance. CPD is especially relevant to finance where risk resultingnfrom parameter changes is often neglected in models. For example, Gaussian copula models used innpricing collateralized debt obligations (CDOs) had two key flaws: assuming that subprime mortgagendefaults have a fixed correlation structure, and using a point estimate of these correlation parametersnlearned from historical data prior to the burst of the real-estate bubble [1, 2]. Bayesian change pointnanalysis avoids both of these problems by assuming a change point model of the parameters andnintegrating out the uncertainty in the parameters rather than using a point estimate.
11360 en Temporal Segmentation with Kernel Change-point Detection We introduce a kernel-based method for change-point analysis within a sequencenof temporal observations. Change-point analysis of an unlabeled sample of observationsnconsists in, first, testing whether a change in the distribution occurs withinnthe sample, and second, if a change occurs, estimating the change-point instantnafter which the distribution of the observations switches from one distribution tonanother different distribution. We propose a test statistic based upon themaximumnkernel Fisher discriminant ratio as a measure of homogeneity between segments.nWe derive its limiting distribution under the null hypothesis (no change occurs),nand establish the consistency under the alternative hypothesis (a change occurs).nThis allows to build a statistical hypothesis testing procedure for testing the presencenof a change-point, with a prescribed false-alarm probability and detectionnprobability tending to one in the large-sample setting. If a change actually occurs,nthe test statistic also yields an estimator of the change-point location. Promisingnexperimental results in temporal segmentation of mental tasks from BCI data andnpop song indexation are presented.
11408 en Presentation 1: Global port cities 
11409 en Presentation 2: Decorative brickwork: a global history 
11410 en Presentation 3: Before the scramble for Africa: tracing African architecture through trade 
11411 en Presentation 4: 20th century African urbanism: three vignettes 
11412 en Presentation 5: A global history of pilgrimage: an introduction 
11413 en Presentation 6: The temple in the cave: rock-cut architecture 
11414 en Lecture 1: The importance of chemical principles 
11415 en Lecture 2: Discovery of electron and nucleus, need for quantum mechanics 
11417 en Lecture 4: Wave-particle duality of matter, SchrÃ¶dinger equation 
11418 en Lecture 5: Hydrogen atom energy levels 
11419 en Lecture 6: Hydrogen atom wavefunctions (orbitals) 
11421 en Lecture 8: Multielectron atoms and electron configurations 
11423 en Lecture 10: Periodic trends continued; Covalent bonds 
11425 en Lecture 12: Exceptions to Lewis structure rules; Ionic bonds 
11426 en Lecture 13: Polar covalent bonds; VSEPR theory 
11427 en Lecture 14: Molecular orbital theory 
11428 en Lecture 15: Valence bond theory and hybridization 
11429 en Lecture 16: Determining hybridization in complex molecules; Thermochemistry and bond energies/bond enthalpies 
11430 en Lecture 17: Entropy and disorder 
11431 en Lecture 18: Free energy and control of spontaneity 
11433 en Lecture 20: Le Chatelier's principle and applications to blood-oxygen levels 
11434 en Lecture 21: Acid-base equilibrium: Is MIT water safe to drink? 
11435 en Lecture 22: Chemical and biological buffers 
11436 en Lecture 23: Acid-base titrations 
11437 en Lecture 24: Balancing oxidation/reduction equations 
11439 en Lecture 26: Chemical and biological oxidation/reduction reactions 
11440 en Lecture 27: Transition metals and the treatment of lead poisoning 
11441 en Lecture 28: Crystal field theory 
11442 en Lecture 29: Metals in biology 
11443 en Lecture 30: Magnetism and spectrochemical theory 
11445 en Lecture 32: Nuclear chemistry and elementary reactions 
11447 en Lecture 34: Temperature and kinetics 
11732 en The Black Hole at the Center of Our Galaxy The Nobel Prize-winning physicist talks about various aspects of our galaxy, and discusses new methods in astronomy and astrophysics that make possible explorations deep into the heart of the Milky Way.nn;**Link to** - **[[http://mitworld.mit.edu/video/603|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/42|Ford/MIT Nobel Laureate Lecture Series]]**
11734 en Bose-Einstein Condensates: The Coldest Matter in the Universe What happens when a gas is cooled to absolute zero? A new door to the quantum world opens up because all the atoms start "marching in lockstep", forming one giant matter wave Z the Bose-Einstein condensate. This was predicted by Einstein in 1925, but only realized in 1995 in laboratories at JILA in Boulder and at MIT. Since then, many properties of this mysterious form of matter have been revealed, including matter wave amplification and quantized vortices. Bose condensates have been used to realize a basic atom laser, an intense source of coherent matter waves.nn;**Link to** - **[[http://mitworld.mit.edu/video/77|Lecture´s Homepage]]**n;**Series** - **[[http://mitworld.mit.edu/host/view/64|Physics Department]]**n;**Host** - **[[http://mitworld.mit.edu/series/view/26|2001 Physics Colloquium]]**
11735 en The Philosophy of Conflict Resolution ;**Link to** - **[[http://mitworld.mit.edu/video/62|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/42|Ford/MIT Nobel Laureate Lecture Series]]**n;**Event Co-Sponsors**n;**[[http://mitworld.mit.edu/host/view/43|Graduate Student Council]]**n;**[[http://mitworld.mit.edu/host/view/44|Undergraduate Association]]**n;**[[http://mitworld.mit.edu/host/view/48|Community Services Office at MIT]]**n;**[[http://mitworld.mit.edu/host/view/46|Office of the Chancellor]]**
11738 en 2002 Nobel Prize in Physiology or Medicine Nobel Lecture In October 2002, The Nobel Prize in Physiology or Medicine was awarded to H. Robert Horvitz, Sydney Brenner and John El Sulston "for their discoveries concerning genetic regulation of organ development and programmed cell death."nn;**Link to** - **[[http://mitworld.mit.edu/video/71|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/58|McGovern Institute for Brain Research at MIT]]**n;**Event Co-Sponsor** - **[[http://mitworld.mit.edu/host/view/59|MIT School of Science]]**
11801 en New Frontiers with Ultracold Gases Better bring a sweater when you visit Wolfgang Ketterle’s laboratories. This master of cool has managed to reduce temperatures in his vacuum chambers below those found in interstellar space. “The colder we are, the more we have the potential to make new discoveries,” says Ketterle. He won the 2001 Nobel Prize in Physics for generating Bose-Einstein condensates -- atoms that clump together briefly in a gas at frigid temperatures. Now, Ketterle is manipulating these transient events in novel ways. Using a magnetic field, he can generate Bose-Einstein condensates and transport them, potentially to microchips. He envisions supersensitive chips that will help measure rotation and gravity to aid navigation and geological exploration in the decades ahead. And in his latest triumph, Ketterle actually formed ultracold molecules after bringing two atoms together in the chilliest manmade conditions ever generated. He flouted chemists’ predictions and achieved no heat release. “We’re holding atoms in a laser beam, turn down the laser power and those atoms turn into molecules…. If nature had knocked on my door and said you have one free wish, wish for something you want in science, I wouldn’t have been bold enough to ask for that!”nn;**Link to** - **[[http://mitworld.mit.edu/video/192|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/64|Physics Department]]**
11802 en Progress in the Study of the X-Ray Background Riccardo Giacconi has probably seen deeper into the universe than any other human being. He has conducted his explorations not with the naked eye, but with a series of increasingly sensitive detectors, relentlessly searching for the source of cosmic x-ray radiation. In this first-person account of pursuing one question for 40 years, what emerges most clearly is the kind of focus, determination, and invention required to make discoveries in the Nobel Prize league. Giacconi confesses that “X-ray astronomy is not easy” – an admirable understatement – but he succeeds in proving three key points: from the Uhuru satellite to the Hubble and Chandra telescopes, the success of experiments depends as much on brilliant instrument design as on data analysis; individual, identifiable galaxies are the source of the universe’s x-ray radiation background; and so we are now “looking at objects whose nature we do not know” – objects that the next generation of astronomers will understand only if they have the resources to build new instruments.nn;**Link to** - **[[http://mitworld.mit.edu/video/197|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/57|Massachusetts Space Grant Consortium]]**
11803 en The Origin of Mass and the Feebleness of Gravity A stunning roster of awards all identify Frank Wilczek as one of the most profound and influential theoretical physicists alive today. This lecture proves the point, as Wilczek goes after one of the deepest questions in science: What is the origin of mass? Rewriting Einstein’s famous equation as m=E / c2 dramatizes that energy is the source of mass; energetic but massless quarks and gluons, Wilczek argues, give rise to mass by finding quasi-stable equilibrium states, better know as protons and neutrons.nnHaving reinterpreted the theory of quantum chromodynamics in a brisk half hour, Wilczek plunges into another brain-straining question: What makes gravity so feeble? Here the more tentative answer derives from the unimaginably tiny dimensions of the Planck scale. Fundamental forces make sense in that realm; gravity is weak only relative to the enormously larger scales we live on. Wilczek looks forward to testing some of these speculations via experimental results as early as 2009.nn;**Link to** - **[[http://mitworld.mit.edu/video/204|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/64|Physics Department]]**
11804 en 2004 Nobel Colloquium There’s no magic formula for winning the Nobel Prize. But you can’t find a more classic model than the career of Frank Wilczek, Feshbach Professor of Physics and 2004 Nobel laureate.nnWilczek was a 21 year-old graduate student at Princeton when he made his breakthrough discovery. High energy physics was baffled by the “strong force,” which binds the quarks that make up protons and neutrons. Wilczek (with two colleagues who share the prize) was brave enough to entertain a really startling idea: the strong force works in just the opposite way from the more familiar forces in nature – the closer together the particles are, the weaker the force becomes, an idea Wilczek captured in the phrase "asymptotic freedom.” This profound insight into the fundamental forces of nature has astonishing explanatory power, not only for physics but also for cosmology. We now understand the early universe – the first few minutes of existence – better than we understand the universe around us today.nnFar from resting on his Nobel laurels, Wilczek is still working at the most puzzling frontiers of his science – for example, struggling to explain The Origin of Mass and the Feebleness of Gravity, the subject of his previous Physics Colloquium.nn;**Link to** - **[[http://mitworld.mit.edu/video/237|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/64|Physics Department]]**
11806 en Ending Global Poverty Imagine a bank that loans money based on a borrower’s desperate circumstances -- where, as Muhammad Yunus says, “the less you have, the higher priority you have.” Turning banking convention on its head has accomplished a world of good for millions of impoverished Bangladeshis, as the pioneering economist Yunus has demonstrated in the last three decades. What began as a modest academic experiment has become a personal crusade to end poverty. Yunus reminds us that for two-thirds of the world’s population, “financial institutions do not exist.” Yet, “we’ve created a world which goes around with money. If you don’t have the first dollar, you can’t catch the next dollar.” It was Yunus’ notion, in the face of harsh skepticism, to give the poorest of the poor their first dollar so they could become self-supporting. “We’re not talking about people who don’t know what to do with their lives….They’re as good, enterprising, as smart as anybody else.” His Grameen Bank spread from village to village as a lender of tiny amounts of money (microcredit), primarily to women. Yunus heard that “all women can do is raise chickens, or cows or make baskets. I said, ‘Don’t underestimate the talent of human beings.’ ” No collateral is required, nor paperwork—just an effort to make good and pay back the loan. Now the bank boasts 5 million borrowers, receiving half a billion dollars a year. It has branched out into student loans, health care coverage, and into other countries. Grameen has even created a mobile phone company to bring cell phones to Bangladeshi villages. Yunus envisions microcredit building a society where even poor people can open “the gift they have inside of them.”nn;**Link to** - **[[http://mitworld.mit.edu/video/289|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/124|Poverty Action Lab]]**n;**Event Co-Sponsors** n;**[[http://mitworld.mit.edu/host/view/26|Economics Department]]**n;**[[http://mitworld.mit.edu/host/view/67|MIT School of Humanities, Arts, and Social Sciences]]**
11807 en Leadership in a Complex, Technology-Driven World Sometimes the best way to achieve leadership is by pursuing a vision or meeting some personal goals, these three top-flight technologists suggest.nnRobert Langer admits, “I don’t tend to think of myself as a leader. I have simple ideas; I just want to see if I can do some good, and get satisfaction out of that.” He counts himself lucky to have gotten a job at Harvard Medical School, which allowed him to apply engineering to medical problems. “I wanted to see if we could make things that might help improve people’s health.” He attributes some of his leadership learning to years of struggle in acquiring grant money—in one case a 17-year battle with the NIH to back a novel drug delivery system (for which Langer was awarded the Charles Stark Draper Prize in 2002).nnSays Robert Metcalfe, “We have an idealization of innovative leadership—that it’s lovely. But the enemy is the status quo, and it’s resourceful and determined to defeat innovation.” Metcalfe’s personal style figures in his successes. He went to war against IBM in the 1980s, “when I had an invention that was better than what they had, and they threw all their monopoly resources against me. I was alone and surrounded and I beat them.” To make progress against the status quo, Metcalfe states, “you have to be obnoxious.”nnDon’t forget schmoozing and team playing, reminds Nobel Laureate Phillip Sharp, who acquired much of his savvy moving through academic ranks at MIT and partnering with outside firms. “I like to set a goal – that I’d like to see this technology do that, or this scientific question answered.” While you must set goals and seize opportunities, he says, you also need to attract optimal talents to your environment and “get others to play the game.” These are skills, Sharp says, he learned in high school sports.nn;**Link to** - **[[http://mitworld.mit.edu/video/304|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/131|MIT Leadership Center]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/83|The Passion to Action Summit: The MIT Leadership Center Launch]]**
11818 en How Would Climate Change Influence Society in the 21st Century? (Panel) Rajendra K. Pachauri leads fellow members of the Nobel Prize-winning IPCC in a remarkable public session of soul-searching. Now that the IPCC has helped make climate change a signal issue of our times, what next?nnJohn Reilly wonders whether the IPCC should be celebrating any success, given that greenhouse gas emissions continue to rise in spite of all the comprehensive study. Given the “dismal outcome so far,” it’s important that the IPCC “avoid the complacency that comes with big awards,” and that “much, all of the work is still there to be done.”nn“It’s probably time for sunset, Michael Golay suggests.” Now that the IPCC has succeeded in establishing climate change as “a reality among at least the chattering classes,” the next step is actually a social question, one that is much more difficult than coming up with new technologies. “We’re really talking about interfering with markets, and doing this in a way that doesn’t become simply another vehicle for creating profits for special interests….”nnWilliam Moomaw believes IPCC reports have made possible policy and corporate innovations that would have been unthinkable only a decade ago, and the IPCC should continue to serve in an advisory capacity to the world, laying out the technological and economic possibilities. Says Moomaw, “We got off to a bad start. We talked about global warming as being an environmental issue when in fact global warming is a symptom of maldevelopment."nnThe IPCC “should continue as the voice of science and help a well-informed society make tough decisions,” declares Andreas Fischlin . This will mean “facing the issue of sustainability in the context of climate change to an extent many of us won’t like.” Research challenges in developing nations may impede efforts to “optimize the IPCC’s work and help in the whole issue of moving toward a more sustainable world.”nnAkimasa Sumi believes IPCC should continue to have a powerful role in the future, because the “climate change issue is driven by science.” He proposes refining climate models in the hope of reducing uncertainty around such matters as the role of aerosols and clouds. He says the focus must now be on adaptation and mitigation, particularly over a 30-year time scale.nnThe IPCC established its relevance because it drew a line between being policy relevant and policy prescriptive, says Adil Najam. Now, “we need to claim victory on understanding the mechanics of the science and stop debating.” The next step must mean “focusing not on the scope of the problem, but on potential for solutions.”nnShould the IPCC attempt to become more prescriptive, believes Howard Herzog, “it would lose respect.” In his years with the organization, “anytime we got into policy prescriptive areas, when we got close to the line, tensions rose, arguments intensified, we lost consensus.” He thinks it’s important to continue the IPCC’s work, because the science will change, and we need a “broker out there to summarize where science is on critical issues.”nn;**Link to** - **[[http://mitworld.mit.edu/video/551|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/158|MIT Energy Initiative]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/123|Alliance for Global Sustainability Conference]]**
11824 en The Energy Problem and the Interplay Between Basic and Applied Research The situation facing our planet could hardly be more dire: There’s increasingly dangerous competition among nations for ever scarce energy resources, and climate change is racing ahead of predictions. Although Steven Chu believes “We are getting close to where it’s very nervous time,” he also sees “reason for hope.”nnJust as science in the 1970s produced a “green revolution” in agricultural productivity, preventing mass starvation in a swelling global population, Chu is counting on transformative scientific and engineering ideas to achieve sustainable energy and cap climate change.nnAs chief architect of new policy, and with tens of billions of dollars to pump into his vision, Chu is targeting key areas. Number one on his list: energy efficiency and conservation. Since buildings use 40% of the nation’s total energy, designing more efficient homes and offices will make a big difference. There are “tune ups” possible for existing buildings, and software that can direct lighting, heating and cooling where it’s needed that can achieve 50% plus energy savings, and won’t break the bank. Says Chu, “This is truly low-hanging fruit, but we have to build the tools that allow architects and structural engineers to get on with it.”nnOn the supply side, Chu has his heart set on transformative technologies such as nanotech breakthroughs in solar power. He’s looking for ways to scale up biomass fuel production, now that synthetic biology can make microbes manufacture gas-like fuels. Noting in particular the work of MIT’s Dan Nocera, Chu says he “wants to use nature as an inspiration, but go beyond nature,” performing artificial photosynthesis to create new hydrocarbons. And as the U.S. and China continue dependence on coal, figuring out how to capture and sequester carbon from these plants figures “high on the list of things we must do.” He’s again hoping researchers will find some analog to nature’s ability to grab and neutralize CO2.nnThe ideal environment for jumpstarting such urgent scientific efforts, believes Chu, is something like Bell Labs, where Chu himself worked. The Labs performed “mission-driven research” around communications and for U.S. war efforts, but along the way also developed the transistor, information theory, radio astronomy, and lasers, among many examples. These scientist-led labs emphasized exchange of ideas and rapid infusion of research funds to the most promising work. This led to inventions that in turn transformed the U.S. economy. Chu envisions energy lab equivalents that “deliver the goods” along with fundamental science, “so you can have the Nobel Prize and save the world at the same time.”nn;**Link to** - **[[http://mitworld.mit.edu/video/683|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/36|The Office of the President of MIT]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/108|Karl Taylor Compton Lecture]]**
11825 en Challenges in Nation Building At times humorous and other times defiant, José Ramos-Horta describes nurturing the 21st century’s first sovereign state through its formative years. The journey of East Timor from brutal Indonesian rule to fragile self-governance has involved Ramos-Horta in conflict and debate from the halls of the U.N. to the smallest villages of this tiny Southeast Asian island.nnHe describes the scene in 2002, after two years of UN-supervised transition, when Indonesia handed off a nation it had governed by force for decades: “A human calamity -- close to 200 thousand people lost their lives.” Another 200 thousand were forcibly displaced into West Timor. As it departed “in anger and frustration,” Indonesia’s military orchestrated the destruction of the nation’s cities, roads, schools and clinics. “The economy was at a standstill,” says Ramos-Horta. “We received barely a sketch of a state, a skeleton.”nnThe challenge of rebuilding East Timor is all the more daunting given “the psychological-emotional trauma of 24 years of violence.” There are bitter disputes involving how to conduct a national process of reconciliation. Western ambassadors recently called on Ramos-Horta, “representatives of two countries most notorious…for providing weapons and the red carpet treatment to the dictatorship of Indonesia.” They advocated establishing an international tribunal to pursue crimes against humanity during Indonesian rule. Says Ramos-Horta, “Had I been in a bad mood, I would have said, ‘Excuse me, the two of you are lecturing me on human rights and justice?’”nnDespite warnings from the U.N. that “lack of justice encourages impunity,” he believes East Timor must travel its own path toward reconciliation. If East Timor set up such a tribunal, “Who would it start with -- Indonesia or the U.S., which provided weapons to Suharto, or Australia, or all of them at once?” He states, “If you pursue justice at any cost without being sensitive to the challenges and complexities on the ground, you undermine the incipient nation, democracy and justice.”nnToday, when Ramos-Horta travels in the countryside, people don’t want to discuss security and unity. Recounts Ramos-Horta, “They joke with me: ‘Mr. President, we really like your road to peace, but we prefer a road to our village.’” He’s now focused on providing his people with such essentials as clean water and electricity, and shoring up the nation’s fragile social and economic institutions. “Let’s put all the past behind us. Look after the victims, the wounded, in their minds, bodies and souls, build a country that is deserving of so much sacrifice. Chasing the ghosts of the past leads us nowhere,” says Ramos-Horta.nn;**Link to [[http://mitworld.mit.edu/video/714|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/175|Legatum Center for Development and Entrepreneurship]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/151|The Legatum Pericles Lecture Series]]**
11826 en America's Leadership in Clean Energy In welcoming President Obama, MIT PresidentnSusan Hockfield summarizes the vast array of energy innovation at MIT, including the MIT Energy Initiative and the student-led 1700 member Energy Club, and declares, "We share President Obama's view that clean energy is the defining challenge of this era."nnIn his introduction of President Obama, Professor Ernest Moniz, Director of the MIT Energy Initiative (MITEI) and member of the President's Council of Advisors on Science and Technology (PCAST), discusses global issues on clean energy, science and innovation, and credits Obama for expanding the nation's energy vision.nnBarack Obama came to MIT not just to praise the Institute's leading edge energy research but to encourage all of America’s “heirs to a legacy of innovation” in their pursuit of discovery. The nation owes much of its prosperity to risk-takers and entrepreneurs, Obama said, and now, given the linked challenges of energy and climate change, we need such pioneers more than ever.nnAfter visiting MIT labs working on more efficient solar cells and lighting, batteries “that aren’t built, but grown,” and offshore wind plants that function even when the air is still, Obama told a large crowd that as the nation inevitably transitions from fossil fuels to renewable energy, we’re counting on the kind of “innovative potential on display at MIT.”nnObama acknowledges the great challenges facing energy researchers and entrepreneurs. As traditional energy supplies become more precious, and energy demands grow, nations are competing to develop new ways to produce and use energy, said Obama, and the winner will lead the global economy. “I want America to be that nation. It’s that simple.”nnHis administration’s response has been to make massive investments in both clean energy and basic science. Obama aims these efforts at both the current recession, and the nation’s future economic health. Clean energy jobs today and research “to produce the technologies of tomorrow” will “lay a new foundation for lasting prosperity.” He hopes this comprehensive approach will culminate in legislation that will transform America’s entire energy system.nnBut Obama is under no delusion that all will embrace his plan. “The closer we get,” says Obama, the “more we’ll hear from those whose interest or ideology run counter to that much-needed action we’re engaged in.” What worries the president more, though, is a dangerous pessimism shared by many, “that our politics are too broken and our people too unwilling to make hard choices for us to actually deal with this energy issue.” Implicit in this argument, he says, is that America has lost its fighting spirit.nnObama rejects this argument “because of what I’ve seen here at MIT … and because of what we know we are capable of achieving when called upon ….” The nation that harnessed electricity and the atom is one that has always sought out new frontiers, “and this generation is no different.” Obama invokes the achievements of the past as a call to arms “in what is sure to be a difficult fight in the months and years ahead” -- to ensure that “we are the energy leader that we need to be.”nn;**Link to** - **[[http://mitworld.mit.edu/video/716|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/36|The Office of the President of MIT]]**
11870 en A Beautiful Mind: Genius, Madness, Reawakening Dr. Sylvia Nasar, the author of "A Beautiful Mind" tells the extraordinary story of mathematician John Nash a drama about the mystery of the human mind and shares some of her experiences in writing her prize-winning biography.nn;**Link to** - **[[http://mitworld.mit.edu/video/39|Lecture´s Homepage]]**n;**Lecture Host** - **[[http://mitworld.mit.edu/host/view/25|Department of Mathematics at MIT]]**n;**Series ** - **[[http://mitworld.mit.edu/series/view/11|Applied Mathematics Colloquium]]**
11871 en A New Kind of Science Wouldn’t it be exciting, Stephen Wolfram wonders, to have a little computer program that could function as a precise, ultimate model of our universe? If you ran the program long enough, it would reproduce every single thing that happens. It’s not out of the question, according to Wolfram’s lecture which somehow encapsulates his 1,200-page opus, A New Kind of Science, in a single hour.nnWolfram’s vast and penetrating research uses simple computations to generate complex computer models that resemble designs found in nature. He embraces the really big subjects, and the really small ones—from patterns on mollusk shells and the shapes of leaves and snowflakes, to free will, evolution, and extra-terrestrial life. This new kind of thinking might provide alternatives to evolution in explaining how different forms of life emerged. Wolfram believes his work is already transforming the study of science, as well as making possible a host of new technologies.nn;**Link to** - **[[http://mitworld.mit.edu/video/149|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/25|Department of Mathematics at MIT]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/11|Applied Mathematics Colloquium]]**
11872 en Space Shuttle Discovery Mission to the International Space Station (STS-121) The sign-up sheet for astronaut school is likely to grow even longer after viewing Stephanie Wilson’s reality video about her 13 days in space. Wilson, a self-described “robo chick,” served as a specialist in July 2006 on one of NASA’s return-to-flight test missions following the Columbia accident. She narrates a video account -- a day-to-day diary –- of the work, and fun, she and fellow astronauts engaged in.nnMuch of Wilson’s job involved using a robotic arm to help unload supplies onto the International Space Station, to which the shuttle Discovery was docked for several days. When she wasn’t helping transfer 28,000 pounds of food, gear and experiments, she was assisting crew members on space walks, during which they assembled another piece of the space station and tested a putty-like material for repairing cracks and holes in the shuttle's delicate heat tiles. Wilson, who was operating a 50-foot long robotic boom arm for these jobs, describes the challenge of functioning in “45 minutes of day and 45 minutes of night,” as the astronauts swiftly circled the earth. “It got very cold and dark, and my colleagues said it was very lonely to be at the end of a bendy stick.”nnWilson’s video clearly demonstrates the awesome solitude of these spacewalkers, as well as the mundane, almost household nature of their chores: Astronauts used tools resembling cordless drills to assemble new hardware onto the space station. Her footage also reveals the camaraderie and joy of life above earth. She takes us spinning like a fish through the submarine-narrow chambers of the attached shuttle and space station, and we view astronauts in zero gravity play with floating balls of water containing air bubbles, and attempt to catch myriad M&Ms in their mouths. Wilson herself performs a flipping sequence, admitting, “There’s a child in all of us.”nnTo Wilson’s clear regret, this may be her last shuttle flight. After a mission, an astronaut goes to the bottom of a long list of flight aspirants. But more to the point: NASA, facing budget cuts and the mandate of lunar and Mars missions, will retire the shuttles in 2010, with the goal of sending a new vehicle up in 2014. During the interim years, Russia’s Soyuz space ships will exclusively bear the burden of transport to the space station.nn;**Link to** - **[[http://mitworld.mit.edu/video/391|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/25|Department of Mathematics at MIT]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/11|Applied Mathematics Colloquium]]**
11873 en The Politically Correct Atomic Reactor With energy emerging as an issue of domestic and international importance, the Nuclear Engineering Department at MIT has been quietly working on a technology that it hopes will meet the challenge of providing a clean, safe, and reliable source of electricity. What started in January 1998 as an Independent Activities Period (IAP) project, has blossomed into a full design effort whose goal is to build a nuclear plant that can compete with natural gas, be meltdown proof, and have a waste form that can be disposed of without reprocessing.nnThe technology chosen was a high temperature, helium cooled, gas turbine powered, modular pebble bed reactor which was originally developed in Germany in the late 70's and 80's. The MIT design team is taking a fresh look at all aspects of the technology, from factory manufacture and site assembly to advanced fuel designs, safety analyses, modularity features that allow the entire plant to be shipped by truck, and advanced instrumentation and control systems. This technology was mentioned in the current Bush national energy plan as an example of the type of fresh thinking that is needed.nnThe colloquium will describe the technology and identify opportunities for other departments to contribute to the design of this plant, which might actually be built as a consortium product in collaboration with other universities, national laboratories and industry.nn;**Link to** - **[[http://mitworld.mit.edu/video/46|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/27|Electrical Engineering and Computer Science Department]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/12|EECS Colloquium Series]]**
11874 en Cryptography - Science or Magic? Examples of the "tricks" that can be performed with modern cryptographic techniques will be presented and each trick explored to see whether it is "science" (i.e., it can be proved to do what it seems to do) or "magic" (i.e., what it seems to do is, or may be, only an illusion). The tricks considered will include no-break cryptography, no-leak secret sharing, no-key cryptography, no-see signatures, no-watch coin tossing, and no-knowledge proofs.nn;**Link to** - **[[http://mitworld.mit.edu/video/42|Lecture´s Homepage]]**n;**Host** - **[[http://mitworld.mit.edu/host/view/27|Electrical Engineering and Computer Science Department]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/12|EECS Colloquium Series]]**
11925 en Space Exploration: The Next 100 Years High hopes meet high frustration in this panel, whose participants collectively yearn for a new vision to guide our space program. Andrew Chaikin recommends a three-step self-help regimen to move the program forward: lowering the cost of access to space (the going rate is 10 thousand dollars per pound!); embracing “outside-the-box” ideas; and engaging in a national conversation about space. Supriya Chakrabarti predicts that in around 30 years, NASA will be deploying robotic terrestrial planet finders and using the moon for both tourism and commercial development like mining. This will be possible if in the short term space scientists look for low-cost launch options, which might include exploiting existing missile technology. Richard Binzel puts the odds of a civilization-threatening asteroid impact in the next 100 years at one in a million, but believes the odds are a whole lot better that human beings will be exploring asteroids in space. We’ve got a leg up since we’ve already sent robot reconnaissance to the moons of Jupiter. If we’re worried about catastrophic asteroid strikes, Binzel says, we should start taking incremental steps, such as putting nuclear reactors in space to power vehicles for long inter-planetary journeys. nn;**Link to** - **[[http://mitworld.mit.edu/video/167|Lecture´s Homepage]]**n;**Lecture Host** - **[[http://mitworld.mit.edu/host/view/75|Technology and Culture Forum]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/47|100th Anniversary of Flight]]**
11926 en Flight: The Next 100 Years This panel delves into both the fabled and likely future of air travel. As a boy, Joseph Corn treasured the Popular Mechanics issue whose cover featured a man parking a helicopter in the garage. America’s romance with aviation, which started soon after the Wright Brothers’ flight, wasn’t just about hardware or transportation, says Corn, but about a utopian dream. Air travel would bring about a world of peace and brotherhood – a dream, says Corn, shattered by the dropping of the nuclear bomb. Jane Garvey reminisces about Y2K knuckle-biting – she was bravely aloft on New Year’s Eve 1999. The tremendous growth of regional jet travel and urban hub congestion will shift aviation to rural communities, Garvey projects, but believes no new runways will be built without strong local commitment.nnAllen Haggerty says that with only five airplane manufacturers left, expect streamlined plane travel -- you get to your desired destination more directly but without amenities. Some new and different planes are in the works: the Airbus A-380 will carry 555 passengers, with less noise than a 747. Boeing is designing a cargo plane called “The Pelican,” which will have a 500-foot wing span and length greater than a football field. nn;**Link to** - **[[http://mitworld.mit.edu/video/174|Lecture´s Homepage]]**n;**Lecture Host** - **[[http://mitworld.mit.edu/host/view/75|Technology and Culture Forum]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/47|100th Anniversary of Flight]]**
11940 en Organizational Economics and Management Education In less than an hour, Robert Gibbons not only gets through 200 years of economic history, but outlines a new curriculum in organizational research planned by the Sloan School. Gibbons recounts some classic business stories – for instance, how Birds Eye’s success with frozen peas made the frozen food market “thick,” opening it up to scores of smaller players. Management students must master the basics of economics, says Gibbons, but Sloan must also help them train their sights on the current economy. The planned course offerings will knit together such traditional studies as economic theory, markets and competition with the latest in the fields of corporate governance and culture, technology and innovation.nn;**Link to** - **[[http://mitworld.mit.edu/video/144|Lecture´s Homepage]]**n;**Lecture Host** - **[[http://mitworld.mit.edu/host/view/39|MIT Sloan School of Management]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/41|Back to the Classroom 2003]]**
11941 en Innovation: Are You A Predator or Are You Prey? It seems a well-established truth that new technologies drive out older, established ones. In this lecture, MIT Sloan Professor James Utterback demonstrates just the opposite, that a symbiotic relationship can evolve between new “predator” and older “prey” industries that can sustain both. Using such vivid historical examples as the lightbulb, safety match and mousetrap, he describes how the original companies that created these products thrived even as they were challenged by newer firms that harnessed automated manufacturing or different distribution methods. Playing a remarkable film shot in 1927, Utterback shows how the transition from ice harvesting to mechanical refrigeration expanded the market for both – exemplifying the idea that new and old business ideas can and often do reinforce each other.nn;**Link to** - **[[http://mitworld.mit.edu/video/142|Lecture´s Homepage]]**n;**Lecture Host** - **[[http://mitworld.mit.edu/host/view/39|MIT Sloan School of Management]]**n;**Series** - **[[http://mitworld.mit.edu/series/view/41|Back to the Classroom 2003]]**
11956 en Responding to the environment: Epigenetic variations in heredity and evolution / Odgovor na razmere v okolju: Epigenetska variabilnost pri dedovanju in evoluciji In his theory of evolution, Darwin recognized that thenconditions of life play a role in the generation of hereditarynvariations as well as in their selection. However,nsince the mid 20th century, the Modern Synthesis versionnof Darwinism expelled developmental responses to environmentalnchanges from the study of heritable variation.nIt identifi ed heredity with genetics, with hereditary variationnbeing seen in terms of combinations of randomly generatedngene mutations. This view has dominated evolutionaryntheorizing for the last sixty years. Since the 1990s,ndata coming from developmental biology (particularlynthe molecular aspects of differentiation and morphogenesis),nfrom ecology (in particular ideas about niche constructionnand studies of symbiosis), from behavior (wherenthe transmission of information through social learning isna major focus), and from cultural studies (where the relationnbetween cultural evolution and genetic evolution isnunder scrutiny) is challenging the modern synthesis view.nMarion Lamb and I devoted our recent book Evolutionnin Four Dimensions to this challenge, and identifi ed fourntypes of inheritance (genetic, epigenetic, behavioral, andnsymbol-based), each of which can provide variations on which natural selection will act. Some of these variationsnarise in response to developmental and environmentalnconditions, so developmentally induced or reconstructednheritable variations can be selected and lead to changesnin the nature and frequency of phenotypes in populations.nFurthermore, under certain conditions, the mechanismsnunderlying epigenetic inheritance can also lead to the reorganizationnof the epigenome. In this lecture I reviewnthe challenge that comes from the work on epigeneticsncarried out since the late 1980s, discuss different types ofnepigenetic inheritance mechanisms, examine the prevalence,nstability and inducibility of cellular epigenetic variants,nand point to some of the ways in which epigeneticnmechanisms have affected micro- and macro-evolution.n----nV svoji teoriji o evoluciji je Darwin izpostavil vlogo ?ivljenjskihnrazmer v nastanku dednih razlik in v njihovinselekciji. Od sredine 20. stoletja dalje je moderna sintezandarvinizma izklju?ila razvojne odgovore na okoljskenspremembe iz ?tudij o dednih razlikah. Dedovanjenje izena?ila z genetiko, dedne razlike pa opisovala kotnkombinacije naklju?nih genskih mutacij. Ta pogled jenprevladoval v evolucijskih polemikah zadnjih ?estdesetnlet. Od devetdesetih let dalje novi podatki iz razvojnenbiologije (predvsem molekularni vidiki diferenciacije innmorfogeneze), ekologije (ustvarjanje ni?, raziskave simbioze),nraziskav vedenja (prenos informacij prek socialneganu?enja) in kulturolo?kih raziskav (povezave medngenetsko in kulturno evolucijo) spreminjajo pogled modernensinteze. Z Marion Lamb sva o teh izzivih pisali vnnajini knjigi Evolution in Four Dimensions, v kateri svandolo?ili ?tiri tipe dedovanja (genetsko, epigenetsko, vedenjskonin simbolno), od katerih lahko vsak predstavljanvir razlik, na katerih deluje naravni izbor. Nekatere odnteh razlik izvirajo iz razvojnih in okoljskih razmer, karnvodi v spremembe v naravi in v frekvenci fenotipov vnpopulaciji. Nadalje lahko mehanizmi, ki so odgovorni za epigenetsko dedovanje, pod dolo?enimi pogoji vodijo vnreorganizacijo epigenoma. V tem predavanju bom predstavilanpregled epigenetskih raziskav od poznih osemdesetihnlet naprej, nadalje bom predstavila razli?ne tipenoz. mehanizme epigenetskega dedovanja, prevalenco,nstabilnost in inducibilnost razli?nih epigenetskih variantnin izpostavila nekatere na?ine, s katerimi so epigenetskinmehanizmi vplivali na mikro- in makroevolucijo.
11970 en The ecosystem concept / Koncept ekosistema The Convention on Biological Diversity (CBD) defi nesnan “ecosystem” as a “dynamic complex of plant, animalnand micro-organism communities and their non-livingnenvironment interacting as a functional unit” (see alsonTansley, 1935; Christopherson, 1996). The basic idea isnthat living (and dead?) organisms are permanently involvednin the exchange of energy, material and informationnwith their environment. Ecosystem function is governednby organisms acting at various stages within the system.nPrimary producers (green, photosynthetically activenplants and bacteria) form the base of the system. Theynhave the unique ability to fi x and conserve the energynof extraterrestrial radiation and produce digestible carbohydrates,nlipids and proteins. They are hierarchicallynfollowed by herbivorous/phytophagous organisms feedingnon them. The fi rst order consumers are the preynfor second order consumers (carnivores) which huntnand feed on herbivores. There may be further levels andnsteps in carnivorous consumers depending on the ecosystemsnselected. Finally, decomposers (destruents andnreducers) are mineralizing dead organic material, thusnplaying their role in organic material recycling – a prerequisitenfor plant growth. There is a broad spectrum ofnecosystems including terrestrial and aquatic ones. Deepnsee, coral reefs, sea shores, ponds, rivers and creeks arenaquatic ecosystems as are tundra, taiga, deserts, or grassnlands on the terrestrial side. But also anthropogenicallyninfl uenced systems may be called ecosystems like urbannareas, gardens or waste land. Ecosystems can be threatenednby various stressors. The toxicity of introducednelements (toxic gases, heavy metals, pesticides) or thenaggressiveness of newly introduced or invasive speciesn(global warming, economic globalization) are only a fewnexamples of gradually unbalancing existing ecosystems.n----nV konvenciji o biolo?ki raznolikosti (CBD) je ekosistemnopredeljen kot dinami?en kompleks zdru?b rastlin, ?ivalinin mikroorganizmov ter njihovega ne?ivega okolja, kinskupaj delujejo kot funkcionalna enota (glej tudi Tansley,n1935; Christopherson, 1996). Osnovna ideja je, da son?ivi (in odmrli?) organizmi stalno vklju?eni v izmenjavonenergije, snovi in informacij s svojim okoljem. Delovanjenekosistema poganjajo organizmi s svojim delovanjem nanrazli?nih stopnjah sistema. Primarni producenti (zelene,nfotosintetsko aktivne rastline in bakterije) so baza ekosistema,nker lahko sprejmejo in zadr?ijo energijo zunajzemeljskegansevanja in proizvedejo razgradljive ogljikovenhidrate, ma??obe in beljakovine. Naslednjo stopnjo predstavljajonrastlinojedi organizmi. To so potro?niki prveganreda in predstavljajo plen za potro?nike drugega redan(mesojedci), ki rastlinojedce lovijo in se z njimi hranijo.nV izbranem ekosistemu je lahko ve? zaporednih stopenjnmesojedih potro?nikov. Kon?no pa razkrojevalci razgradijonmrtev organski material in igrajo klju?no vlogo v recikliranjunsnovi, potrebnih za rast rastlin. Obstaja mnogonekosistemov, vklju?no s kopenskimi in vodnimi: globokonmorje, koralni grebeni, obale, ribniki, reke in potoki sonprimeri vodnih ekosistemov, tako kot so tundra, tajga,npu??ava in travi??a primeri kopenskih. Med ekosistemenlahko pri?tevamo tudi antropogene, kot so urbane povr?ine,nvrtovi, odlagali??a odpadkov. Na ekosisteme lahkonvplivajo razli?ni stresni dejavniki (strupeni plini, te?kenkovine, pesticidi) ali agresivnost novih oz. invazivnihnvrst (globalno segrevanje, ekonomska globalizacija).
11975 en Molecular markers and their application in biogeography / Molekulski markerji in njihova uporaba v biogeografiji The application of molecular tools (DNA sequences,nDNA fingerprinting) has revolutionized biogeographicnresearch over the last two decades. Molecular biogeographyncan be summarized under “phylogeography”, anterm coined by John Avise twenty years ago. It is describingnthe branch of science that explores and interprets thengeographical distribution of genealogical lineages. In mynpresentation, I am going to discuss the following issues:n1. Why should we use molecular markers in bio geographicalnresearch?n2. Which types of molecular markers are being used,nhow are the data generated and how do they looknlike?n3. Which questions can be answered with molecularndata and which ones not?nTo illustrate the above points, I will present studies thatnexplored the effect of the glaciations of the ice ages onnthe flora of the Alps as well as the EU 6th framework projectn“INTRABIODIV” as an example for the integrationnof molecular data, habitat data and species distributionndata.nA starting point for that project was that species richnessnis the most widely used measure for biodiversity assessment.nHowever, it is intraspecific diversity (geneticnpolymorphism) that represents the evolutionary andnadaptive potential of each species in changing environments.nThe main point of the project was to study possible correlationsnbetween intraspecific diversity and species richnessnor habitat variation. The objectives were:n(i) to fi nd and explain possible relationships among inter-nand intraspecifi c plant diversity and habitat variation,n(ii) to elaborate a modeling approach to predict intraspecific plant diversity, using more efficiently accessiblensurrogates, on a large scale,n(iii) to establish tools for the design of a network of protectednareas to effectively ensure the sustainable managementnof natural genetic resources.nThe following questions were asked, using the Alps andnthe Carpathians as model systems:n(i) Is there congruence between intra-/interspecific biodiversity?n(ii) Do areas of high endemism, often coinciding withnglacial refugia, harbour a great degree of intraspecificndiversity?n(iii) Is habitat variation, characterized by environmentalnparameters, a good surrogate for intra- and interspecific diversity?nIn order to accomplish the aims, intraspecific diversity wasnmapped by using molecular markers in 25 model species,nthe species richness was mapped on the same area usingnmainly existing data on plant distributions. Furthermore,nenvironmental data were compiled for a map of habitatndiversity, and finally these maps were compared to findnpossible correlations among these variables.n----nUporaba molekularnih orodij (zaporedje DNA, prstninodtis DNA) je v zadnjih dvajsetih letih prevetrila biogeografskenraziskave. Molekularno biogeografi jo lahkonpovzamemo kot filogeografijo (izraz je skoval JohnnAvise) in predstavlja tisto znanost, ki odkriva in razlagangeografsko raz?irjenost genealo?kih linij. Predstavil bomnnaslednja vpra?anja:n1. Zakaj uporabljati molekularne markerje v biogeografskihnraziskavah?n2. Tipe molekularnih markerjev, na?ine pridobivanjanin vrste podatkov.n3. Na katera vpra?anja lahko odgovorimo z molekularniminpodatki in na katera ne moremo?nKot primer povezovanja molekularnih, habitatnih in podatkovno raz?irjenosti vrst bom predstavil raziskavo onposledicah poledenitev med ledenimi dobami na alpskonfloro in evropski program 6. okvira INTRABIODIV. Izhodi??entega projekta je bilo dejstvo, da je pestrost vrstnnajpogostej?i kazalnik biodiverzitete. Vendar pa intraspecifin?na raznolikost (genetski polimorfizem) predstavljanevolucijsko in prilagoditveno mo? vsake od vrst vnspreminjajo?ih se okoljih. Glavni poudarek projekta jenbila mo?nost povezave med intraspecifi ?no raznolikostjonin pestrostjo vrst oz. ranolikostjo habitatov. Cilji so bili:n1) poiskati in razlo?iti mo?na razmerja med inter- in intraspecifin?no raznolikostjo rastlin ter habitatno raznolikostjo,n2) izdelati model za napoved intraspecifi ?ne raznolikostinrastlin v velikem obsegu z uporabo bolj dostopnih nadomestkov,n3) dolo?iti orodja za izdelavo mre?e za??itenih obmo?ijnin s tem zagotoviti u?inkovito trajnostno upravljanjennaravnih genetskih virov.n?eleli smo odgovoriti na naslednja vpra?anja, pri ?emernsmo uporabili Alpe in Karpate kot modelna sistema:n1) Ali obstaja skladnost med intra- in interspecifi?no raznolikostjo?n2) Ali imajo obmo?ja z veliko pojavnostjo endemitov innobenem tudi ledenodobnih reliktov visoko stopnjo intraspecifi?ne raznolikosti?n3) Ali so habitatne razlike, ki jih dolo?ajo okoljski parametri,ndober pribli?ek za intra- in interspecifi?no raznolikost?nDa bi dosegli na? namen, smo pregledali intraspecifi?nonraznolikost z molekularnimi markerji pri 25 modelnihnvrstah. Na istih obmo?jih smo ocenili vrstno pestrost izndostopnih podatkov o raz?irjenosti rastlin. Okoljske podatkensmo vklju?ili kot kazalnike habitate raznolikosti.nNa koncu smo vse tri vrste podatkov primerjali, da binna?li morebitne povezave med vsemi spremenljivkami.
12173 en Lecture 5: The Cognitive Architecture 1 
12174 en Lecture 6: The Cognitive Architecture 2 
12175 en Lecture 7: The Cognitive Architecture 3 
12176 en Lecture 8: The Cognitive Architecture 4 
12177 en Lecture 9: The Cognitive Architecture 5 
12178 en Lecture 10: The Cognitive Architecture 6 
12179 en Lecture 11: Theories of Knowledge 1 
12180 en Lecture 12: Theories of Knowledge 2 
12181 en Lecture 13: Theories of Knowledge 3 
12182 en Lecture 14: Theories of Knowledge 4 
12183 en Lecture 17: Theories of Knowledge 5 
12184 en Lecture 15: Complex Cognition 1 
12185 en Lecture 16: Complex Cognition 2 
12186 en Lecture 18: Complex Cognition 3 
12187 en Lecture 19: Complex Cognition 4 
12188 en Lecture 20: Emotion, Motivation, and Volition 1 
12189 en Lecture 21: Emotion, Motivation, and Volition 2 
12190 en Lecture 22: Emotion, Motivation, and Volition 3 
12191 en Lecture 23: Emotion, Motivation, and Volition 4 
12192 en Lecture 25: Emotion, Motivation, and Volition 5 
12193 en Lecture 26: Emotion, Motivation, and Volition 6 
12194 en Lecture 24: Cognitive Development Through the Life Span 1 
12195 en Lecture 27: Cognitive Development Through the Life Span 2 
12196 en Lecture 28: Cognitive Development Through the Life Span 3 
12197 en Lecture 29: Cognitive Development Through the Life Span 4 
12198 en Lecture 30: Cognitive Development Through the Life Span 5 
12199 en Lecture 31: Cognitive Development Through the Life Span 6 
12200 en Lecture 32: The Brain and Cognition 1 
12201 en Lecture 33: The Brain and Cognition 2 
12248 en The Cyc Lexicon The Cyc knowledge base (KB) is a formalized representation of a vast quantity of fundamental human knowledge: facts, rules of thumb, and heuristics for reasoning about the objects and events of everyday life. The medium of representation is the formal language CycL, described below. The KB consists of terms--which constitute the vocabulary of CycL--and assertions which relate those terms. These assertions include both simple ground assertions and rules. Cyc is not a frame-based system: the Cyc team thinks of the KB instead as a sea of assertions, with each assertion being no more "about" one of the terms involved than another.nnThe Cyc KB is divided into many (currently thousands of) "microtheories", each of which is essentially a bundle of assertions that share a common set of assumptions; some microtheories are focused on a particular domain of knowledge, a particular level of detail, a particular interval in time, etc. The microtheory mechanism allows Cyc to independently maintain assertions which are prima facie contradictory, and enhances the performance of the Cyc system by focusing the inferencing process.nnAt the present time, the Cyc KB contains nearly five hundred thousand terms, including about fifteen thousand types of relations, and about five million facts (assertions) relating these terms. New assertions are continually added to the KB through a combination of automated and manual means. Additionally, term-denoting functions allow for the automatic creation of millions of non-atomic terms, such as (LiquidFn Nitrogen); and Cyc adds a vast number of assertions to the KB by itself as a product of the inferencing process.
12290 en PAC-Bayes Theory in Supervised Learning 
12293 en Incompatibilities(?) between PAC-Bayes and Exploration 
12294 en Bounding the Gaussian Process Information Gain: Applications to PAC-Bayes and GP Bandit Optimization 
12296 en PAC-Bayes, Sample Compress and Kernel Methods 
12297 en PAC-Bayes Analysis: Links to Luckiness and Applications 
12299 en Expectation-prior PAC-Bayes Bounds for SVMs 
12300 en Data-dependent Prior PAC-Bayes Bounds: Empirical Study 
12301 en PAC-Bayesian Analysis in Unsupervised Learning 
12302 en Bayes Average Case Performance of PAC - Bayes Bounds 
12303 en PAC Bayesian Bounds for Spare Regression Estimation with Exponential Weights 
12304 en Efficient Mixture Modeling with RKHS Embeddings: A PAC-Bayesian Analysis 
12339 en Reconstructing networks from experimental and natural genetic perturbations Functional genomics has demonstrated considerable success in inferring the inner working of a cell through analysis of its response to various perturbations. Perturbations can take the form of experimental interventions, like gene deletions or RNA interference, or natural perturbations, like SNPs or copy-number alterations. In my talk I will describe methods my lab has developed to reconstruct networks from the phenotypic effects of gene perturbations. In particular, I will (1) describe Nested Effects Models, a class of probabilistic graphical to reconstruct signaling pathways from downstream effects, and (2) introduce methods to correlate the impact of copy-number variation on gene expression with different sub-types of breast cancer.
12340 en Networking genes and drugs: Understanding gene function and drug mode of action from large-scale experimental data A gene regulatory network, where two genes are connected if they are directly, or functionally, regulating each other, can be 'reverse-engineered' from large-scale experimental data such as gene expression profiles. Here used a simple but effective reverse-engineering approach using all the available gene expression profiles in mammals, solving along the way the problems of handling, normalizing and analysing such massive dataset. We reverse-engineered a coexpression network for Homo Sapiens (Mus Musculus) from a set of 20,255 (8895) gene expression profiles. The human (mouse) network is characterized by a set of 22283 (45101) nodes (i.e. genes) and a set of 4,817,629 (14,641,095) edges, where the edge is weighted by the Mutual Information (MI) measure between the two genes.nnnWe show how the resulting network can be then used to understand the function of a gene, the modularity of gene regulation, as well as, as a tool to analyse "gene signatures" to identify the mode of action of a drug.nnnWe will also show how it is possible to use gene expression profile to build a "drug network", where drugs can be automatically grouped in subnetworks ('communities') of drugs sharing a similar mode of action.
12342 en Estimating the contribution of non-genetic factors to gene expression using Gaussian process latent variable models Thanks to the recent increase in the amount of genetic profiling data available and to the ability to characterize disease activity through gene expression, it is possible to understand more in detail the multitude of causal factors linked with each disease. This is a challenging task because the integration of different sources of biological data is not straightforward and because non-genetic factors (such as differences in the experimental setting or individual characteristics such as gender and ethnicity) are not always artificially controlled. Since these non-genetic factors may cause most of the variation in gene-expression reducing the accuracy of genetic studies, there’s a pressing need for models that take them explicitly into account. We present a model in which non-genetic factors are unobserved latent variables the gene expression levels can be described as linear functions of both these latent variables and Single Nucleotide Polymorphisms (SNPs). From a generative point of view, we can see the gene expression levels Y as nnY = SV + XW +mu 1^T + epsilon nnWhere S is the matrix containing the SNPs, X are the latent variables, V and W are mapping matrices, is a Gaussian distributed isotropic error model and mu allows the model to have non-zero mean.nnThe model is inspired by the one proposed by Stegle et al. [1], but instead of optimizing parameters and marginalising latent variables (as in Probabilistic PCA), we marginalise the parameters and optimize the latent variables. For a particular choice of prior over the mapping matrices W and V the two approaches are equivalent.nnThis kind of model is called dual Probabilistic PCA and it belongs to a wider class of models called Gaussian Process - Latent Variable Models. Indeed, dual PPCA is the special case where the output dimensions are assumed to be linear, independent and identically distributed. Each of these assumptions can be relaxed obtaining new probabilistic models. Many extensions of this model are possible, but even in its simplest form the eQTL study results are extremely promising in terms of number of significant associations found.n
12343 en Using sequential Monte Carlo approaches as a design tool in synthetic biology In many engineering contexts it is easy to state what we want but hard to achieve our desired outcomes. The more potential solutions exist, the harder it becomes to identify optimal solutions. Here we show how this problem can be approached in an approximate Bayesian computation framework. Our approach has the advantage that it builds on the powerful Bayesian model selection formalism, includes sensitivity and robustness analysis at no extra cost, and flexibly incorporates diverse design objectives. We illustrate the performance of this approach in the context of bacterial two-component systems (TCS). These systems enable prokaryotes (and some simple eukaryotes and plants) to sense their environments and adapt their internal state to changing circumstances. We present a detailed analysis of orthodox and unorthodox TCSs and show how we can rationally construct TCS that show robust and optimal response characteristics to different stimuli encountered during bacterial infections or in biotechnological (e.g. biofuels production and bioremediation) applications. We conclude by elaborating on the connections between our approach and maximum-entropy procedures and the advantages over traditional engineering strategies.n
12345 en Decoding underlying behaviour from destructive time series experiments through Gaussian process models A major problem for biological time series is that often experiments (such as gene expression measurements using microarrays or RNA-seq) require the organism or cells to be destroyed. This means that a particular time series is often a series of measurements of different organisms (or batches of cells) at different times. Biological replicates normally consist of a separate biological sample measured at the same time. With the advent of single cell expression experiments, where it is not currently conceivable to make genome-wide gene expression measurements without destroying the cell, we expect such set ups to be sustained.nnMany existing approaches to modelling transcriptional data postulate a differential equation model for continuous-time expression profiles from which the repeated observations arise. Two ways of modelling repeat experiments would be either to handle repeated observations as being from a shared profile, or from completely independent profiles. The former approach assumes that gene expression profile for each experiment does not vary, whilst the latter approach assumes no relationship between the gene expression profiles. For many experimental set ups we might expect something in between these two extremes where, whilst each individual measurement comes from a different collection of cells or a different organism, the experimental set up is broadly the same. We therefore expect some shared affects and some independent affects for the experiments.nnIn this work we propose an integrated Gaussian process framework for analysis of such experiments. In our approach, independent aspects of the experiments are modelled as independent Gaussian process draws, while the common profile across the experiments is modelled by a separate Gaussian process. The method adds power through sharing of replicates for the common profile while being robust to outliers from individual rogue experiments.
12347 en Identifying interactions in the time and frequency domains in local and global networks Reverse-engineering approaches such as Bayesian network inference, ordinary differential equations (ODEs) and information theory are widely applied to deriving causal relationships among different elements such as genes, proteins, metabolites, neurons, brain areas and so on, based upon multi-dimensional spatial and temporal data. Here we focused on the Granger causality approach in both the time and frequency domains in local and global networks, and applied our approach to experimental data (genes and proteins). For a small gene network, Granger causality outperformed all the other three approaches mentioned above. A global protein network from 812 proteins was reconstructed, using a novel approach. The obtained results fitted well with known experimental findings and opened up many experimentally testable predictions. In addition to interactions in the time domain, interactions in the frequency domain were also recovered. Our approach is general and can be easily applied to other types of temporal data.
12349 en Deterministic and stochastic models of bicoid protein gradient formation in Drosophila embryos: Modelling maternal mRNA degradation Passive diffusion of a class of molecules known as morphogens as a mechanism that helps to establish spatial patterns of gene expression during embryonic development was proposed by Turing [1]. This mechanism is usually modelled as passive diffusion of morphogen proteins translated from maternally deposited messenger RNAs. Such diffusion models assume a constant supply of morphogens at the source throughout the establishment of the required profile at steady state [2]. Working with the bicoid morphogen which establishes the anterior-posterior axis in the Drosophila embryo, we note that this constant source assumption is unrealistic since the maternal mRNA is known to decay after a certain time since egg laying. In [3], we have incorporated a more realistic model of the morphogen source since the maternal mRNA should be expected to decay.We explicitly model the source as a constant supply followed by exponential decay and solve the reaction diffusion equation numerically for one dimensional morphogen propagation. By minimising the squared error between model outputs and measurements published in the FlyEx database, we show how parameters of diffusion rate, mRNA and protein decay constants, and the onset of maternal mRNA decay can be assigned sensible values. We also extend this work to further show how such a realistic source model may be combined with a recently published flow model [4] that takes into account advective transport. Moreover, a stochastic simulation based model [5] which includes Bicoid molecule reactions has also been implemented with new source model in our work.
12350 en Statistical analysis of protein patternation on cell membranes during immunological synapse A statistical analysis of two different experiments is considered. Both of them are related to understanding the mechanism behind the distribution of molecules involved in formation of organized patterns of protein complexes and molecules in the contact interface between the membranes of an immune cell and an antigen presenting cell. Such patterns are called immunological synapses.nnIn the first experiment a T-cell is adhering to the flat surface of a lipid bilayer. There are molecules of two types on the surface of the bilayer. They are fluorescently labelled with different colours so their distribution can be observed using microscope. During the contact molecules of one type are binding while second type molecules stay unbound. This results in segregation of different type molecules and forming a synapse pattern that can be observed and scanned using confocal microscopy. In the case of lipid bilayer the contact interface is flat and the whole contact interface can be scanned as a single image.nnThe second experiment deals with NK-cells forming synapses with target antigen presenting cells. Two-colour fluorescent labelling is used again and a similar protein patternation on the cell-cell contact interface can observed using confocal microscopy. The main difference with the first experiment is in imaging technique as instead of a single image a series of confocal images is made along the same axis which is approximately parallel to the synapse interface. As a result a stack of cross-section fluorescence images of the interacting cells is considered for the quantitative analysis.nnIn both experiments it is possible to observe the segregation of labelled molecules during the formation of the synapse pattern. In terms of fluorescence intensity values this is expressed in strong negative correlation between different colour fluorescence. We introduce a model based on the hypothesis of exclusion by size which explains the mutual segregation of molecules as a result of elastic properties of single molecules and bonds combined with the properties of the cell membrane. Based on this model a computational algorithm for the Bayesian statistical analysis of fluorescence images is developed in order to estimate relevant physical parameters that cannot be measured explicitly.n
12351 en Machine learning methods for effective proteomics image analysis Two-dimensional gel electrophoresis (2DGE) remains the most widely used method for proteins identification and differential expression analysis, due to its lower cost and the existence of mature commercial software tools for 2DGE image analysis, despite the fact that non-gel based methods are gaining in popularity. Although there are several software packages that promise automation of the whole protein spot detection and quantification process, the hard reality remains today [1] that as Fey and Larsen stated in 2001, "There is no program that is remotely automatic when presented with complex 2-DE images" ... "most programs require often more than a day of user hands-on time to edit the image before it can be fully entered into the database‚" [2]. nnTo address these limitations and develop an automated 2DGE image analysis workflow we have developed in previous works an effective image analysis methodology that first denoises the 2DGE image based on the Controurlet transform [3] and then separates effectively the parts of the denoised image which include true protein spots (to be called Regions of Interest (ROIs) from the background-only areas, by using Active Contours (AC) without edges [4]. In this work we complete the image analysis workflow by adding a well tuned pipeline of operations based on unsupervised machine learning methods for analyzing further each isolated ROI, in order to "fish" in it the centers and estimate the quantities of the individual "hidden" spots.One-dimensional mixture modeling of the ROI pixel intensities histogram is applied first to identify and remove any remaining background pixels. Then the surviving ROI pixels are used as "molecules generators", in order to convert (by random sampling) the processed ROI image to an isomorphic dataset (through appropriate random sampling) representing the distribution of molecules of the underlying protein species (that are "projected" as spots on the gel image). This reverse engineering action rooted on machine learning constitutes a unique innovation of this work that, to the best of our knowledge, has not been applied before in 2DGE image analysis. The candidate protein spot centers are then located by applying hierarchical clustering. Finally the individual spot boundaries are delineated by fitting 2D Gaussian models to the data using generalized mixture modeling and the Minimum Message Length (MML) criterion to control the best model complexity. An extensive evaluation of this novel spot modeling methodology using both real and synthetic 2DGE images reveals that it is more precise and more specific than PDQuest in terms of spot detection while both methods achieve comparable high sensitivity. Furthermore, it can estimate more reliably the volumes of the extracted spots, even in the presence of substantial noise and in areas of the image where faint and overlapping (or saturated) spots are located close to each other. It should be noted that the end-to-end workflow that we have developed for 2DGE image analysis does not require any re-calibration of parameters every time a new gel image is presented for analysis. This desirable characteristic makes it a suitable candidate for the automatic processing of image stacks, as needed for highthroughput proteomics analysis to support systems biology projects.
12355 en Ethics for the New Millennium Ethics for the New Millennium is addressed to a general audience. It presents a moralnframework based on universal rather than religious principles. It rests on the observationnthat those whose conduct is ethically positive are happier and more satisfied and the beliefnthat much of the unhappiness we humans endure is actually of our own making. Itsnultimate goal is happiness for every individual, irrespective of religious belief.nThough the Dalai Lama is himself a practicing Buddhist, his approach to life and the moralncompass that guides him can be of use to each and every one of us – Muslim, Christian,nJew, Buddhist or atheist – in our quest to lead a happier, more fulfilling life.nAccording to the Dalai Lama our survival has depended and will continue to depend on ournbasic goodness as human beings. In the past, the respect people had for their religionnhelped maintain ethical practice through a majority following one religion or another.nToday, with the growing secularization and globalization of society, we must find a waynthat transcends religion to establish consensus as to what constitutes positive and negativenconduct, what is right and wrong and what is appropriate and inappropriate.nnLink to [[http://www.dalailama.com/| The Office of His Holiness the Dalai Lama]]\\nLink to [[http://www.dalailamafoundation.org/dlf/en/index.jsp| The Dalai Lama Foundation]]
12378 en Lecture 34: The Brain and Cognition 3 
12379 en Lecture 35: The Brain and Cognition 4 
12380 en Lecture 36: The Brain and Cognition 5 
12381 en Lecture 37: Assessment and Individual Differences 1 
12382 en Lecture 38: Assessment and Individual Differences 2 
12383 en Lecture 39: Assessment and Individual Differences 3 
12384 en Lecture 40: Assessment and Individual Differences 4 
12437 en Cholera, Canker Rash and Consumption: Historical epidemiology and nosology in Massachusetts, 1850-1920 
12438 en Leadership, Regulatory Agencies and Public Health 
12439 en California Department of Public Health and Pandemics: All Hands on Deck! 
12440 en Health Under Siege: The Gazan Model 
12441 en Rural Hypertension in China, a Growing Epidemic 
12469 en Course introduction In the first lecture, an introduction to solar cells is given. Simple examples illustrate the relevance of this topic. The question: Why solar cells? Will be answered in more than one way.
12470 en Solar radiation In this session, solar radiation is reviewed. Spectralconsiderations and its relation to improve the performance of solar cells isaddressed.
12471 en Semiconductor properties In this lecture, the semiconductor physics thatare involved in the generation of photovoltaic energy are studied. Theabsorption, separation and collection concepts are explained
12472 en PN junction After the study of the physics principles, in this session a deeper review of the topic is carry on. In the lecture, the PN junction is covered: Part 1 presents the basics to be used in Part 2.
12473 en Losses and Optimization In this lecture, the cell performance limits are explained. Techniques to improve the performance of the solar cells end up with the record c-Si solar cells. Topics related to losses (Part 1) and optimization (Part 2) are shown.
12474 en Crystalline Si Technology In this lecture, crystalline silicon technology is reviewed in two parts:Part 1 and Part 2.
12475 en PV system + design Practical use of solar cells is as important as understand it, therefore a design strategy for PV systems is made in this lecture.PV systems are studied in two parts: Part 1 and Part 2.
12476 en Organic solar cells Organic solar cells are an excellent option to continue with the research of solar cells. In this lecture the last principles and advances in organic solar cells are presented: Part 1 and Part 2.
12479 en Why users need semantic search While users dependence on search continues to increase, user satisfaction is not improving. This is partly because search is hard, and partly because users are becoming more demanding and pushing search beyond the traditional scope of information retrieval. Our research reveals three key problems of search: imprecise results, need for query refinement, and need to support complex tasks and decisions. Semantic technologies can help address these problems by providing improvements to core search results and also through enabling richer user experiences such as faceted navigation, entity-centered experiences, and task completion and decision tools.
12480 en Paraphrasing Invariance Coefficient: Measuring Para-Query Invariance of Search Engines Paraphrasing is the restatement (or reuse) of text which preserves its meaning in another form. A para-query is a paraphrase of a search query. Humans easily recognize paraqueries, but search engines are still far away from it. We claim that in order for a search engine to be called semantic it is necessary that it recognizes para-queries by returning the same search results for all para-queries of a given query.nRecognizing para-queries is an important and desired ability of a search engine. It can relieve users of the burden of rephrasing queries in order to improve the relevance of results.
12481 en Using BM25F for Semantic Search Information Retrieval (IR) approaches for semantic web search engines have become very popular in the last years. Popularization of different IR libraries, like Lucene, that allows IR implementations almost out-of-the-box have make easier IR integration in Semantic Web search engines. However, one of the most important features of Semantic Web documents is the structure, since this structure allow us to represent semantic in a machine readable format. In this paper we analyze the specific problems of structured IR and how to adapt weighting schemas for semantic document retrieval.
12482 en Distributed Indexing for Semantic Search In this paper we describe the process of building indices for semantic search using MapReduce. We compare the two most straightforward representations of RDF data, the horizontal index structure using parallel indices and the vertical index structure using fields. We measure the cost of building indices and also compare retrieval performance on keyword queries and queries restricted to particular properties.
12483 en Dear Search Engine: What’s your opinion about...? - Sentiment Analysis for Semantic Enrichment of Web Search Results Search Engines have become the main entry point to Web content, and a large part of the \visible" Web consists in what is presented by them as top retrieved results. Therefore, it would be desirable if the first few results were a representative sample of the entire result set. This paper provides a preliminary study about opinions contained in search engine results for controversial queries such as "cloning" or "immigration". To this end, we extract sentiment metadata from web pages, and compare search engine results for several queries. Furthermore, we compare opinions expressed in the top results to those in other retrieved results to examine whether the top-ranked pages are a good sample of all results from an opinion perspective. In a preliminary empirical analysis, we compare up to 50 results from 3 commercial search engines on 14 controversial queries to study the relation between sentiments, topics, and rankings.
12484 en Automatic Modeling of User's Real World Activities from the Web for Semantic IR We have been developing a task-based service navigation system that offers to the user services relevant to the task the user wants to perform. The system allows the user to concretize his/her request in the task-model developed by human-experts. In this study, to reduce the cost of collectingna wide variety of activities, we investigate the automatic modeling of users’ real world activities from the web. To extract the widest possible variety of activities with high precision and recall, we investigate the appropriate number of contents and resources to extract. Our results show that we do not need to examine the entire web, which is too time consuming; a limited number of search results (e.g. 900 from among 21,000,000 search results) from blog contents are needed. In addition, to estimate the hierarchical relationships present in the activity model with the lowest possible error rate, we propose a method that divides the representation of activities into a noun part and a verb part,nand calculates the mutual information between them. The result shows almost 80% of the hierarchical relationships can be captured by the proposed method.
12485 en The Wisdom in Tweetonomies: Acquiring Latent Conceptual Structures from Social Awareness Streams Although one might argue that little wisdom can be conveyed in messages of 140 characters or less, this paper sets out to explore whether the aggregation of messages in social awareness streams, such as Twitter, conveys meaningful information about a given domain. As a research community, we know little about the structural and semantic properties of such streams, and how they can be analyzed, characterized and used. This paper introduces a network-theoretic model of social awareness stream, a so-called "tweetonomy", together with a set of stream-based measures that allow researchers to systematically define and compare different stream aggregations. We apply the model and measures to a dataset acquired from Twitter to study emerging semantics in selected streams. The network-theoretic model and the corresponding measures introduced in this paper are relevant for researchers interested in information retrieval and ontology learning from social awareness streams. Our empiricalnfindings demonstrate that different social awareness stream aggregations exhibit interesting differences, making them amenable for different applications.
12486 en A Large-Scale System for Annotating and Querying Quotations in News Feeds In this paper, we describe a system that automatically extracts quotations from news feeds, and allows efficient retrieval of the semantically annotated quotes. APIs for real-time querying of over 10 million quotes extracted from recent news feeds are publicly available. In addition, each day we add around 60 thousand new quotes extracted from around 50 thousand news articles or blogs. We apply computationalnlinguistic techniques such as co-reference resolution, entity recognition and disambiguation to improve both precision and recall of the quote detection. We support faceted search on both speakers and entities mentioned in the quotes.
12487 en Entity Search: Building Bridges between Two Worlds We consider the task of entity search and examine to which extent state-of-art information retrieval (IR) and semantic web (SW) technologies are capable of answering information needs that focus on entities. We also explore the potential of combining IR with SW technologies to improve the end-to- end performance on a specfiic entity search task. We arrive at and motivate a proposal to combine text-based entity models with semantic information from the Linked Open Data cloud.
12488 en Methodology and Campaign Design for the Evaluation of Semantic Search Tools The main problem with the state of the art in the semantic search domain is the lack of comprehensive evaluations. There exist only a few efforts to evaluate semantic search tools and to compare the results with other evaluations of their kind.nIn this paper, we present a systematic approach for testing and benchmarking semantic search tools that was developed within the SEALS project. Unlike other semantic web evaluations our methodology tests search tools both automatically and interactively with a human user in the loop. This allows us to test not only functional performance measures, such as precision and recall, but also usability issues, such as ease of use and comprehensibility of the query language. The paper describes the evaluation goals and assumptions; the criteria and metrics; the type of experiments we will conduct as well as the datasets required to conduct the evaluation in the context of the SEALS initiative. To our knowledge it is the first effort to present a comprehensive evaluation methodology for Semantic Web search tools.
12489 en Discussion on the Entity Search Track 
12490 en Predicting Positive and Negative Links in Online Social Networks We study online social networks in which relationships can be both positive (indicating friendship) and negative (indicating opposition or antagonism). Such a mix of positive and negative links arise in a variety of online settings; we study datasets from Epinions, Slashdot and Wikipedia. Despite the diversity of settings considered, we find that the signs of links in the underlying social networks can be predicted with high accuracy using models that generalize across the different domains. These models provide insight into some of the fundamental principles that drive the formation of signed links in networks, and also shed light on theories of balance and status from social psychology.
12491 en Empirical Comparison of Algorithms for Network Community Detection Detecting clusters or communities in large real-world graphs such as large social or information networks is a problem of considerable interest. In practice, one typically chooses an objective function that captures the intuition of a network cluster as set of nodes with better internal connectivity than external connectivity, and then one applies approximation algorithms or heuristics to extract sets of nodes that are related to the objective function and that “look like” good communities for the application of interest. In this paper, we explore a range of network community detection methods in order to compare them and to understand their relative performance and the systematic biases of clusters they identify. We evaluate several common objective functions that are used to formalize the notion of a network community, and examine several different classes of approximation algorithms that aim to optimize such objective functions. In addition, rather than simply fixing an objective and asking for an approximation to the best cluster of any size, we consider a size-resolved version of the optimization problem. Considering community quality as a function of its size provides a much finer lens with which to examine community detection algorithms, since objective functions and approximation algorithms often have non-obvious size-dependent behavior.
12492 en Statistical Models of Music-listening Sessions in Social Media User experience in social media is characterized by rich interaction with the media content and other participants within the online community. We use statistical models to describe the patterns of music listening in online communities at different levels of model complexity. First, we adapt the LDA model to capture the users’ taste in songs and identify the corresponding clusters of media and users. Second, we define a graphical model that takes into consideration the listening sessions and captures the listening mood of users. Our session model yields clusters of media and users that capture the behavior exhibited across listening sessions, and it allows faster inference when compared to the LDA model. Our experiments with the data from an online media site (Zune Social music community) demonstrate that the session model is better in terms of the perplexity on the music genre co-occurrence compared to the LDA-based taste model that does not incorporate cross-session information and a baseline model that does not use latent clusters.
12493 en Video Search: Are Algorithms All We Need? During the 1990s, search technology improved sufficiently to handle large volumes of textual material without the need for manual abstracting, indexing, and cataloging. Taking professional cataloguers out of the process of text indexing created enormous value; an analogous set of advances is underway with video in the 2010s. But how and when will it be possible to take professionals out of the process of cataloging video manually? How can we expect different approaches to video search and search UI to evolve, and which ones will prove most useful? What kind of societal value can we reap by making making all of our broadcast history as readily accessible as books, journal articles, and newspapers are now? How can the W3C help with the development of appropriate standards for video description and search? Answers to and debate about these questions will be the focus of this panel, which has representatives from some of the largest, most viewed, and carefully indexed collections of digital video available online.
12618 en Machine learning for cognitive science 1: What is machine learning? 
12619 en Cognitive science for machine learning 1:What is cognitive science? 
12620 en Machine learning for cognitive science 2: Bayesian methods and statistical learning theory 
12621 en Cognitive science for machine learning 2: Empirical methods 
12622 en Machine learning for cognitive science 3: Kernel methods and Bayesian methods 
12623 en Cognitive science for machine learning 3: Models and theories in cognitive science 
12626 en Bayesian modeling of action and perception and some other stuff 
12629 en Language acquisition and Kolmogorov complexity: Why is language acquisition possible 
12631 en Machine learning and the cognitive science of natural language 
12633 en Neuroscience, cognitive science and machine learning 
12634 en How could networks of neurons learn to carry out probabilistic inference? 
12636 en Reinforcement learning: Tutorial + Rethinking State, Action & Reward 
12719 en Building the Knowledge Base for Environmental Governance - EU Perspectives 
12720 en Teaching Climate Change and the UN System in Southeast Europe 
12721 en The Role of UNESCO in Climate Change Education and Education for Sustainable Development in Southeast Europe 
12722 en Regional Coordination of Nationale Climate Change Strategies and the Role of Human/Scientific Capacity Building 
12723 en Climate Change Capacity Building of United Nations Environment Programme 
12725 en Teaching Climate Change: the multiple relationships between climate change related education, climate (un)sustainability, business sector and civil society 
12726 en Innovative Diplomatic Training on Climate Change - Harnessing the Potentials of Modern Information and Communication Technologies 
12727 en UNITAR Approach to Capacity Building and Development for Climate Change 
12728 en Innovative Funding of Scientific Capacity Building on Climate Change - the IPCC Scholarship Programme 
12729 en UN Studies Portal and Preparation of a SEE specific Subportal 
12730 en Practical Experience with Regional Climate Change related Capacity Building in CEE and SEE 
12731 en Belgrade Initiative on Climate Change for Scientific Capacity Building and Policy Development in the SEE Region 
12732 en Building the Knowledge base for Climate Change Governance in the SEE 
12733 en How to achieve Interministerial and International Cooperation related to Climate Change Capacity Building in SEE 
12736 en International and Crosssectoral Cooperation of Climate Change Scientists:Slovenian Experience and Issues for SEE region 
12737 en Graz Climate Change research in SEE networking expiriences and outlook 
12738 en Teaching Climate Change: The Role of Tertiary Education Institutions and Barriers to Effective Curriculum Transformation 
12739 en Climate Change and United Nations Studies in the Context of SEE Regional Inter-University Co-operation 
12741 en Global Challenges and the Need for International Cooperation in SEE 
12742 en Global Climate Change Situation Room-an Innovative Way of Global Knowledge Collaboration zdruzi z videom ki pride
12745 en Technology Park Ljubljana and Hi-Tech SME development in Slovenia The current situation and perspectives for new High-tech SMEs are presented. The basic steps needed for the commercialization of an idea are listed, emphasizing the concrete procedures and experiences from the Technological park Ljubljana.
12746 en From Wireless Sensor Networks to Internet of Things and Future Internet In this presentation, first an overview of the main PROSENSE project goals and achievements is presented. The main wireless sensor networks topics as well as the application addressed are described. Building on this, a view on the further technological development of the domain towards Internet of Things and Future Internet is given. Finally, the main research topics, visions and activities on the Europe level in the Internet of Things domain are overviewed.
12747 en Smart Grids and WSN in Slovenia New concepts of smart electricity networks - SmartGrids represent the (r)evolution in the production and efficient use of electricity. On the one hand, the developed system-oriented solutions will allow high penetration of distributed energy resources and, on the other, consumer-oriented solutions will allow the efficient use of energy for end-users.
12748 en Antennas and electromagnetic simulators for demanding wireless applications in complex environments Antennas for wireless sensor applications need to be extremely versatile: small, adaptive, low-cost, mobile, “smart”. Efficient and versatile software tools for antenna analysis and simulation are presented. Modeling approaches for electromagnetic radiation in layered structures or in biological tissues are introduced with practical realizations of adequate antennas. Some antenna applications in wireless sensor networks (WSN) on/in a human or animal body are also addressed.
12749 en Sensor networks in telemedicine and telecare The development of information technology and telecommunications has reached a level where its usefulness can be applied for health care needs. Telemedicine and telecare rely on two extensive and interconnected professional fiels: Information and Communication Technology (ICT) and medicine. An approach for use of sensors for data capture, along with telecommunication connections and program tools harmonized with the biomedical profession and the organization of health care activities will be proposed.
12750 en Applying integrated sensor networks in public distribution systems Experiences with integration of various WSNs in a single global sensor network, with possibilities for alerting and subscribing sensor data to interested users, could be efficiently used in public distribution systems. In today’s public distribution systems, regardless it is a water, gas, or public heat distribution, there are large number of installed sensor nodes, with accompanying software for monitoring and alerting, provided usually from a single vendor, making the network very homogeneous. Our solution tries to provide more flexible system with integration of various installed sensors from different vendors, which enable scalability of the system, with facilitated installation of new sensors in the future. In the same time, the unified functions of alerting, subscribing and monitoring are still preserved.
12751 en Enhancing fitness with WSN systems: From the laboratory to the market Personal WSN systems have to be targeted to individual users. The system can offered on the market as a package of the necessary devices and software for monitoring. Another approach would be to target groups of users, providing them with the entire exercise environments and competitive exercising. Finally, with the Smart Running Track (SRT) equipment already in use and collected sensor data a digital image of the person’s level of fitness can be maintained. Future Internet technology can be used to provide various services to the users of WNS supported fitness systems.
12752 en Versatile Sensor Node - A Platform for the Sensor as a Service Concept Versatile Sensor Node (VSN) is a high performance sensor network platform with modular structure, long-life autonomy and flexible radio. Wireless interface spans over several industrial, scientific and medical frequency bands and supports multiple communication technologies, including ZigBee, 6LoWPAN, Bluetooth and WiFi. Various sensors and actuators can be connected via digital and analog peripherals, which makes VSN adaptable to diverse application requirements. By supporting semantic technologies and intelligent machine learning algorithms it provides a transparent infrastructure in which sensors are offered as a service.nnn
12753 en APSIS - Autonomous surface vehicle for measurements and logistic A miniature boat is designed to perform hydrographic surveys, on-line measurements and sample collection in shallow waters. The boat is light-weight, measuring 220 x 65 x 40 cm and is easy to handle by two persons.
12754 en DIONIS: Efficient monitoring system for vineyards A novel vineyard monitoring WSN based application currently under development as a joint collaboration between FEEIT – Skopje and ECS – Skopje is presented. The application, named DIONIS, incorporates various sensors (temperature, humidity and chemical substances sensors) for data gathering, wireless communication devices for data dissemination to relevant entities, database for storing the acquired data and a Graphical User Interface (GUI) for user friendly data statistics monitoring. Based on the extracted field parameters, DIONIS is able to detect abnormal situations and alarm local and remote authorities. DIONIS can enhance the grape growing process and provide higher wine quality, as well as exhibit water savings and more efficient pesticide use.
12755 en Power and Energy Management with Energy Control Modules Future households' requirements for demand-side management are auto demand response (AutoDR) function and full control over distributed power sources. Existing solutions feature a direct link between power meters and appliances. A new concept called Smart Distribution Box (SmartDB) provides complete energy and power management solution. It represents an intermediate layer, extending smart grid power meter functionality to support AutoDR with fast and guaranteed response times, distributed power sources, and full control over energy management with extra safety functions. Demo implementation of Smart Distribution Box is composed of Energy Control modules and management applications running on mobile phones and Linux/Windows systems.
12756 en Application of biosensors in sport and medicine - dynamics of muscles A rapid development of low-power electronic biosensors is enabling a production of devices that are small enough not to constrain the measured subject thus providing the scientists with a whole new insight in both dynamical and electrical properties of muscles. The usage of biosensors can be applied in medicine, sport, industry or any field where work output quality depends on any kind of muscle activity. Knowledge obtained through biosensors will lead in development both in medicine, sport and other related fields.
12757 en Harvesting Wireless Sensor Solutions and Networks by EnOcean technology Most wireless sensors, which are connected standalone or into a network are battery-driven. In the last time new batteryless and wireless sensors are in use, based on new approaches with extremely small amount of energy for sending information that even harvesting from environment energy can be sufficient. The environment energy can be present in mechanical thermal or photonic form. Sensors, which are harvesting environment energy are result of EnOcean technology that is spreading very successfully for example in the field of electrical installations and industry.
12758 en Wireless sensor network measurements of high density wind dynamics in the region Vipavska dolina, Slovenia with embedded real-time algorithm for prediction of the wind conditions We will demonstrate case of intelligent wireless sensor network algorithm for prediction of wind conditions in the region of Vipavska dolina. The system was developed for traffic security on the highway during the periods of evolution of high density wind energy. Detection wireless sensor network communicate with host super computing system for real-time computation of fluid dynamics based on boundary condition from discrete nodes and GPU technology. We apply self organized neural network dynamics of attractor space on sensor nodes and fuzzy network prediction for detection the threshold for traffic security.
12759 en Examples of WSN applications through EU-funded projects In this talk an overview is given of different applications of wireless sensor networks developed in the framework of recent EU founded projects and cooperating industrial partners. These "success stories" illustrate how WSN can lead to the development of new services.
12854 en Hands-on 1: Working with an existing LarKC workflow 
12855 en Hands-on 2: Building a LarKC decider plug-in to create a workflow from existing plug-ins 
12856 en Introduction to Distributed Processing in LarKC 
12857 en Hands-on 3: Building a LarKC plug-in and integrating it into an existing workflow 
12858 en Hands-on 4: Understanding and Manipulating the Urban Computing workflow 
12860 en Real-time processing on intensive web streams 
12861 en Logic-based ad-hoc business process management: Concepts and challenges 
12862 en From Web 2.0 to Web 3.0 using Data Mining Web 2.0 applications such as Flickr offer a rich set of data with a huge potential for exploitation by the human users.nUnfortunately, the sifting through such data is far from easy and rewarding due to a lack of semantics on the one side and anlack of rich data description on the other side. For instance, most photos on Flickr have very little description attached that could be used for retrieving or exploring the photos. In this talk, we demonstrate how the enrichment of Web 2.0 data by automatically discovered (more or less) semantic relationships improves the user experience.nn
12863 en An XML Schema and a Topic Map Ontology for Formalization of Background Knowledge in Data Mining Background (or sometimes referred to as domain) knowledge is extensively used in data mining for data pre-processing and fornnugget-oriented data mining tasks: it is essential for constraining thensearch space and pruning the results. Despite the costs of eliciting background knowledge from domain experts, there has been so far little effortnto devise a common exchange standard for its representation. This paper proposes the Background Knowledge Exchange Format (BKEF), anlightweight XML Schema for storing information on features and patterns, and the Background Knowledge Ontology (BKOn), as its semantic abstraction. The purpose of BKOn is to allow reasoning over andnintegration of analysed data with existing domain ontologies. We shownan elicitation interface producing BKEF and discuss the possibilities fornintegration of such background knowledge with domain ontologies.
12864 en Importing Knowledge Fragments to CMS-Enabled Data Mining Analytical Reports Descriptive data mining only brings its fruits when the results are providednto the end user in a palatable form. The vehicle for end-user delivery of miningnresults (and associated information such as data schema, task settings, and domain background knowledge) are so-called analytical reports. In order to managena huge number of reports referring to different mining sessions, we designed andata mining web portal based on a content management system, together callednSEWEBAR-CMS.1 One of the requirements on the CMS was the ability to interact with semantic knowledge sources and other structured data, see [1].nThe data analyst who authors an analytical report in the CMS has differentnpossibilities of (semi-)automatically entering structured data into the text.nFirst, for locally stored data such as mining task/result/data descriptionsnexported from mining tools in PMML (Predictive Model Mark-Up Language), anCMS plugin can pick marked segments of HTML code, produced from PMMLnusing XSLT, and insert them into the report as indicated by the analyst.nSecond, sophisticated support for remote data/knowledge has been newlynadded. The infrastructure for this functionality allows to persistently specifyn– Links to queriable resourcesn– Template queries for these resources (which can be paramatrized by thenend-user at runtime)n– XSLT transformations allowing to insert the results of queries as HTMLnfragments, either static or dynamically updated from the resources.nCurrently we experiment with queriable resources in the form of native XMLndatabase (Berkeley, queried via XQuery), which stores PMML data, and semantic knowledge bases both in the form of SPARQL endpoint and Ontopia Knowledge Suite (a Topic Maps tool, queried via a Prolog-like language called tolog).nInclusion of further types of resources such as Lucene indices is in progress.n
12865 en Towards a semantic foundation for bioinformatics With a two and half thousand year tradition logic is the best understood way ofnrepresenting scientific knowledge. Only logic provides the semantic clarity necessarynto ensure the comprehensibility, reproducibility, and free exchange of knowledge.nThe use of logic is also necessary to enable computers to play a full part in sciencen[1]. The semantic web is transforming the dissemination of science by making for thenfirst time making a large amount of scientific knowledge available expressed in logic.nBioinformatics is one of the undoubted successes stories of the semantic web, withnbioinformatic knowledge making up a large percentage of the scientific semantic web.nMany of the problems that make semantic web reasoning difficult don't apply tonbioinformatics: a ground truth of scientific knowledge exists, top level ontologiesnhave been agreed (BFO), many other ontological standards exist, and thenbioinformatic semantic web is large but not too large.nThe use of bioinformatic software is essential to modern biology. However, therenis a clear mismatch between the increasing use of the semantic web and logic, and thenway bioinformatic systems utlilise and make inferences with this knowledge. This isnbecause almost all computer based bioinformatic reasoning is done using ad hocnprograms. From a formal point of view these programs are invariably making logicalninferences: deductions, abductions, inductions, with perhaps a probabilistic element.nHowever, what exactly these inferences exactly are is generally unclear.nThe aim of my research is to make these inferences clear and to express them innlogic, and make them executable across the semantic web.n
12868 en From Disasters to WoW: Using Web Science to understand and enable 21st century multidimensional networks Recent advances in Web Science provide comprehensive digital traces of social actions, interactions, and transactions. These data provide an unprecedented exploratorium to model the socio-technical motivations for creating, maintaining, dissolving, and reconstituting multidimensional social networks. Multidimensional networks include multiple types of nodes (people, documents, datasets, tags, etc.) and multiple types of relationships (co-authorship, citation, web links, etc). Using examples from research in a wide range of activities such as disaster response, public health and massively multiplayer online games (WoW - the World of Warcraft), Contractor will argue that Web Science serves as the foundation for the development of theories and methods to help advance our ability to understand and enable multidimensional networks.
12869 en Linked Data: Now what? Since the Linked Data principles were first outlined we have witnessed an outstanding growth and heterogeneity of linked data publicly available on the Web, which has even managed to trigger the interest of large companies and governmental bodies world-wide. In the light of this evolution and given the current take up and expectations raised, we believe it is now time to reflect about the progress thus far and plan for the future challenges and opportunities that the Web of Data will bring to us. In this panel we shall discuss what has worked and what has not so far, and we shall try to identify what we should do and what we should not do in the future.
12870 en The Semantic Product Memory: An Interactive Black Box for Smart Objects A semantic product memory stores a diary of an individual physical object in a persistent way on an embedded sensor system that is networked by wireless communication to a smart environment. The product monitors itself and its environment. Semantic technologies based on OWL ontologies guarantee interoperability of the product memory across the complete supply chain and lifecycle of smart objects and enable end user access to the product’s lifelog.nIn this talk, we present the layered architecture together with the representation and inference formalisms used in our SemProM project, funded by the German Ministry of Education and Research (BMBF) with 16 Million Euro. SemProM goes well beyond traditional RFID technology and is the basis for intelligent automation in smart factories, event-driven logistics as well as smart retail and after-sales. Collecting information logs about objects in such smart environments and making it available - for example about an object’s origin, location, movements, physical properties, environmental conditions, usage history, as well as warranty and maintenance data - can help enterprises to improve their business processes and create new ones. Existing business process models become more accurate since information taken directly from the point of action can be used to manage or adapt processes in real time for the emerging Internet of Things.nWe show how such embedded “black box” event recorders can transform everyday objects like cars, circuit boards, pizzas and drug blister packs into smart products. We show how consumers of smart products can access the lifelogs of products by NFC-enabled smartphones using SemProM’s browser and track the complete history of a product in multimodal dialogues. A role-based access control mechanism ensures privacy and security of the SemProM product memories. We will discuss fully operational pilot implementations of semantic product memories developed in the SemProM consortium together with major German companies like SAP, BMW, Siemens, DHL, Globus Retail and Kohl Pharma.
12871 en A Pattern Science for the Semantic Web I will present the current state of play with respect to collecting, finding, classifying, and using (design) patterns on the semantic web. The talk will compare historically related work, highlight some successful stories and curious unexpected bottlenecks, and will envision some charming research and development directions from the Web of Data.
12872 en SKOS: Past, Present, Future - and a little bit of history, architecture and engineering SKOS (Simple Knowledge Organisation System) is a common data model for sharing and linking knowledge organization systems via the Web. Many knowledge organization systems, such as thesauri, taxonomies, classification schemes and subject heading systems, share a similar structure, and are used in similar applications. SKOS captures much of this similarity and makes it explicit, enabling data and technology sharing across diverse applications.nThe SKOS data model provides a standard, low-cost migration path for porting existing knowledge organization systems to the Semantic Web. SKOS also provides a light weight, intuitive language for developing and sharing new knowledge organization systems. It may be used on its own, or in combination with formal knowledge representation languages such as the Web Ontology language (OWL). SKOS was published as a W3C Recommendation in August 2009 and is seeing growing take-up in a number of fields including (among others) cultural heritage, economics, astronomy, and local government. SKOS also looks set to play a key role in providing vocabularies for the Data Web through its use in Open Linked Data.
12984 en Verbmobile - A machine translation story 
13008 en Object-Graphs for Context-Aware Category Discovery How can knowing about some categories help us to dis-ncover new ones in unlabeled images? Unsupervised visualncategory discovery is useful to mine for recurring objectsnwithout human supervision, but existing methods assumenno prior information and thus tend to perform poorly forncluttered scenes with multiple objects. We propose to lever-nage knowledge about previously learned categories to en-nable more accurate discovery. We introduce a novel object-ngraph descriptor to encode the layout of object-level co-noccurrence patterns relative to an unfamiliar region, andnshow that by using it to model the interaction betweennan image’s known and unknown objects we can better de-ntect new visual categories. Rather than mine for all cat-negories from scratch, our method identifies new objectsnwhile drawing on useful cues from familiar ones. We eval-nuate our approach on benchmark datasets and demonstratenclear improvements in discovery over conventional purelynappearance-based baselines.
13009 en Grouplet: A Structured Image Representation for Recognizing Human and Object Interactions Psychologists have proposed that many human-object interactionnactivities form unique classes of scenes. Recognizingnthese scenes is important for many social functions. Tonenable a computer to do this is however a challenging task.nTake people-playing-musical-instrument (PPMI) as an example;nto distinguish a person playing violin from a personnjust holding a violin requires subtle distinction of characteristicnimage features and feature arrangements that differentiatenthese two scenes. Most of the existing image representationnmethods are either too coarse (e.g. BoW) orntoo sparse (e.g. constellation models) for performing thisntask. In this paper, we propose a new image feature representationncalled “grouplet”. The grouplet captures thenstructured information of an image by encoding a numbernof discriminative visual features and their spatial configurations.nUsing a dataset of 7 different PPMI activities,nwe show that grouplets are more effective in classifying andndetecting human-object interactions than other state-of-theartnmethods. In particular, our method can make a robustndistinction between humans playing the instruments and humansnco-occurring with the instruments without playing.
13010 en Modeling Mutual Context of Object and Human Pose in Human-Object Interaction Activities Detecting objects in cluttered scenes and estimating articulatednhuman body parts are two challenging problems inncomputer vision. The difficulty is particularly pronouncednin activities involving human-object interactions (e.g. playingntennis), where the relevant object tends to be small ornonly partially visible, and the human body parts are oftennself-occluded. We observe, however, that objects and humannposes can serve as mutual context to each other – recognizingnone facilitates the recognition of the other. In this papernwe propose a new random field model to encode the mutualncontext of objects and human poses in human-object interactionnactivities. We then cast the model learning task as anstructure learning problem, of which the structural connectivitynbetween the object, the overall human pose, and differentnbody parts are estimated through a structure searchnapproach, and the parameters of the model are estimatednby a new max-margin algorithm. On a sports data set of sixnclasses of human-object interactions [12], we show that ournmutual context model significantly outperforms state-of-theartnin detecting very difficult objects and human poses.
13011 en The chains model for detecting parts by their context Detecting an object part relies on two sources of informationn- the appearance of the part itself, and the contextnsupplied by surrounding parts. In this paper we considernproblems in which a target part cannot be recognized reliablynusing its own appearance, such as detecting lowresolutionnhands, and must be recognized using the contextnof surrounding parts. We develop the ‘chains model’nwhich can locate parts of interest in a robust and precisenmanner, even when the surrounding context is highly variablenand deformable. In the proposed model, the relationnbetween context features and the target part is modeled in annon-parametric manner using an ensemble of feature chainsnleading from parts in the context to the detection target. Thenmethod uses the configuration of the features in the imagendirectly rather than through fitting an articulated 3-D modelnof the object. In addition, the chains are composable, meaningnthat new chains observed in the test image can be composednof sub-chains seen during training. Consequently,nthe model is capable of handling object poses which areninfrequent, even non-existent, during training. We test thenapproach in different settings, including object parts detection,nas well as complete object detection. The results shownthe advantages of the chains model for detecting and localizingnparts of complex deformable objects.
13013 en Multimodal semi-supervised learning for image classification In image categorization the goal is to decide if an imagenbelongs to a certain category or not. A binary classifier cannbe learned from manually labeled images; while using morenlabeled examples improves performance, obtaining the imagenlabels is a time consuming process.nWe are interested in how other sources of informationncan aid the learning process given a fixed amount of labelednimages. In particular, we consider a scenario wherenkeywords are associated with the training images, e.g. asnfound on photo sharing websites. The goal is to learn anclassifier for images alone, but we will use the keywordsnassociated with labeled and unlabeled images to improventhe classifier using semi-supervised learning. We first learnna strong Multiple Kernel Learning (MKL) classifier usingnboth the image content and keywords, and use it to scorenunlabeled images. We then learn classifiers on visual featuresnonly, either support vector machines (SVM) or leastsquaresnregression (LSR), from the MKL output values onnboth the labeled and unlabeled images.nIn our experiments on 20 classes from the PASCALnVOC’07 set and 38 from the MIR Flickr set, we demonstratenthe benefit of our semi-supervised approach over only usingnthe labeled images. We also present results for a scenarionwhere we do not use any manual labeling but directly learnnclassifiers from the image tags. The semi-supervised approachnalso improves classification accuracy in this case.
13014 en What Helps Where - And Why? Semantic Relatedness for Knowledge Transfer Remarkable performance has been reported to recognizensingle object classes. Scalability to large numbers of classesnhowever remains an important challenge for today’s recognitionnmethods. Several authors have promoted knowledgentransfer between classes as a key ingredient to address thisnchallenge. However, in previous work the decision whichnknowledge to transfer has required either manual supervisionnor at least a few training examples limiting the scalabilitynof these approaches. In this work we explicitly addressnthe question of how to automatically decide which informationnto transfer between classes without the need of any humannintervention. For this we tap into linguistic knowledgenbases to provide the semantic link between sources (what)nand targets (where) of knowledge transfer. We provide a rigorousnexperimental evaluation of different knowledge basesnand state-of-the-art techniques from Natural Language Processingnwhich goes far beyond the limited use of languagenin related work. We also give insights into the applicabilityn(why) of different knowledge sources and similarity measuresnfor knowledge transfer.
13015 en Towards Semantic Embedding in Visual Vocabulary Visual vocabulary serves as a fundamental componentnin many computer vision tasks, such as object recognition,nvisual search, and scene modeling. While state-of-the-artnapproaches build visual vocabulary based solely on visualnstatistics of local image patches, the correlative image labelsnare left unexploited in generating visual words. In thisnwork, we present a semantic embedding framework to integratensemantic information from Flickr labels for supervisednvocabulary construction. Our main contribution is anHidden Markov Random Field modeling to supervise featurenspace quantization, with specialized considerations tonlabel correlations: Local visual features are modeled asnan Observed Field, which follows visual metrics to partitionnfeature space. Semantic labels are modeled as a HiddennField, which imposes generative supervision to the ObservednField with WordNet-based correlation constraints asnGibbs distribution. By simplifying the Markov property innthe Hidden Field, both unsupervised and supervised (labelnindependent) vocabularies can be derived from our framework.nWe validate our performances in two challengingncomputer vision tasks with comparisons to state-of-the-arts:n(1) Large-scale image search on a Flickr 60,000 database;n(2) Object recognition on the PASCAL VOC database.
13017 en Common Visual Pattern Discovery via Spatially Coherent Correspondences 
13018 en Unsupervised Detection and Segmentation of Identical Objects We address an unsupervised object detection and segmentationnproblem that goes beyond the conventional assumptionsnof one-to-one object correspondences or modeltestnsettings between images. Our method can detect andnsegment identical objects directly from a single image or anhandful of images without any supervision. To detect andnsegment all the object-level correspondences from the givennimages, a novel multi-layer match-growing method is proposednthat starts from initial local feature matches and exploresnthe images by intra-layer expansion and inter-layernmerge. It estimates geometric relations between object entitiesnand establishes ‘object correspondence networks’ thatnconnect matching objects. Experiments demonstrate robustnperformance of our method on challenging datasets.
13019 en A Novel Riemannian Framework for Shape Analysis of 3D Objects In this paper we introduce a novel Riemannian framework for shape analysis of parameterized surfaces. Wenderive a distance function between any two surfaces thatnis invariant to rigid motion, global scaling, and reparametrization. It is the last part that presents the mainndifficulty. Our solution to this problem is twofold: (1) wendefine a special representation, called a q-map, to represent each surface, and (2) we develop a gradient-based algorithm to optimize over different re-parameterizations ofna surface. The second step is akin to deforming the meshnon a fixed surface to optimize its placement. (This is different from the current methods that treat the given meshes asnfixed.) Under the chosen representation, with the L2 metric, the action of the re-parametrization group is by isometries. This results in, to our knowledge, the first Riemannian distance between parameterized surfaces to have allnthe desired invariances. We demonstrate this frameworknwith several examples using some toy shapes, and real datanwith anatomical structures, and cropped facial surfaces. Wenalso successfully demonstrate clustering and classificationnof these objects under the proposed metric.
13020 en Global and Efficient Self-Similarity for Object Classification and Detection Self-similarity is an attractive image property which has recentlynfound its way into object recognition in the form ofnlocal self-similarity descriptors [5, 6, 14, 18, 23, 27] In thisnpaper we explore global self-similarity (GSS) and its advantagesnover local self-similarity (LSS). We make three contributions:n(a) we propose computationally efficient algorithmsnto extract GSS descriptors for classification. Thesencapture the spatial arrangements of self-similarities withinnthe entire image; (b) we show how to use these descriptorsnefficiently for detection in a sliding-window frameworknand in a branch-and-bound framework; (c) we experimentallyndemonstrate on Pascal VOC 2007 and on ETHZ ShapenClasses that GSS outperforms LSS for both classificationnand detection, and that GSS descriptors are complementarynto conventional descriptors such as gradients or color.
13022 en Object Recognition by Discriminative Combinations of Line Segments and Ellipses 
13023 en On Detection of Multiple Object Instances using Hough Transforms To detect multiple objects of interest, the methods basednon Hough transform use non-maxima supression or modenseeking in order to locate and to distinguish peaks in Houghnimages. Such postprocessing requires tuning of extra parametersnand is often fragile, especially when objects of interestntend to be closely located. In the paper, we develop annew probabilistic framework that is in many ways related tonHough transform, sharing its simplicity and wide applicability.nAt the same time, the framework bypasses the problemnof multiple peaks identification in Hough images, andnpermits detection of multiple objects without invoking nonmaximumnsuppression heuristics. As a result, the experimentsndemonstrate a significant improvement in detectionnaccuracy both for the classical task of straight line detectionnand for a more modern category-level (pedestrian) detectionnproblem.
13024 en Cascade Object Detection with Deformable Part Models We describe a general method for building cascade classifiersnfrom part-based deformable models such as pictorialnstructures. We focus primarily on the case of star-structurednmodels and show how a simple algorithm based on partialnhypothesis pruning can speed up object detection bynmore than one order of magnitude without sacrificing detectionnaccuracy. In our algorithm, partial hypotheses arenpruned with a sequence of thresholds. In analogy to probablynapproximately correct (PAC) learning, we introduce thennotion of probably approximately admissible (PAA) thresholds.nSuch thresholds provide theoretical guarantees on thenperformance of the cascade method and can be computednfrom a small sample of positive examples. Finally, we outlinena cascade detection algorithm for a general class ofnmodels defined by a grammar formalism. This class includesnnot only tree-structured pictorial structures but alsonricher models that can represent each part recursively as anmixture of other parts.
13025 en Food Recognition Using Statistics of Pairwise Local Features Food recognition is difficult because food items are deformablenobjects that exhibit significant variations in appearance.nWe believe the key to recognizing food is to exploitnthe spatial relationships between different ingredientsn(such as meat and bread in a sandwich). We propose annew representation for food items that calculates pairwisenstatistics between local features computed over a soft pixellevelnsegmentation of the image into eight ingredient types.nWe accumulate these statistics in a multi-dimensional histogram,nwhich is then used as a feature vector for a discriminativenclassifier. Our experiments show that the proposednrepresentation is significantly more accurate at identifyingnfood than existing methods.
13027 en Proximate Sensing: Inferring What-Is-Where From Georeferenced Photo Collections 
13028 en Detecting Text in Natural Scenes with Stroke Width Transform We present a novel image operator that seeks to find the valuenof stroke width for each image pixel, and demonstrate its use onnthe task of text detection in natural images. The suggestednoperator is local and data dependent, which makes it fast andnrobust enough to eliminate the need for multi-scale computationnor scanning windows. Extensive testing shows that the suggestednscheme outperforms the latest published algorithms. Itsnsimplicity allows the algorithm to detect texts in many fonts andnlanguages.
13029 en Reading Between The Lines: Object Localization Using Implicit Cues from Image Tags Current uses of tagged images typically exploit onlynthe most explicit information: the link between the nounsnnamed and the objects present somewhere in the image. Wenpropose to leverage “unspoken” cues that rest within annordered list of image tags so as to improve object localization.nWe define three novel implicit features from an image’sntags—the relative prominence of each object as signifiednby its order of mention, the scale constraints impliednby unnamed objects, and the loose spatial links hinted bynthe proximity of names on the list. By learning a conditionalndensity over the localization parameters (positionnand scale) given these cues, we show how to improve bothnaccuracy and efficiency when detecting the tagged objects.nWe validate our approach with 25 object categories fromnthe PASCAL VOC and LabelMe datasets, and demonstratenits effectiveness relative to both traditional sliding windowsnas well as a visual context baseline.
13030 en Beyond Active Noun Tagging: Modeling Contextual Interactions for Multi-Class Active Learning We present an active learning framework to simultaneouslynlearn appearance and contextual models for scene understandingntasks (multi-class classification). Existing multi-class active learningnapproaches have focused on utilizing classification uncertaintynof regions to select the most ambiguous region for labeling. Thesenapproaches, however, ignore the contextual interactions betweenndifferent regions of the image and the fact that knowing the labelnfor one region provides information about the labels of othernregions. For example, the knowledge of a region being sea is informativenabout regions satisfying the “on” relationship with respectnto it, since they are highly likely to be boats. We explicitly modelnthe contextual interactions between regions and select the questionnwhich leads to the maximum reduction in the combined entropy ofnall the regions in the image (image entropy). We also introduce annew methodology of posing labeling questions, mimicking the waynhumans actively learn about their environment. In these questions,nwe utilize the regions linked to a concept with high confidence asnanchors, to pose questions about the uncertain regions. For example,nif we can recognize water in an image then we can use the regionnassociated with water as an anchor to pose questions such asn“what is above water?”. Our active learning framework also introducesnquestions which help in actively learning contextual concepts.nFor example, our approach asks the annotator: “What isnthe relationship between boat and water?” and utilizes the answernto reduce the image entropies throughout the training dataset andnobtain more relevant training examples for appearance models.
13032 en Rectilinear Parsing of Architecture in Urban Environment 
13033 en Hybrid Multi-view Reconstruction by Jump-Diffusion We propose a multi-view stereo reconstruction algorithmnwhich recovers urban scenes as a combination of meshesnand geometric primitives. It provides a compact modelnwhile preserving details: irregular elements such as statuesnand ornaments are described by meshes whereas regularnstructures such as columns and walls are described by primitivesn(planes, spheres, cylinders, cones and tori). A Jump-nDiffusion process is designed to sample these two types ofnelements simultaneously. The quality of a reconstructionnis measured by a multi-object energy model which takesninto account both photo-consistency and semantic considerationsn(i.e. geometry and shape layout). The sampler isnembedded into an iterative refinement procedure which providesnan increasingly accurate hybrid representation. Experimentalnresults on complex urban structures and largenscenes are presented and compared to multi-view basednmeshing algorithms.
13034 en Building Reconstruction using Manhattan-World Grammars We present a passive computer vision method thatnexploits existing mapping and navigation databases innorder to automatically create 3D building models. Ournmethod defines a grammar for representing changes innbuilding geometry that approximately follow thenManhattan-world assumption which states there is anpredominance of three mutually orthogonal directions innthe scene. By using multiple calibrated aerial images, wenextend previous Manhattan-world methods to robustlynproduce a single, coherent, complete geometric model of anbuilding with partial textures. Our method uses annoptimization to discover a 3D building geometry thatnproduces the same set of façade orientation changesnobserved in the captured images. We have applied ournmethod to several real-world buildings and have analyzednour approach using synthetic buildings.
13035 en Estimating Camera Pose from a Single Urban Ground-View Omnidirectional Image and a 2D Building Outline Map A framework is presented for estimating the pose of ancamera based on images extracted from a single omnidirectionalnimage of an urban scene, given a 2D map withnbuilding outlines with no 3D geometric information nor appearancendata. The framework attempts to identify verticalncorner edges of buildings in the query image, whichnwe term VCLH, as well as the neighboring plane normals,nthrough vanishing point analysis. A bottom-up process furtherngroups VCLH into elemental planes and subsequentlyninto 3D structural fragments modulo a similarity transformation.nA geometric hashing lookup allows us to rapidlynestablish multiple candidate correspondences between thenstructural fragments and the 2D map building contours. Anvoting-based camera pose estimation method is then employednto recover the correspondences admitting a cameranpose solution with high consensus. In a dataset that is evennchallenging for humans, the system returned a top-30 rankingnfor correct matches out of 3600 camera pose hypothesesn(0.83% selectivity) for 50.9% of queries.
13037 en Monocular 3D Pose Estimation and Tracking by Detection Automatic recovery of 3D human pose from monocularnimage sequences is a challenging and important researchntopic with numerous applications. Although current methodsnare able to recover 3D pose for a single person in controllednenvironments, they are severely challenged by realworldnscenarios, such as crowded street scenes. To addressnthis problem, we propose a three-stage process building onna number of recent advances. The first stage obtains an initialnestimate of the 2D articulation and viewpoint of the personnfrom single frames. The second stage allows early datanassociation across frames based on tracking-by-detection.nThese two stages successfully accumulate the available 2Dnimage evidence into robust estimates of 2D limb positionsnover short image sequences (= tracklets). The third andnfinal stage uses those tracklet-based estimates as robust imagenobservations to reliably recover 3D pose. We demonstratenstate-of-the-art performance on the HumanEva IInbenchmark, and also show the applicability of our approachnto articulated 3D tracking in realistic street conditions.
13038 en Dynamical Binary Latent Variable Models for 3D Human Pose Tracking We introduce a new class of probabilistic latent variablenmodel called the Implicit Mixture of Conditional RestrictednBoltzmann Machines (imCRBM) for use in humannpose tracking. Key properties of the imCRBM are as follows:n(1) learning is linear in the number of training exemplarsnso it can be learned from large datasets; (2) it learnsncoherent models of multiple activities; (3) it automaticallyndiscovers atomic “movemes”; and (4) it can infer transitionsnbetween activities, even when such transitions are notnpresent in the training set. We describe the model and hownit is learned and we demonstrate its use in the context ofnBayesian filtering for multi-view and monocular pose tracking.nThe model handles difficult scenarios including multiplenactivities and transitions among activities. We reportnstate-of-the-art results on the HumanEva dataset.
13039 en Contour People: A Parameterized Model of 2D Articulated Human Shape We define a new “contour person” model of the humannbody that has the expressive power of a detailed 3D modelnand the computational benefits of a simple 2D part-basednmodel. The contour person (CP) model is learned from an3D SCAPE model of the human body that captures naturalnshape and pose variations; the projected contours of thisnmodel, along with their segmentation into parts forms thentraining set. The CP model factors deformations of the bodyninto three components: shape variation, viewpoint changenand part rotation. This latter model also incorporates anlearned non-rigid deformation model. The result is a 2Dnarticulated model that is compact to represent, simple toncompute with and more expressive than previous models.nWe demonstrate the value of such a model in 2D pose es-ntimation and segmentation. Given an initial pose from anstandard pictorial-structures method, we refine the pose andnshape using an objective function that segments the sceneninto foreground and background regions. The result is anparametric, human-specific, image segmentation.
13040 en Combining Discriminative and Generative Methods for 3D Deformable Surface and Articulated Pose Reconstruction Historically non-rigid shape recovery and articulatednpose estimation have evolved as separate fields. Recentnmethods for non-rigid shape recovery have focused on improvingnthe algorithmic formulation, but have only considerednthe case of reconstruction from point-to-point correspondences.nIn contrast, many techniques for pose estimationnhave followed a discriminative approach, which allowsnfor the use of more general image cues. However, thesentechniques typically require large training sets and suffernfrom the fact that standard discriminative methods do notnenforce constraints between output dimensions. In this paper,nwe combine ideas from both domains and propose anunified framework for articulated pose estimation and 3Dnsurface reconstruction. We address some of the issues ofndiscriminative methods by explicitly constraining their prediction.nFurthermore, our formulation allows for the combinationnof generative and discriminative methods into ansingle, common framework.
13042 en Visual Tracking Decomposition We propose a novel tracking algorithm that can work robustly in a challenging scenario such that several kinds ofnappearance and motion changes of an object occur at thensame time. Our algorithm is based on a visual tracking decomposition scheme for the efficient design of observationnand motion models as well as trackers. In our scheme, thenobservation model is decomposed into multiple basic observation models that are constructed by sparse principalncomponent analysis (SPCA) of a set of feature templates.nEach basic observation model covers a specific appearancenof the object. The motion model is also represented by thencombination of multiple basic motion models, each of whichncovers a different type of motion. Then the multiple basic trackers are designed by associating the basic observation models and the basic motion models, so that each specific tracker takes charge of a certain change in the object.nAll basic trackers are then integrated into one compoundntracker through an interactive Markov Chain Monte Carlon(IMCMC) framework in which the basic trackers communicate with one another interactively while run in parallel.nBy exchanging information with others, each tracker further improves its performance, which results in increasingnthe whole performance of tracking. Experimental resultsnshow that our method tracks the object accurately and reliably in realistic videos where the appearance and motionnare drastically changing over time.
13043 en A Globally Optimal Data-Driven Approach for Image Distortion Estimation Image alignment in the presence of non-rigid distortionsnis a challenging task. Typically, this involves estimatingnthe parameters of a dense deformation field thatnwarps a distorted image back to its undistorted template.nGenerative approaches based on parameter optimizationnsuch as Lucas-Kanade can get trapped within local minima.nOn the other hand, discriminative approaches likenNearest-Neighbor require a large number of training samplesnthat grows exponentially with the desired accuracy. Innthis work, we develop a novel data-driven iterative algorithmnthat combines the best of both generative and discriminativenapproaches. For this, we introduce the notion ofna “pull-back” operation that enables us to predict the parametersnof the test image using training samples that arennot in its neighborhood (not ?-close) in parameter space.nWe prove that our algorithm converges to the global optimumnusing a significantly lower number of training samplesnthat grows only logarithmically with the desired accuracy.nWe analyze the behavior of our algorithm extensively usingnsynthetic data and demonstrate successful results on experimentsnwith complex deformations due to water and clothing.
13044 en Tracking the Invisible: Learning Where the Object Might Be Objects are usually embedded into context. Visual contextnhas been successfully used in object detection tasks,nhowever, it is often ignored in object tracking. We propose anmethod to learn supporters which are, be it only temporally,nuseful for determining the position of the object of interest.nOur approach exploits the General Hough Transformnstrategy. It couples the supporters with the target and naturallyndistinguishes between strongly and weakly couplednmotions. By this, the position of an object can be estimatedneven when it is not seen directly (e.g., fully occluded or outsidenof the image region) or when it changes its appearancenquickly and significantly. Experiments show substantial improvementsnin model-free tracking as well as in the trackingnof “virtual” points, e.g., in medical applications.
13045 en Motion Detail Preserving Optical Flow Estimation We discuss the cause of a severe optical flow estimation problem that fine motion structures cannot always bencorrectly reconstructed in the commonly employed multi-scale variational framework. Our major finding is that significant and abrupt displacement transition wrecks small-scale motion structures in the coarse-to-fine refinement. Annovel optical flow estimation method is proposed in thisnpaper to address this issue, which reduces the reliance ofnthe flow estimates on their initial values propagated fromnthe coarser level and enables recovering many motion details in each scale. The contribution of this paper also includes adaption of the objective function and developmentnof a new optimization procedure. The effectiveness of ournmethod is borne out by experiments for both large- andnsmall-displacement optical flow estimation.
13047 en What's going on? Discovering Spatio-Temporal Dependencies in Dynamic Scenes We present two novel methods to automatically learnnspatio-temporal dependencies of moving agents in complexndynamic scenes. They allow to discover temporal rules,nsuch as the right of way between different lanes or typicalntraffic light sequences. To extract them, sequences ofnactivities need to be learned. While the first method extractsnrules based on a learned topic model, the secondnmodel called DDP-HMM jointly learns co-occurring activitiesnand their time dependencies. To this end we employ DependentnDirichlet Processes to learn an arbitrary numbernof infinite Hidden Markov Models. In contrast to previousnwork, we build on state-of-the-art topic models that allownto automatically infer all parameters such as the optimalnnumber of HMMs necessary to explain the rules governingna scene. The models are trained offline by Gibbs Samplingnusing unlabeled training data.
13048 en Visual Event Recognition in Videos by Learning from Web Data We propose a visual event recognition framework fornconsumer domain videos by leveraging a large amount ofnloosely labeled web videos (e.g., from YouTube). First,nwe propose a new aligned space-time pyramid matchingnmethod to measure the distances between two video clips,nwhere each video clip is divided into space-time volumesnover multiple levels. We calculate the pair-wise distancesnbetween any two volumes and further integrate the informationnfrom different volumes with Integer-flow Earth Mover’snDistance (EMD) to explicitly align the volumes. Second,nwe propose a new cross-domain learning method in ordernto 1) fuse the information from multiple pyramid levels andnfeatures (i.e., space-time feature and static SIFT feature)nand 2) cope with the considerable variation in feature distributionsnbetween videos from two domains (i.e., web domainnand consumer domain). For each pyramid level andneach type of local features, we train a set of SVM classifiersnbased on the combined training set from two domainsnusing multiple base kernels of different kernel types andnparameters, which are fused with equal weights to obtainnan average classifier. Finally, we propose a cross-domainnlearning method, referred to as Adaptive Multiple KernelnLearning (A-MKL), to learn an adapted classifier based onnmultiple base kernels and the prelearned average classifiersnby minimizing both the structural risk functional andnthe mismatch between data distributions from two domains.nExtensive experiments demonstrate the effectiveness of ournproposed framework that requires only a small number ofnlabeled consumer videos by leveraging web data.
13050 en Anomaly Detection in Crowded Scenes A novel framework for anomaly detection in crowdednscenes is presented. Three properties are identified as importantnfor the design of a localized video representationnsuitable for anomaly detection in such scenes: 1) joint modelingnof appearance and dynamics of the scene, and thenabilities to detect 2) temporal, and 3) spatial abnormalities.nThe model for normal crowd behavior is based on mixturesnof dynamic textures and outliers under this model arenlabeled as anomalies. Temporal anomalies are equated tonevents of low-probability, while spatial anomalies are handlednusing discriminant saliency. An experimental evaluationnis conducted with a new dataset of crowded scenes,ncomposed of 100 video sequences and five well defined abnormalityncategories. The proposed representation is shownnto outperform various state of the art anomaly detectionntechniques.
13052 en Face Recognition Based on Image Sets We introduce a novel method for face recognition fromnimage sets. In our setting each test and training examplenis a set of images of an individual’s face, not just a singlenimage, so recognition decisions need to be based on comparisonsnof image sets. Methods for this have two mainnaspects: the models used to represent the individual imagensets; and the similarity metric used to compare the models.nHere, we represent images as points in a linear ornaffine feature space and characterize each image set by anconvex geometric region (the affine or convex hull) spannednby its feature points. Set dissimilarity is measured by geometricndistances (distances of closest approach) betweennconvex models. To reduce the influence of outliers we usenrobust methods to discard input points that are far from thenfitted model. The kernel trick allows the approach to be extendednto implicit feature mappings, thus handling complexnand nonlinear manifolds of face images. Experiments onntwo public face datasets show that our proposed methodsnoutperform a number of existing state-of-the-art ones.
13054 en 3D Morphable Model Construction for Robust Ear and Face Recognition Recent work suggests that the human ear varies significantlynbetween different subjects and can be used for identification.nIn principle, therefore, using ears in addition tonthe face within a recognition system could improve accuracynand robustness, particularly for non-frontal views. Thenpaper describes work that investigates this hypothesis usingnan approach based on the construction of a 3D morphablenmodel of the head and ear. One issue with creating a modelnthat includes the ear is that existing training datasets containnnoise and partial occlusion. Rather than exclude thesenregions manually, a classifier has been developed which automatesnthis process. When combined with a robust registrationnalgorithm the resulting system enables full headnmorphable models to be constructed efficiently using lessnconstrained datasets. The algorithm has been evaluated usingnregistration consistency, model coverage and minimalismnmetrics, which together demonstrate the accuracy of thenapproach. To make it easier to build on this work, the sourcencode has been made available online.
13056 en Interest Seam Image We propose interest seam image, an efficient visual synopsisnfor video. To extract an interest seam image, a spatiotemporalnenergy map is constructed for the target videonshot. Then an optimal seam which encompasses the highestnenergy is identified by an efficient dynamic programmingnalgorithm. The optimal seam is used to extract a seam ofnpixels from each video frame to form one column of an image,nbased on which an interest seam image is finally composited.nThe interest seam image is efficient both in terms ofncomputation and memory cost. Therefore it is able to powerna wide variety of web-scale video content analysis applications,nsuch as near duplicate video clip search, video genrenrecognition and classification, as well as video clustering,netc.. The representation capacity of the proposed interestnseam image is demonstrated in a large scale video retrievalntask. Its advantages are clearly exhibited when comparednwith previous works, as reported in our experiments.
13057 en Aggregating local descriptors into a compact image representation We address the problem of image search on a very largenscale, where three constraints have to be considered jointly:nthe accuracy of the search, its efficiency, and the memorynusage of the representation. We first propose a simple yetnefficient way of aggregating local image descriptors into anvector of limited dimension, which can be viewed as a simplificationnof the Fisher kernel representation. We then shownhow to jointly optimize the dimension reduction and the indexingnalgorithm, so that it best preserves the quality of vectorncomparison. The evaluation shows that our approachnsignificantly outperforms the state of the art: the search accuracynis comparable to the bag-of-features approach fornan image representation that fits in 20 bytes. Searching an10 million image dataset takes about 50ms.
13058 en Automatic Image Annotation Using Group Sparsity Automatically assigning relevant text keywords to imagesnis an important problem. Many algorithms have been proposednin the past decade and achieved good performance.nEfforts have focused upon model representations of keywords,nbut properties of features have not been well investigated.nIn most cases, a group of features is preselected,nyet important feature properties are not well used to selectnfeatures. In this paper, we introduce a regularization basednfeature selection algorithm to leverage both the sparsity andnclustering properties of features, and incorporate it into thenimage annotation task. A novel approach is also proposednto iteratively obtain similar and dissimilar pairs from bothnthe keyword similarity and the relevance feedback. Thusnkeyword similarity is modeled in the annotation framework.nNumerous experiments are designed to compare the performancenbetween features, feature combinations and regularizationnbased feature selection methods applied on the imagenannotation task, which gives insight into the propertiesnof features in the image annotation task. The experimentalnresults demonstrate that the group sparsity based method isnmore accurate and stable than others.
13060 en Polynomial Shape from Shading We examine the shape from shading problem withoutnboundary conditions as a polynomial system. This view allows,nin generic cases, a complete solution for ideal polyhedralnobjects. For the general case we propose a semidefinitenprogramming relaxation procedure, and an exact linensearch iterative procedure with a new smoothness term thatnfavors folds at edges. We use this numerical technique toninspect shading ambiguities.
13061 en Analysis of Light Transport in Scattering Media We propose a new method to analyze light transport innhomogeneous scattering media. The incident light undergoesnmultiple bounces in translucent objects, and producesna complex light field. Our method analyzes the light transportnin two steps. First, single and multiple scatteringnare separated by projecting high-frequency stripe patterns.nThen, multiple scattering is decomposed into each bouncencomponent based on the light transport equation. The lightnfield for each bounce is recursively estimated. Experimentalnresults show that light transport in scattering media can bendecomposed and visualized for each bounce.
13062 en A New Texture Descriptor Using Multifractal Analysis in Multi-orientation Wavelet Pyramid Based on multifractal analysis in wavelet pyramids ofntexture images, a new texture descriptor is proposed in thisnpaper that implicitly combines information from both spatialnand frequency domains. Beyond the traditional waveletntransform, a multi-oriented wavelet leader pyramid is usednin our approach that robustly encodes the multi-scale informationnof texture edgels. Moreover, the resulting texturenmodel shows empirically a strong power law relationshipnfor nature textures, which can be characterized well bynmultifractal analysis. Combined with a statistics on affineninvariant local patches, our proposed texture descriptor isnrobust to scale and rotation changes, more general geometricalntransforms and illumination variations. In addition,nthe proposed texture descriptor is computationally efficientnsince it does not require many expensive processing steps,ne.g., texton generation and cross-bin comparisons, whichnare often used by existing methods. As an application, thenproposed descriptor is applied to texture classification andnthe experimental results on several public texture datasetsnverified the accuracy and efficiency of our descriptor
13064 en A Theory of Plenoptic Multiplexing Multiplexing is a common technique for encoding highdimensionalnimage data into a single, two-dimensional image.nExamples of spatial multiplexing include Bayer patternsnto capture color channels, and integral images to encodenlight fields. In the Fourier domain, optical heterodyningnhas been used to acquire light fields.nIn this paper, we develop a general theory of multiplexingnthe dimensions of the plenoptic function onto an imagensensor. Our theory enables a principled comparisonnof plenoptic multiplexing schemes, including noise analysis,nas well as the development of a generic reconstruction algorithm.nThe framework also aides in the identification andnoptimization of novel multiplexed imaging applications.
13065 en Non-uniform Deblurring for Shaken Images Blur from camera shake is mostly due to the 3D rotationnof the camera, resulting in a blur kernel that can bensignificantly non-uniform across the image. However, mostncurrent deblurring methods model the observed image asna convolution of a sharp image with a uniform blur kernel.nWe propose a new parametrized geometric model ofnthe blurring process in terms of the rotational velocity ofnthe camera during exposure. We apply this model to twondifferent algorithms for camera shake removal: the first onenuses a single blurry image (blind deblurring), while the secondnone uses both a blurry image and a sharp but noisy imagenof the same scene. We show that our approach makesnit possible to model and remove a wider class of blurs thannprevious approaches, including uniform blur as a specialncase, and demonstrate its effectiveness with experiments onnreal images.
13066 en Axial Light Field for Curved Mirrors: Reflect Your Perspective, Widen Your View Mirrors have been used to enable wide field-of-viewn(FOV) catadioptric imaging. The mapping between the incomingnand reflected light rays depends non-linearly on thenmirror shape and has been well-studied using caustics. Wenanalyze this mapping using two-plane light field parameterization,nwhich provides valuable insight into the geometricnstructure of reflected rays. Using this analysis, we study thenproblem of generating a single-viewpoint virtual perspectivenimage for catadioptric systems, which is unachievablenfor several common configurations.nInstead of minimizing distortions appearing in a singlenimage, we propose to capture all the rays required to generatena virtual perspective by capturing a light field. Wenconsider rotationally symmetric mirrors and show that antraditional planar light field results in significant aliasingnartifacts. We propose axial light field, captured by movingnthe camera along the mirror rotation axis, for efficientnsampling and to remove aliasing artifacts. This allows usnto computationally generate wide FOV virtual perspectivesnusing a wider class of mirrors than before, without usingnscene priors or depth estimation. We analyze the relationshipnbetween the axial light field parameters and thenFOV/resolution of the resulting virtual perspective. Real resultsnusing a spherical mirror demonstrate generating 140?nFOV virtual perspective using multiple 30? FOV images.
13067 en Rectifying rolling shutter video from hand-held devices This paper presents a method for rectifying video sequencesnfrom rolling shutter (RS) cameras. In contrast tonprevious RS rectification attempts we model distortions asnbeing caused by the 3D motion of the camera. The cameranmotion is parametrised as a continuous curve, with knotsnat the last row of each frame. Curve parameters are solvednfor using non-linear least squares over inter-frame correspondencesnobtained from a KLT tracker. We have generatednsynthetic RS sequences with associated ground-truthnto allow controlled evaluation. Using these sequences, wendemonstrate that our algorithm improves over to two previouslynpublished methods. The RS dataset is available onnthe web to allow comparison with other methods.
13069 en RASL: Robust Alignment by Sparse and Low-rank Decomposition for Linearly Correlated Images This paper studies the problem of simultaneously aligning a batch of linearly correlated images despite gross corruption (such as occlusion). Our method seeks an optimal set of image domain transformations such that the matrix of transformed images can be decomposed as the sum of a sparse matrix of errors and a low-rank matrix of recovered aligned images. We reduce this extremely challenging optimization problem to a sequence of convex programs that minimize the sum of 1 -norm and nuclear norm of the two component matrices, which can be efficiently solved by scalable convex optimization techniques with guaranteed fast convergence. We verify the efficacy of the proposed robust alignment algorithm with extensive experiments with both controlled and uncontrolled real data, demonstrating higher accuracy and efficiency than existing methods over a wide range of realistic misalignments and corruptions.
13071 en On the design of robust classifiers for computer vision The design of robust classifiers, which can contend withnthe noisy and outlier ridden datasets typical of computer vision,nis studied. It is argued that such robustness requiresnloss functions that penalize both large positive and negativenmargins. The probability elicitation view of classifier designnis adopted, and a set of necessary conditions for the designnof such losses is identified. These conditions are used to derivena novel robust Bayes-consistent loss, denoted Tangentnloss, and an associated boosting algorithm, denoted TangentBoost.nExperiments with data from the computer visionnproblems of scene classification, object tracking, and multipleninstance learning show that TangentBoost consistentlynoutperforms previous boosting algorithms.
13072 en Online-Batch Strongly Convex Multi Kernel Learning Several object categorization algorithms use kernelnmethods over multiple cues, as they offer a principled approachnto combine multiple cues, and to obtain state-of-theartnperformance. A general drawback of these strategies isnthe high computational cost during training, that preventsntheir application to large-scale problems. They also do notnprovide theoretical guarantees on their convergence rate.nHere we present a Multiclass Multi Kernel Learningn(MKL) algorithm that obtains state-of-the-art performancenin a considerably lower training time. We generalize thenstandardMKL formulation to introduce a parameter that allowsnus to decide the level of sparsity of the solution. Thanksnto this new setting, we can directly solve the problem in thenprimal formulation. We prove theoretically and experimentallynthat 1) our algorithm has a faster convergence rate asnthe number of kernels grow; 2) the training complexity isnlinear in the number of training examples; 3) very few iterationsnare enough to reach good solutions. Experiments onnthree standard benchmark databases support our claims.
13074 en Using Cloud Shadows to Infer Scene Structure and Camera Calibration We explore the use of clouds as a form of structured lightingnto capture the 3D structure of outdoor scenes observednover time from a static camera. We derive two cues that relaten3D distances to changes in pixel intensity due to cloudsnshadows. The first cue is primarily spatial, works with lownframe-rate time lapses, and supports estimating focal lengthnand scene structure, up to a scale ambiguity. The secondncue depends on cloud motion and has a more complex, butnstill linear, ambiguity. We describe a method that uses thenspatial cue to estimate a depth map and a method that combinesnboth cues. Results on time lapses of several outdoornscenes show that these cues enable estimating scene geometrynand camera focal length.
13075 en Depth from Diffusion An optical diffuser is an element that scatters light and isncommonly used to soften or shape illumination. In this paper,nwe propose a novel depth estimation method that placesna diffuser in the scene prior to image capture. We call thisnapproach depth-from-diffusion (DFDiff).nWe show that DFDiff is analogous to conventional depthfrom-ndefocus (DFD), where the scatter angle of the diffuserndetermines the effective aperture of the system. The mainnbenefit of DFDiff is that while DFD requires very largenapertures to improve depth sensitivity, DFDiff only requiresnan increase in the diffusion angle – a much less expensivenproposition. We perform a detailed analysis of the imagenformation properties of a DFDiff system, and show a varietynof examples demonstrating greater precision in depthnestimation when using DFDiff.
13076 en Self-calibrating Photometric Stereo We present a self-calibrating photometric stereo method.nFrom a set of images taken from a fixed viewpoint underndifferent and unknown lighting conditions, our method automaticallyndetermines a radiometric response function andnresolves the generalized bas-relief ambiguity for estimatingnaccurate surface normals and albedos. We show that colornand intensity profiles, which are obtained from registerednpixels across images, serve as effective cues for addressingnthese two calibration problems. As a result, we developna complete auto-calibration method for photometric stereo.nThe proposed method is useful in many practical scenariosnwhere calibrations are difficult. Experimental results validatenthe accuracy of the proposed method using variousnreal-world scenes.
13078 en Probabilistic Temporal Inference on Reconstructed 3D Scenes Modern structure from motion techniques are capablenof building city-scale 3D reconstructions from large imagencollections, but have mostly ignored the problem of largescalenstructural changes over time. We present a generalnframework for estimating temporal variables in structurenfrom motion problems, including an unknown date for eachncamera and an unknown time interval for each structural element.nGiven a collection of images with mostly unknown ornuncertain dates, we use this framework to automatically recovernthe dates of all images by reasoning probabilisticallynabout the visibility and existence of objects in the scene. Wenpresent results on a collection of over 100 historical imagesnof a city taken over decades of time.
13079 en Piecewise Planar and Non-Planar Stereo for Urban Scene Reconstruction Piecewise planar models for stereo have recently becomenpopular for modeling indoor and urban outdoornscenes. The strong planarity assumption overcomes thenchallenges presented by poorly textured surfaces, and resultsnin low complexity 3D models for rendering, storage,nand transmission. However, such a model performs poorlynin the presence of non-planar objects, for example, bushes,ntrees, and other clutter present in many scenes. We presentna stereo method capable of handling more general scenesncontaining both planar and non-planar regions. Our proposedntechnique segments an image into piecewise planarnregions as well as regions labeled as non-planar. The nonplanarnregions are modeled by the results of a standardnmulti-view stereo algorithm. The segmentation is driven bynmulti-view photoconsistency as well as the result of a colorandntexture-based classifier, learned from hand-labeled planarnand non-planar image regions. Additionally our methodnlinks and fuses plane hypotheses across multiple overlappingnviews, ensuring a consistent 3D reconstruction overnan arbitrary number of images. Using our system, we havenreconstructed thousands of frames of street-level video. Resultsnshow our method successfully recovers piecewise planarnsurfaces alongside general 3D surfaces in challengingnscenes containing large buildings as well as residentialnhouses.
13080 en Disambiguating Visual Relations Using Loop Constraints Repetitive and ambiguous visual structures in generalnpose a severe problem in many computer vision applications.nIdentification of incorrect geometric relations betweennimages solely based on low level features is not alwaysnpossible, and a more global reasoning approach aboutnthe consistency of the estimated relations is required. Wenpropose to utilize the typically observed redundancy in thenhypothesized relations for such reasoning, and focus on thengraph structure induced by those relations. Chaining then(reversible) transformations over cycles in this graph allowsnto build suitable statistics for identifying inconsistentnloops in the graph. This data provides indirect evidence fornconflicting visual relations. Inferring the set of likely falsenpositive geometric relations from these non-local observationsnis formulated in a Bayesian framework. We demonstratenthe utility of the proposed method in several applications,nmost prominently the computation of structure andnmotion from images.
13082 en Non-Rigid Structure from Locally-Rigid Motion We introduce locally-rigid motion, a general framework fornsolving the M-point, N-view structure-from-motion problemnfor unknown bodies deforming under orthography. Thenkey idea is to first solve many local 3-point, N-view rigidnproblems independently, providing a “soup” of specific,nplausibly rigid, 3D triangles. The main advantage here isnthat the extraction of 3D triangles requires only very weaknassumptions: (1) deformations can be locally approximatednby near-rigid motion of three points (i.e., stretching notndominant) and (2) local motions involve some generic rotationnin depth. Triangles from this soup are then groupedninto bodies, and their depth flips and instantaneous relativendepths are determined. Results on several sequences,nboth our own and from related work, suggest these conditionsnapply in diverse settings—including very challengingnones (e.g., multiple deforming bodies). Our starting pointnis a novel linear solution to 3-point structure from motion,na problem for which no general algorithms currently exist.
13083 en Bundled Depth-Map Merging for Multi-View Stereo 
13084 en Multi-View Structure Computation without Explicitly Estimating Motion 
13086 en A Generative Perspective on MRFs in Low-Level Vision Markov random fields (MRFs) are popular and genericnprobabilistic models of prior knowledge in low-level vision.nYet their generative properties are rarely examined, whilenapplication-specific models and non-probabilistic learningnare gaining increased attention. In this paper we revisitnthe generative aspects of MRFs, and analyze the quality ofncommon image priors in a fully application-neutral setting.nEnabled by a general class of MRFs with flexible potentialsnand an efficient Gibbs sampler, we find that common modelsndo not capture the statistics of natural images well. Wenshow how to remedy this by exploiting the efficient samplernfor learning better generative MRFs based on flexible potentials.nWe perform image restoration with these modelsnby computing the Bayesian minimum mean squared errornestimate (MMSE) using sampling. This addresses a numbernof shortcomings that have limited generative MRFs so far,nand leads to substantially improved performance over maximumna-posteriori (MAP) estimation. We demonstrate thatncombining our learned generative models with sampling basednMMSE estimation yields excellent application resultsnthat can compete with recent discriminative methods.
13087 en Manifold Blurring Mean Shift Algorithms for Manifold Denoising We propose a new family of algorithms for denoisingndata assumed to lie on a low-dimensional manifold. Thenalgorithms are based on the blurring mean-shift update,nwhich moves each data point towards its neighbors, but constrain the motion to be orthogonal to the manifold. Thenresulting algorithms are nonparametric, simple to implement and very effective at removing noise while preservingnthe curvature of the manifold and limiting shrinkage. Theyndeal well with extreme outliers and with variations of density along the manifold. We apply them as preprocessing forndimensionality reduction; and for nearest-neighbor classification of MNIST digits, with consistent improvements upnto 36% over the original data.
13088 en Increasing Depth Resolution of Electron Microscopy of Neural Circuits using Sparse Tomographic Reconstruction Future progress in neuroscience hinges on reconstructionnof neuronal circuits to the level of individual synapses.nBecause of the specifics of neuronal architecture, imagingnmust be done with very high resolution and throughput.nWhile Electron Microscopy (EM) achieves the required resolutionnin the transverse directions, its depth resolution isna severe limitation. Computed tomography (CT) may benused in conjunction with electron microscopy to improventhe depth resolution, but this severely limits the throughputnsince several tens or hundreds of EM images need to benacquired. Here, we exploit recent advances in signal processingnto obtain high depth resolution EM images computationally.nFirst, we show that the brain tissue can be representednas sparse linear combination of local basis functionsnthat are thin membrane-like structures oriented in variousndirections. We then develop reconstruction techniquesninspired by compressive sensing that can reconstruct thenbrain tissue from very few (typically 5) tomographic viewsnof each section. This enables tracing of neuronal connectionsnacross layers and, hence, high throughput reconstructionnof neural circuits to the level of individual synapses.
13090 en Visual Classification with Multi-Task Joint Sparse Representation 
13091 en Classification and Clustering via Dictionary Learning with Structured Incoherence and Shared Features 
13092 en The Automatic Design of Feature Spaces for Local Image Descriptors using an Ensemble of Non-linear Feature Extractors The design of feature spaces for local image descriptorsnis an important research subject in computer vision due tonits applicability in several problems, such as visual classificationnand image matching. In order to be useful, these descriptorsnhave to present a good trade off between discriminatingnpower and robustness to typical image deformations.nThe feature spaces of the most useful local descriptors havenbeen manually designed based on the goal above, but thisndesign often limits the use of these descriptors for some specificnmatching and visual classification problems. Alternatively,nthere has been a growing interest in producing featurenspaces by an automatic combination of manually designednfeature spaces, or by an automatic selection of featurenspaces and spatial pooling methods, or by the use ofndistance metric learning methods. While most of these approachesnare usually applied to specific matching or classificationnproblems, where test classes are the same as trainingnclasses, a few works aim at the general feature transformnproblem where the training classes are different fromnthe test classes. The hope in the latter works is the automaticndesign of a universal feature space for local descriptornmatching, which is the topic of our work. In this paper,nwe propose a new incremental method for learning automaticallynfeature spaces for local descriptors. The methodnis based on an ensemble of non-linear feature extractorsntrained in relatively small and random classification problemsnwith supervised distance metric learning techniques.nResults on two widely used public databases show that ourntechnique produces competitive results in the field.
13095 en Parallel and Distributed Graph Cuts by Dual Decomposition Graph cuts methods are at the core of many state-of-theartnalgorithms in computer vision due to their efficiencynin computing globally optimal solutions. In this paper, wensolve the maximum flow/minimum cut problem in parallelnby splitting the graph into multiple parts and hence, furthernincrease the computational efficacy of graph cuts. Optimalitynof the solution is guaranteed by dual decomposition, or morenspecifically, the solutions to the subproblems are constrainednto be equal on the overlap with dual variables.nWe demonstrate that our approach both allows (i) fasternprocessing on multi-core computers and (ii) the capabilitynto handle larger problems by splitting the graph across multiplencomputers on a distributed network. Even though ournapproach does not give a theoretical guarantee of speedup,nan extensive empirical evaluation on several applicationsnwith many different data sets consistently shows goodnperformance. An open source implementation of the dualndecomposition method is also made publicly available.
13096 en Object Separation In X-Ray Image Sets In the segmentation of natural images, most algorithmsnrely on the concept of occlusion. In x-ray images, however,nthis assumption is violated, since x-ray photons penetratenmost materials. In this paper, we introduce SATIS?, anmethod for separating objects in a set of x-ray images usingnthe property of additivity in log space, where the logattenuationnat a pixel is the sum of the log-attenuationsnof all objects that the corresponding x-ray passes through.nOur method leverages multiple projection views of the samenscene from slightly different angles to produce an accuratenestimate of attenuation properties of objects in thenscene. These properties can be used to identify the materialncomposition of these objects, and are therefore crucialnfor applications like automatic threat detection. We evaluatenSATIS? on a set of collected x-ray scans, showing that itnoutperforms a standard image segmentation approach andnreduces the error of material estimation.
13098 en Isoperimetric Cut on a Directed Graph 
13100 en Tiered Scene Labeling with Dynamic Programming Dynamic programming (DP) has been a useful tool for anvariety of computer vision problems. However its applicationnis usually limited to problems with a one dimensionalnor low treewidth structure, whereas most domains in visionnare at least 2D. In this paper we show how to apply DPnfor pixel labeling of 2D scenes with simple “tiered” structure.nWhile there are many variations possible, for the applicationsnwe consider the following tiered structure is appropriate.nAn image is first divided by horizontal curvesninto the top, middle, and bottom regions, and the middle regionnis further subdivided vertically into subregions. Undernthese constraints a globally optimal labeling can be foundnusing an efficient dynamic programming algorithm. We applynthis algorithm to two very different tasks. The first isnthe problem of geometric class labeling where the goal isnto assign each pixel a label such as “sky”, “ground”, andn“surface above ground”. The second task involves incorporatingnsimple shape priors for segmentation of an imageninto the “foreground” and “background” regions.
13101 en Segmentation of Building Facades Using Procedural Shape Priors In this paper we propose a novel approach to the perceptualninterpretation of building facades that combines shapengrammars, supervised classification and random walks.nProcedural modeling is used to model the geometric andnthe photometric variation of buildings. This is fused with visualnclassification techniques (randomized forests) that providena crude probabilistic interpretation of the observationnspace in order to measure the appropriateness of a proceduralngeneration with respect to the image. A random explorationnof the grammar space is used to optimize the sequencenof derivation rules towards a semantico-geometricninterpretation of the observations. Experiments conductednon complex architecture facades with ground truth validatenthe approach.
13102 en Layered Object Detection for Multi-Class Segmentation We formulate a layered model for object detection andnmulti-class segmentation. Our system uses the output of anbank of object detectors in order to define shape priors fornsupport masks and then estimates appearance, depth orderingnand labeling of pixels in the image. We train our systemnon the PASCAL segmentation challenge dataset and showngood test results with state of the art performance in severalncategories including segmenting humans.
13104 en Measuring Visual Saliency by Site Entropy Rate 
13105 en Context-Aware Saliency Detection We propose a new type of saliency – context-aware saliencyn– which aims at detecting the image regions that representnthe scene. This definition differs from previous definitionsnwhose goal is to either identify fixation points or detect thendominant object. In accordance with our saliency definition,nwe present a detection algorithm which is based onnfour principles observed in the psychological literature. Thenbenefits of the proposed approach are evaluated in two applicationsnwhere the context of the dominant objects is justnas essential as the objects themselves. In image retargetingnwe demonstrate that using our saliency prevents distortionsnin the important regions. In summarization we show thatnour saliency helps to produce compact, appealing, and informativensummaries.
13106 en Minimum length in the tangent bundle as a model for curve completion 
13114 en Reproducible Research in Computational Science: Problems and Solutions For Data and Code Sharing? Scientific computation is emerging as absolutely central to the scientific method, but the prevalence of very relaxed practices is leading to a credibility crisis. Reproducible computational research, in which all details of computations—code and data—are made conveniently available to others, is a necessary response to this crisis. Results from a 2009 survey of the Machine Learning community (NIPS participants) designed to elucidate factors that affect data and code sharing will be presented. Intellectual property concerns create a significant barrier to sharing, and I will also present work on the “Reproducible Research Standard” giving open licensing options designed to create an intellectual property framework for scientists consonant with longstanding scientific norms and facilitating reproducible research.n
13115 en Universal Java Matrix Package The Universal Java Matrix Package (UJMP) is an open source Java library which provides sparse and dense matrix classes, as well as a large number of calculations for linear algebra like matrix multiplication or matrix inverse. Operations such as mean, correlation, standard deviation, replacement of missing values or the calculation of mutual information are supported also.nnThe Universal Java Matrix Package provides various visualization methods, import and export filters for a large number of file formats, and the possibility to link to tables in JDBC databases. Multi-dimensional matrices as well as generic matrices with a specified object type are supported and very large matrices with up to 2^63 rows and columns can be handled even when they do not fit into memory.nnA central concept of UJMP is the separation of interfaces, abstract classes and their implementations, which makes it very easy to exchange the underlying data storage. Thus, a matrix in our framework can be an array of values in main memory, a file on disk or a table in an SQL database. In fact, the actual storage implementation becomes secondary and UJMP can even integrate other matrix libraries such as JAMA or Colt, making UJMP’s visualization and import and export filters available to these libraries.nnOn the other hand, UJMP can also decide to redirect calculations to other matrix libraries, depending on matrix size and computer hardware. UJMP uses multiple threads for calculations, which results in much better performance compared to JAMA or Colt on modern hardware.nnUJMP also includes interfaces to Matlab, Octave and R, which makes it easy to perform calculations not available in Java.nnWhile some parts of UJMP are pretty stable by now, a lot of development is still going on in other parts. Developers are welcome to contribute!
13116 en Shogun The SHOGUN machine learning toolbox's focus is on large scale kernel methods and especially on Support Vector Machines (SVM). It comes with a generic interface for kernel machines and features 15 different SVM implementations that all access features in a unified way via a general kernel framework or in case of linear SVMs so called "DotFeatures", i.e., features providing a minimalistic set of operations (like the dot product).
13117 en The next steps after UCI - mldata.org Recently, mloss.org has enabled machine learning researchers to register their software and allow other researchers to easily find, download, and reuse software matching their interests. Currently, more than 200 projects are listed. Furthermore, the Journal of Machine Learning Research now accepts papers to its special Open Source Software track, in which papers describing peer-reviewed software can be published, as a further incentive for researchers to publish their software under an open source license. Since its inception, in October 2007, seven papers have been published in this track with more papers currently under review. So far, the initiative has been highly successful, but has focused mostly on the ”method” side of the problem to make machine learning research more reproducible. Hence we see the need to initiate a companion project to mloss.org which focuses on the free exchange and benchmarking of datasets. Additionally, this new repository will emphasise the precise specification of machine learning tasks: detailed definitions of datasets to be used (possibly including feature extraction or other preprocessing steps) together with the desired operation to be performed and the relevant performance metric. Finally, a solution to such a task would provide details of how to apply a general software package (such as on mloss.org) to this particular problem instance, as well as the obtained numerical performance measures. This project will thus focus on providing a platform for publishing, exchanging, collecting, and discussing such data sets, tasks, and solutions for challenging machine learning problems.
13119 en Libra The Libra machine learning toolkit includes implementations of a variety of algorithms for learning and inference with Bayesian networks and arithmetic circuits:nnLearning algorithms -- Structure learning for BNs and ACs; Chow-Liu algorithm; AC weight learningnnInference algorithms - Mean field, belief propagation, Gibbs sampling, AC variable elimination, AC exact inferencennLibra's strength is exploiting context-specific independence (such as decision tree CPDs) to allow exact inference in models with high treewidth.
13120 en FastInf The FastInf C++ library is designed to perform memory and time efficient approximate inference in large-scale discrete undirected graphical models. The focus of the library is propagation based approximate inference methods, ranging from the basic loopy belief propagation algorithm to propagation based on convex free energies. Various message scheduling schemes that improve on the standard synchronous or asynchronous approaches are included. Also implemented are a clique tree based exact inference, Gibbs sampling, and the mean field algorithm. In addition to inference, FastInf provides parameter estimation capabilities as well as representation and learning of shared parameters. It offers a rich interface that facilitates extension of the basic classes to other inference and learning methods.
13121 en PyBrain PyBrain is a versatile machine learning library for Python. Its goal is to provide flexible, easy-to-use yet still powerful algorithms for machine learning tasks, including a variety of predefined environments and benchmarks to test and compare algorithms. Implemented algorithms include Long Short-Term Memory (LSTM), policy gradient methods, (multidimensional) recurrent neural networks and evolution strategies like CMA-ES, NES or FEM.
13122 en Jstacs Sequence analysis is one of the major subjects of bioinformatics. Several existing libraries combine the representation of biological sequences with exact and approximate pattern matching as well as alignment algorithms. We present Jstacs, an open source Java library, which focuses on the statistical analysis of biological sequences instead. Jstacs comprises an efficient representation of sequence data and provides implementations of many statistical models with generative and discriminative approaches for parameter learning. Using Jstacs, classifiers can be assessed and compared on test datasets or by cross-validation experiments evaluating several performance measures. Due to its strictly object-oriented design Jstacs is easy to use and readily extensible.
13123 en JBlas jblas is a fast linear algebra library for Java. jblas is based on BLAS and LAPACK, the de-facto industry standard for matrix computations, and uses state-of-the-art implementations like ATLAS for all its computational routines, making jBLAS very fast.nnjblas can is essentially a light-wight wrapper around the BLAS and LAPACK routines. These packages have originated in the Fortran community which explains their archaic API. On the other hand modern implementations are hard to beat performance wise. jblas aims to make this functionality available to Java programmers such that they do not have to worry about writing JNI interfaces and calling conventions of Fortran code.nnjblas is the only actively developed matrix library which is based on native implementations (The other such project is netlib-java which is apparently not maintained anymore). Therefore, jblas is much faster than other projects, in particular for large complex tasks like matrix-matrix multiplication, solving linear equations, or eigenproblems.
13124 en Scikitlearn Scikits.learn is a Python module integrating classique machine learning algorithmes in the tightly-nit world of scientific Python packagesnnIt aims to provide simple and efficient solutions to learning problems that are accessible to everybody and reusable in various contexts: machine-learning as a versatile tool for science and engineering.
13125 en Mulan Mulan Mulan is an open-source Java library for learning from multi-label datasets. Multi-label datasets consist of training examples of a target function that has multiple binary target variables. This means that each item of a multi-label dataset can be a member of multiple categories or annotated by many labels (classes). This is actually the nature of many real world problems such as semantic annotation of images and video, web page categorization, direct marketing, functional genomics and music categorization into genres and emotions. An introduction on mining multi-label data is provided in (Tsoumakas et al., 2010).nnCurrently, the library includes a variety of state-of-the-art algorithms for performing the following major multi-label learning tasks:nnClassification. This task is concerned with outputting a bipartition of the labels into relevant and irrelevant ones for a given input instance.nRanking. This task is concerned with outputting an ordering of the labels, according to their relevance for a given data itemnClassification and ranking. A combination of the two tasks mentioned-above.nIn addition, the library offers the following features:nnFeature selection. Simple baseline methods are currently supported.nEvaluation. Classes that calculate a large variety of evaluation measures through hold-out evaluation and cross-validation.nAs already mentioned, Mulan is a library. As such, it offers only programmatic API to the library users. There is no graphical user interface (GUI) available. The possibility to use the library via command line, is also currently not supported. The Getting Started page in the Documentation section is the ideal place to start exploring Mulan.
13126 en OpenKernel The OpenKernel library is an open-source software library for designing, combining, learning and using kernels for machine learning applicationsnnThe library supports the design and use of kernels defined over dense and sparse real vectors, as well as over sequences or distributions of sequences.nnFor dense and sparse features, the library provides implementation of the classical kernels: linear, polynomial, Gaussian and sigmoid.nnFor sequences and distributions of sequences, the library implements the rational kernel framework of Cortes et al. (JMLR, 2004). The library supplies the following sequence kernels:nnn -gram kernels,ngappy n-gram kernels,nmismatch kernels (Leslie et al., 2004),nand gives the utilities for creating arbitrary rational kernels simply by providing the weighted finite-state transducers they are based on.nnKernels can be combined by taking their sum or their product, and can be composed with a polynomial, a Gaussian or a sigmoid. They support on-demand evaluation and caching. In addition to its own binary format, the library uses the ASCII format of LIBSVM/LIBLINEAR/SVMlight for representing features (and precomputed kernels for LIBSVM).nnFinally, the OpenKernel library also includes several options for using training data to automatically combine multiple kernels. This is particularly useful when the single best kernel for the task is not known. The algorithms implemented includennL1-regularized linear combinations (Lanckriet et al. JMLR 2004);nL2-regularized linear combinations (Cortes et al. UAI 2009);nL2-regularized quadratic combinations (Cortes et al. NIPS 2009),nas well as kernel correlation, or alignment (Cortes et al. ICML 2010), based combinations. Specialized efficient versions of these algorithms are also made available for weighting features and sparseness and can be used to further improve efficiency. The output kernels can be easily used in conjunction with LIBSVM, SVMlight and included kernel ridge regression implementations. Full reference documentation, tutorials and examples (with formatted datasets) are included.nnThe library is an open-source project distributed under the Apache license (2.0). This work has been partially supported by Google Inc. The library uses the OpenFst library for representing and manipulating weighted finite-state transducers.
13127 en MultiBoost AdaBoost [Freund-Schapire, 1997] is one of the best off-the-shelf supervised classification methods developed in the last fifteen years. Despite (or perhaps due to?) its simplicity and versatility, it is suprisingly under-represented in the family of open softwares. The goal of this submission is to fill this gap.nnOur implementation is based on the AdaBoost.MH algorithm [Schapire-Singer, 1999]. It is an intrinsically multi-class classification method (unlike SVM for example), and it was easy to extend to multi-label or multi-task classification (when one item can belong to several classes). The program package can be divided into four modules that can be changed more-or-less independently depending on the application.nnThe strong learner. It tells you how to boost. The main boosting engine is AdaBoost.MH, but we have also implemented FilterBoost for a research project. Other possible strong learners could be LogitBoost and ADTrees.nnThe base (or weak) learner. It tells you what features to boost. Right now we have two basic (feature-wise) base learners: decision stumps for real-valued features and indicators for nominal features. We have two meta base learners: trees and products. They can use any base learner and construct a generic complex base learner using a "classic" tree-structure (decision trees), or using the product of simple base learners (self advertisement: boosting products of stumps is the best reported no-domain-knowledge algorithm on MNIST after Hinton and Salakhutdinov's deep belief nets). We have also implemented Haar filters [Viola-Jones, 2004] for image classification, a meta base learner that uses stumps over a high dimensional feature space computed "on the fly". It is a nice example of a domain dependent base learner that works hand-in-hand with its appropriate data structure.nnThe data representation. The basic data structure is a matrix of observations with a vector of labels. We also have multi-label classification when the label data is also a full matrix. In addition, we have sparse data representation for both the observation matrix and the label matrix. In general, base learners are implemented to work with their own data representation (for example, sparse stumps work on sparse observation matrices, or Haar filters work on a integral image data representation.nnData parser. We can read in data in arff and svmlight formats.nnThe base learner/data structure combinations cover a large spectrum of possible applications, but the main advantage of the package is that it is easy (for the advanced user) to adapt MultiBoost to a specific (non-standard) application by implementing the base learner and data structure interfaces that work together.nnThe source code is available from the website multiboost.org. It can be compiled on Mac OS X, Linux, and Microsoft Windows. The interface is command line execution with switches.
13128 en Gidoc Transcription of handwritten text in (old) documents is an important, time-consuming task for digital libraries. It might be carried out by first processing all document images off-line, and then manually supervising system transcriptions to edit incorrect parts. However, current techniques for automatic page layout analysis, text line detection and handwriting recognition are still far from perfect, and thus post-editing system output is not clearly better than simply ignoring it.nnA more effective approach to transcribe old text documents is to follow an interactive- predictive paradigm in which both, the system is guided by the user, and the user is assisted by the system to complete the transcription task as efficiently as possible. Following this approach, a system prototype called GIDOC (Gimp-based Interactive transcription of old text DOCuments) has been developed to provide user-friendly, integrated support for interactive-predictive layout analysis, line detection and handwriting transcription.nnGIDOC is designed to work with (large) collections of homogeneous documents, that is, of similar structure and writing styles. They are annotated sequentially, by (par- tially) supervising hypotheses drawn from statistical models that are constantly updated with an increasing number of available annotated documents. And this is done at different annotation levels. For instance, at the level of page layout analysis, GIDOC uses a novel text block detection method in which conventional, memoryless techniques are improved with a “history” model of text block positions. Similarly, at the level of text line image transcription, GIDOC includes a handwriting recognizer which is steadily improved with a growing number of (partially) supervised transcriptions.
13129 en Dependency Modelling Toolbox Investigation of dependencies between multiple data sources allows the discovery of regularities and interactions that are not seen in individual data sets. The increasing availability of co-occurring measurement data in computational biology, social sciences, and in other domains emphasizes the need for practical implementations of general-purpose dependency modeling algorithms.nnThe project collects various dependency modeling approaches into a unified toolbox. The techniques for the discovery and analysis of statistical dependencies are based on well-established models such as probabilistic canonical correlation analysis and multi-task learning whose applicability has been demonstrated in previous case studies.
13131 en Introduction and overview of the SIMBAD project 
13132 en Clustering without any subjective similarity information Consider the task of clustering university web pages based on the graph of links between these pages. Can clusters of "functionally similar" pages be detected from just this link structure? Note that this is a clustering task in which one starts without any prior knowledge of any similarity or distance measure between the domain elements. All the information in the input comes as objective, observed, binary relations among the objects. These relations are not similarity links. For example, the cluster of professors pages have very internal links, whereas the cluster of service pages have lots of internal links. What we are looking for are clusters whose members share similar link patterns with respect to the other clusters. nWe propose a formal model for such clustering tasks. Our model is based on an objective function that measures the homogeneity of between-clusters links. I shall discuss the computational complexity of finding a clustering with minimal objective cost and describe some hardness results as well as efficient approximation algorithms. nThe talk is (partly) based on work with Sharon Wulff.
13133 en Learning with similarity functions Kernel functions have become an extremely popular tool in machine learning, with many applications and an attractive theory. This theory views a kernel as performing an implicit mapping of data points into a possibly very high dimensional space, and describes a kernel function as being good for a given learning problem if data is separable by a large margin in that implicit space. In this talk I will describe an alternative, more general, theory of learning with similarity functions (i.e., sufficient conditions for a similarity function to allow one to learn well) that does not require reference to implicit spaces, and does not require the function to be positive semi-definite (or even symmetric). nIn particular, I will describe a notion of a good similarity function for a given learning problem that (a) is fairly natural and intuitive (it does not require an implicit space and allows for functions that are not positive semi-definite), (b) is a sufficient condition for learning well, and (c) strictly generalizes the notion of a large-margin kernel function in that any such kernel is also a good similarity function, though not necessarily vice-versa.
13134 en From collaborative filtering to multitask learning Recent work on collaborative filtering has led to a large number of both scalable and theoretically well founded algorithms. In this paper, we show that collaborative filtering and multitask learning are innately closely connected. In particular, the 'learning the kernel' paradigm in multitask learning turns out to be identical to a Ky-Fan norm minimization. This allows us to “import” collaborative filtering techniques into multitask learning and vice versa; in particular, we solve a multitask learning problem where the tasks also have features. We show the feasibility of our approach on two large real-world multitask learning applications. nJoint work with Markus Weimer, Wei Chu, Deepayan Chakrabarti.
13135 en On probabilistic hypergraph matching We consider the problem of finding a matching between two sets of features, given complex relations among them, going beyond pairwise. We derive the hyper-graph matching problem in a probabilistic setting represented by a convex optimization. First, we formalize a soft matching criterion that emerges from a probabilistic interpretation of the problem input and output, as opposed to previous methods that treat soft matching as a mere relaxation of the hard matching problem. Second, the model induces an algebraic relation between the hyper-edge weight matrix and the desired vertex-to-vertex probabilistic matching. Third, the model explains some of the graph matching normalization proposed in the past on a heuristic basis such as doubly stochastic normalizations of the edge weights. A key benefit of the model is that the global optimum of the matching criteria can be found via an iterative successive projection algorithm. The algorithm reduces to the well known Sinkhorn row/column matrix normalization procedure in the special case when the two graphs have the same number of vertices and a complete matching is desired. Another benefit of our model is the straightforward scalability from graphs to hyper-graphs. nThe work was done with Ron Zass (and made its debut in CVPR 2008)
13136 en A metric notion of dimension and its applications to learning Let us define the dimension of a metric space as the minimum k>0 such that every ball in the metric space can be covered by 2^k balls of half the radius. This definition has several attractive features besides being applicable to every metric space. For instance, it coincides with the standard notion of dimension in Euclidean spaces, but captures also nonlinear structures such as manifolds. nMetric spaces of low dimension (under the above definition) occur naturally in many contexts. I will discuss recent theoretical results regarding such metric spaces, including questions such as embeddability, dimension reduction, Nearest Neighbor Search, and large-margin classification, the common thread being that low dimension implies algorithmic efficiency.
13137 en A note of caution regarding distances on graphs Non-geometric data is often represented in form of a graph where edges represent similarity or local relationships between instances. One elegant way to exploit the global structure of the graph is implemented by the commute distance (also known as resistance distance). Supposedly it has the property that vertices from the same cluster are "close" to each other whereas vertices from different clusters are "far" from each other. We study the behavior of the commute distance as the size of the underlying graph increases. We prove that the commute distance converges to an expression that does not take into account the structure of the graph and that is completely meaningless as a distance function on the graph. Consequently, the use of the raw commute distance for machine learning purposes is strongly discouraged for large graphs and in high dimensions. We suspect that a similar behavior holds for several other distances on graphs.
13138 en Is non-(geo)metricity an issue for machine learning? 
13139 en Scalable algorithms for learning on graphs Networked data are found in a variety of domains: Web, social networks, biological networks, and many others. In learning tasks, networked data are often represented as a weighted graph whose edge weights reflect the similarity between incident nodes. In this talk, we consider the problem of classifying the nodes of an arbitrary given graph in the game-theoretic mistake bound model. We characterize the optimal predictive performance in terms of the cutsize of the graph's random spanning tree, and describe a randomized prediction algorithm achieving the optimal performance while running in expected time sublinear in the graph size (on most graphs). These results are then extended to the active learning model, where training labels are obtained by querying nodes selected by the algorithm. We describe a fast query placement strategy that, in the special case of trees, achieves the optimal number of mistakes when classifying the non-queried nodes. nJoint work with: Claudio Gentile, Fabio Vitale and Giovanni Zappella.
13167 en Introduction to Machine Learning How can we represent data on a computer and use it to learn to performnuseful tasks? This lecture reviews some simple classification andnregression rules, discusses under- and over-fitting and emphasises thenutility of defining objective functions for learning. There is also anshort overview of Bayesian learning, and some practical tips fornpre-processing and visualizing data. The lecture ends with a briefnmention of unsupervised learning and related topics.
13168 en Probability and Mathematical Needs This lectures covers basics in linear algebra and probabilities as well as anbrief introduction to optimization.nIn linear algebra, the lecture starts with the definition of vectors spaces,ndimension, basis, span of vectors and so forth. Norms and dot products as wellnas Hilbert spaces are introduced. Then the problem of solving linear system isntackled, introducing matrices, eigenvalues... and some common factorizationn(SVD, LU, Choleski, QR).nIn probabilities, we start from the definition of discrete and continuous randomnvariables, give common examples, introduce the concepts of independence andnconditional probabilities. We tackle estimation through Bayes framework, giventhe basic definitions in information theory (entropy, Kullback-Leiblerndivergence) and introduce error bounds (Hoeffding bounds).nOptimization is briefly introduced defining extrema and convex functions. Annexample of constrained minimization is demonstrated.
13169 en Graphical Models We will discuss probabilistic graphical models associated to directed andnundirected graphs. We will introduce exact inference algorithms, such as thensum-product algorithm, and apply it to hidden Markov models. We will alsondiscuss elements of learning in graphical models including maximum likelihood,nmaximum a posteriori and the expectation-maximisation algorithm.
13170 en Introduction to Kernel Methods In this talk, we are going to see the basics of kernels methods. After a briefnpresentation of a very simple kernel classifier, we'll give the definition of anpostive definite kernel and explain Support vector machine learning. Then, a fewnkernels for structured data, namely sequences and graphs, will be described. Thenrepresenter theorem is presented, which explains the rationale for the usualnkernel expansion encountered when working with kernel methods. Finally, a fewnelements from statistical learning theory are given.
13171 en Machine Learning for Natural Languages Processing Probabilistic Context Free Grammars (PCFG) is a powerful formalismnthat has been used for several applications in Computational Linguistics. Onenimportant problem of these models is the probabilistic estimation of thenprobabilistic part of the models. This probabilistic estimation is based onntabular algorithms similar to the CKY algorithm. We review these estimationnalgorithms and their properties. The use of these models for Language Modelingnand Machine Translation is also introduced. Finally, an interactive-predictivenframework for parsing is explained, that can be used for developing both on-linenlearning techniques and active learning techniques.
13172 en Speech Processing The lecture presents various aspects of automatic speech processingn(SP), from spoken contents extraction to high level categorization ofnspeech signals. We show how machine learning provides solutions to thenmain issues that the speech processing systems have to deal with.nSpeech is one of the main way that humans communicate together. It is ancomplex process that involves, in a highly integrated way, perceptionnabilities and cognitive processes. In spite of the efforts produced bynthe scientific community for simulating these abilities, knowledge-basednapproaches failed in modeling of speech.nNowadays, most of the SP methods relies on statistical modeling ofnspeech. The lecture presents this theorical framework in which the majornissues in speech processing are formulated. Then, the main tasks of SPnare overviewed, especially speaker identification and speech recognitionnand understanding.
13174 en Online Learning This talk aim at presenting a first approaching to the Online Learningnmethodology. It starts presenting the linear Perceptron algorithm and severalnderivations. The kernel Perceptron algorithm is also motivated. The Passive-nAggressive (PA) Online Learning algorithm is then presented for binarynclassification, regression and multi-class problems as well. Linear and kernelnversion of PA are presented and discussed playing an especial attention to thenupdate rules derivation. An important issue is to be able to control the modelncomplexity, to this end different budgeted online algorithms are proposed, firstnfor the conventional Perceptron and derivations and second, for the PAnalgorithm. Finally some examples of online learning applications are presentednin order to motivate the audience to the application of these techniques inndifferent real scenarios.
13175 en Grammatical Inference Grammatical inference is about learning automata and grammars givenninformation about the language.nDuring this tutorial we will introduce the problem and its applications,nstudy the various paradigms andnrelated learnability results and discuss some of the most importantnalgorithms in the field.
